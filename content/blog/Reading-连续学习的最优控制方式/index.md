+++

title = "连续学习的最优控制方式"

date = "2025-08-15"

[taxonomies]

tags = ["Machine Learning", "Continual Learning", "Optimal Control"]

+++

> Original Paper: [Optimal Protocols for Continual Learning via Statistical Physics and Control Theory](https://www.123865.com/s/plj7Vv-fvR23)

## Introduction

**Multi-Task Learning**: Training a neural network on a series of tasks.

**Catastrophic Forgetting**: Multi-task learning can lead to catastrophic forgetting, where learning new tasks degrades performance on older ones.

**Replay**: Present the network with examples from the old tasks while training on the new one to minimize forgetting.

## Model-Based Theoretical Framework

**Teacher-Student Framework**: Here we consider a teacher-student framework

- Student: The student network is trained on synthetic inputs $\boldsymbol{x} \in \mathbb{R}^N$, drawn i.i.d. from a standard Gaussian distribution $x_i \sim \mathcal{N}(0, 1)$.
- Teacher: The labels for each task $t = 1,2,\cdots, T$ are generated by the teacher networks, $y^{(t)}=g_{\ast}(\boldsymbol{x}\cdot\boldsymbol{w}_{\ast}^{(t)}/\sqrt{N})$, where $\boldsymbol{W}_{\ast}=(\boldsymbol{w}_{\ast}^{(1)},\cdots,\boldsymbol{w}_{\ast}^{(T)})^{\top}\in\mathbb{R}^{T\times N}$ denote the corresponding teacher vectors, and $g_{\ast}$ the activation function.

![image](assets/image-20250815095756-eyuazzc.png "Representation of the continual learning task in the teacher-student setting: (a) A student network is trained on i.i.d. inputs from two teacher networks, defining two different tasks; (b) Sequential training results in catastrophic forgetting.")

‍

‍
