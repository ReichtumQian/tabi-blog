<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://reichtumqian.pages.dev name=base><title>
Reichtum's Blog • 论文阅读：LiNeS Post-Training Layer Scaling Prevents Forgetting and Enhances Model Merging</title><link title="Reichtum's Blog - Atom Feed" href=https://reichtumqian.pages.dev/atom.xml rel=alternate type=application/atom+xml><link href="https://reichtumqian.pages.dev/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://reichtumqian.pages.dev/main.css?h=28b3fcbb58f2f22a8c44" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Reichtum's Blog" name=description><meta content="Reichtum's Blog" property=og:description><meta content="论文阅读：LiNeS Post-Training Layer Scaling Prevents Forgetting and Enhances Model Merging" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/ property=og:url><meta content="Reichtum's Blog" property=og:site_name><noscript><link href=https://reichtumqian.pages.dev/no_js.css rel=stylesheet></noscript><script src=https://reichtumqian.pages.dev/js/initializeTheme.min.js></script><script defer src=https://reichtumqian.pages.dev/js/themeSwitcher.min.js></script><script src="https://reichtumqian.pages.dev/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunr.zh.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://reichtumqian.pages.dev/>Reichtum's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/projects/>projects </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">论文阅读：LiNeS Post-Training Layer Scaling Prevents Forgetting and Enhances Model Merging</h1><a class="u-url u-uid" href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://reichtumqian.pages.dev rel=author title=Reichtum>Reichtum</a> </span><li><time class=dt-published datetime=2025-10-20>20th Oct 2025</time><li title="1815 words"><span aria-hidden=true class=separator>•</span>10 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/model-merging/>Model Merging</a>, <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/fine-tuning/>Fine-Tuning</a></ul><section class="e-content body"><p>‍<hr><h2 id=introduction>Introduction</h2><p><strong>Model Merging</strong>: Combine avaliable checkpoints, <u>avoiding the costly process of joint trade-offs</u>, such as the forgetting of previously acquired knowledge. However, merging checkpoints fine-tuned on different tasks can lead to <u>significant performance degradation</u>.<p><strong>Catastrophic Forgetting Mitigation</strong>: Many works propose:<ul><li>Regularizing the fine-tuning process<li>Leveraging the insight that shallow layers capture generalizable representations, and applying lower learning rate to the shallow layers.</ul><p>However, <u>modifying the fine-tuning process can be complex and computationally expensive</u>. Model editing and model merging methods that <u>directly edit the checkpoints in the weight space</u>.<p><strong>Drawbacks of Existing Methods</strong>: Most model merging methods <u>overlook the insight that shallow layers should remain close to their pre-trained weights to avoid losing the general representations</u> they encode.<p><strong>Contribution of This Work</strong>:<ul><li>We propose LiNeS, a post-training editing technique that <u>preserves the zero-shot generalization of pre-trained models</u> while <u>retaining fine-tuned knowledge</u> by applying layer-wise scaling on parameter updates.<li>LiNeS significantly enhances multi-task model merging baselines.<li>LiNeS can be applied to enhance existing weight interpolation methods.</ul><blockquote><p>Wight Interpolation 是 Model Merge 的一种技术/方法。</blockquote><hr><h2 id=related-work>Related Work</h2><p><strong>Representation Collapse and Regularized Fine-Tuning</strong>:<ul><li>Pre-trained models <u>exhibit strong zero-shot performance</u> across diverse data distribution due to the <u>robust and transferable feature representations</u> learned during pre-training.</ul><blockquote><p>Feature representation：深度学习模型一般不会直接处理原始数据，而是通过一系列计算将原始数据转换为利于理解的内部表示（Feature Representation）。</blockquote><ul><li>However, fine-tuning on specific tasks often <u>harms the zero-shot generalization performance</u> on distributions different from the fine-tuning domain. This degradation arises from the <u>distortion of pre-trained feature during fine-tuning</u>. A phenomenon referred to as <u><em>representation collapse</em></u>.<li>Modifying the fine-tuning process is far more computationally expensive compared to post-training merging methods.</ul><p><strong>Weight Interpolation and Model Merging</strong>:<ul><li>It is shown that two solutions derived from <u>separate training runs can be connected by nonlinear paths of low loss</u>. Linear mode connectivity extended the paths to the linear case.</ul><blockquote><p>原本我们以为从通过不同的起点（随机初始化）出发，最终到达的两个模型（解）是相互隔离的。但是事实上存在一条 loss 很小的路径连接这两个解。</blockquote><ul><li>These insights enabled the transfer of the benefits regarding <u>robustness output ensembles to weight ensembles</u>.</ul><blockquote><p>Output Ensembles：把同个问题分别询问多个模型，综合它们的答案来得到最终结果<p>Weight Ensembles：不再需要保留多个模型，而是直接合并成一个模型</blockquote><ul><li>These findings can be leveraged to improve performance on <u>single-task, out-of-distribution</u>, multi-task and multi-objective alignment settings.</ul><blockquote><p>单个任务上训练多个模型合并起来能得到更好的模型，多个任务合并起来能获得多任务的模型。</blockquote><ul><li>Several methods have tried to merge by preserving the important parameters defined via the Fisher Information Matrix, heuristics…</ul><blockquote><p>如何合并呢：简单加权肯定不行，因为有时候会直接抵消一些特征。因此有很多改进方法，例如 Fisher Information Matrix、Heuristics 启发式算法等</blockquote><ul><li>Recent works use gradient descent to learn the <u>layer-specific merging coefficients</u> per task, e.g., Ada-merging.</ul><blockquote><p>学习如何合并，每一层、每个任务都学习一个独特的合并系数</blockquote><hr><h2 id=post-training-layer-wise-scaling-mitigates-forgetting>Post-Training Layer-Wise Scaling Mitigates Forgetting</h2><p>We present the key insight of this work: Scaling down the updates of shallow layer after fine-tuning can <u>mitigate catastrophic forgetting</u>, <u>restore zero-shot generalization</u>, <u>preserving performance on the target task</u>.<p><strong>Notation</strong>: We consider a pre-trained model $\theta_0 \in \mathbb{R}^N$. Fine-tuning on a specific task $t$ results in the fine-tuned weights $\theta_t$. The <u>task vector</u> or residual for task $t$ is defined as<p>$$ \tau_t:= \theta_t - \theta_0. $$<p><strong>Fine-Tuning Leads to Catastrophic Forgetting</strong>: Consider an $8$-task image classification problem. We fine-tune a CLIP ViT-B/32 model on each task, measuring performance on the fine-tuned task (target task) and the remaining $7$ tasks (control tasks).<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251018001650-i86x626.png>​<p><strong>Shallow-Layer Updates Impact Minimally on Target Task Accuracy</strong>:<ul><li>Most parameter updates during fine-tuning are <u>redundant</u>, as similar performance is <u>achievable without updating most pre-trained weights</u>.<li>Prior work shows that <u>task-specific features are often concentrated in deeper layers of the network</u>.<li>We apply a scaling factor to the updates to the $\ell$-th layer $\tau^{(\ell)}$,</ul><p>$$ \lambda^{(\ell)}=\gamma+(1-\gamma)\frac{\ell-1}{L-1}, \quad \forall\ell\in[L],\gamma\in[0,1] $$<ul><li>We observe that even with strong downscaling of shallow layers, the target task accuracy remains nearly unaffected. In contrast, when we downscale the deeper layers, target task accuracy drops significantly.</ul><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251018000929-67k34bh.png>​<blockquote><p>首先进行 Fine-Tune 获得各个层的更新量，然后用 $\lambda^{(\ell)}$ 对各层的更新量进行放缩，$\gamma$ 越小说明浅层更新量被缩小得越厉害。左图中可以看出浅层更新量的改变几乎不影响 Target Task。</blockquote><p><strong>Shallow-Layer Updates Undermine Zero-Shot Generalization</strong>: We further hypothesize that the <u>degradation of performance on control tasks is largely due to distortions in the shallow layers</u>. We can see that as the strength of the shallow-layer downscaling increases, the accuracy on control tasks approaches the original pre-trained model’s performance.<p><strong>Improved Trade-Off between Target and Control Performance</strong>: We apply the method to a 20-task computer vision benchmark. Figure 2 shows that <u>fine-tuning degrades zero-shot generalization</u>, and our method improves generalization while maintaining near-full target task accuracy.<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251018001726-mj8r64h.png>​<blockquote><p>上图中的每个点分别对应 20 个任务。</blockquote><hr><h2 id=method>Method</h2><p><strong>Layer-increasing Network Scaling</strong>: Given a task vector $\tau$ with $L$ layer blocks, we apply layer-wise linear scaling to adjust the contributions of shallow and deep layers using:<p>$$ \tau_{\mathrm{LiNeS}}=\mathrm{concat}\left(\lambda^{(1)}\tau^{(1)},\ldots,\lambda^{(L)}\tau^{(L)}\right),\quad\mathrm{where} \quad \lambda^{(\ell)}=\alpha+\beta\frac{\ell-1}{L-1},\quad\forall\ell\in[L]. $$<p><strong>Application in Model Merging</strong>: Notice that $\tau$ can correspond to either a single-task residual or, in the context of model merging, a multi-task vector obtained by merging the residuals of multiple checkpoints fine-tuned starting from a common initialization.<hr><h2 id=experiments>Experiments</h2><p><strong>Improving Robust Fine-Tuning for OOD Generalization</strong>:<ul><li>WiSE-FT: Linearly interpolate between the pre-trained and the fine-tuned weights: $\tau: (1 - \gamma) \theta_0 + \gamma \theta = \theta_0 + \gamma \tau$ for $\gamma \in [0, 1]$.<li>We evaluate CLIP models fine-tuned on ImageNet, considering $5$ OOD datasets.</ul><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251018002145-2cvbvku.png>​<p><strong>Improving Multi-Task Model Merging</strong>:<ul><li>Task Arithmetic: Generate a multi-task vector $\tau_{\text{MTL}} = g(\tau_1, \cdots, \tau_T)$ with merging function $g$, and construct $\theta = \theta_0 + \lambda \cdot \tau_{\text{MTL}}$. In this work, “task arithmetic” is implemented by <u>averaging the task vectors</u>.</ul><blockquote><p>Task vector 指的是在某个 task 微调后，参数的变化量。</blockquote><ul><li><p>Performance Loss during Merging: This performance decrease partially stems from interference among task vectors.</p><li><p>We can edit each task vector with <code>LiNeS</code>​ before merging to restore the generalization to other tasks, or simply <u>edit the merged multi-task vector to preserve the shallow and general features</u>.</p><li><p>We tune $\beta$ and set $\alpha$ to</p></ul><p>$$ \alpha=\frac{1}{N_{\mathrm{models}}}\frac{\|\tau_{\mathrm{sum}}\|}{\|\tau_{\mathrm{MTL}}\|},\quad \mathrm{where} \quad \tau_{\mathrm{sum}}=\sum_{i=1}^{N_{\mathrm{models}}}\tau_{i} $$<p><strong>Computer Vision</strong>:<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251018122124-ftko85a.png>​<p><strong>Natural Language Processing</strong>:<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251018125205-hgttcda.png>​<p><strong>Model Soups for Merging Single-Task Models</strong>: Averaging in weight space <u>multiple models fine-tuned on the same task derived from the same pre-trained model</u> has been shown to increase target performance. We investigate whether <code>LiNeS</code>​ can enhance the test performance when merging single-task models.<p>$$ \theta_{\mathrm{soup}}=\theta_0+\tau_{\mathrm{soup}}, \quad \mathrm{where} \quad \tau_{\mathrm{soup}}=\frac{1}{N_{\mathrm{models}}}\sum_{i=1}^{N_{\mathrm{models}}}\left(\boldsymbol{\theta}_i-\boldsymbol{\theta}_0\right) $$<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251019111558-lcfmm78.png>​<blockquote><p>Uniform Soup 指把所有模型平等看待，Greedy Soup 表示保留一个验证集对模型进行筛选</blockquote><p><strong>Improving Rewarded Soups</strong>:<ul><li>Starting with an LLM parameterized by weights $\theta_0$, we first fine-tune it using SFT resulting in weights $\theta_{\text{SFT}}$.<li>We then apply RLHF, training two independent policies via PPO to maximize the rewards $R_1$ and $R_2$.<li>We linearly interpolate the residuals $\tau_1 = \theta_1 - \theta_{\text{SFT}}$ and $\tau_2 = \theta_2 - \theta_{\text{SFT}}$​</ul><p>$$ \theta_{RS}=\theta_{\mathrm{SFT}}+\lambda\tau_{1}+(1-\lambda)\tau_{2},\quad\lambda\in[0,1], $$<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251019111622-n86fpge.png>​<hr><h2 id=discussion>Discussion</h2><p>We compare <code>LiNeS</code>​ with prior work that optimizes the scaling coefficients via backpropagation:<ul><li>Ada-Merging: Minimizes the entropy loss of the predictions on the test set<li>aTLAS: Minimizes a cross entropy loss on validation samples</ul><p>Both methods operate on a more fine-grained level and introduce coefficients per layer and per task. Ada-merging and aTLAS requires excessive memory overhead and multiple training epochs.<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/assets/image-20251019111723-pkugyw0.png>​<hr><h2 id=questions-and-comments>Questions and Comments</h2><ul><li>模型合并和缓解遗忘很像：无非就是解决任务/数据的冲突<li>Layer-Wise：线性的 scaling 一定是最优的吗？能不能有更好的方式？<li>Task-Wise：不同任务之间能不能更好的合并方式？<li>Catastrophic Forgetting：Representation Learning 中有没有其他相关的结论，能直接拿来用？</ul><p>‍</section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/#introduction>Introduction</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/#related-work>Related Work</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/#post-training-layer-wise-scaling-mitigates-forgetting>Post-Training Layer-Wise Scaling Mitigates Forgetting</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/#method>Method</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/#experiments>Experiments</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/#discussion>Discussion</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-lines-post-training-layer-scaling-prevents-forgetting-and-enhances-model-merging-md/#questions-and-comments>Questions and Comments</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://reichtumqian.pages.dev/katex.min.css rel=stylesheet><script defer src=https://reichtumqian.pages.dev/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://reichtumqian.pages.dev/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://reichtumqian.pages.dev/atom.xml> <img alt=feed loading=lazy src=https://reichtumqian.pages.dev/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>