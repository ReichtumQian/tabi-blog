<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://reichtumqian.pages.dev name=base><title>
Reichtum's Blog • 论文阅读：RL's Razor Why Online Reinforcement Learning Forgets Less</title><link title="Reichtum's Blog - Atom Feed" href=https://reichtumqian.pages.dev/atom.xml rel=alternate type=application/atom+xml><link href="https://reichtumqian.pages.dev/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://reichtumqian.pages.dev/main.css?h=28b3fcbb58f2f22a8c44" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Reichtum's Blog" name=description><meta content="Reichtum's Blog" property=og:description><meta content="论文阅读：RL's Razor Why Online Reinforcement Learning Forgets Less" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/ property=og:url><meta content="Reichtum's Blog" property=og:site_name><noscript><link href=https://reichtumqian.pages.dev/no_js.css rel=stylesheet></noscript><script src=https://reichtumqian.pages.dev/js/initializeTheme.min.js></script><script defer src=https://reichtumqian.pages.dev/js/themeSwitcher.min.js></script><script src="https://reichtumqian.pages.dev/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunr.zh.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://reichtumqian.pages.dev/>Reichtum's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/projects/>projects </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">论文阅读：RL’s Razor Why Online Reinforcement Learning Forgets Less</h1><a class="u-url u-uid" href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://reichtumqian.pages.dev rel=author title=Reichtum>Reichtum</a> </span><li><time class=dt-published datetime=2025-09-10>10th Sep 2025</time><li title="1942 words"><span aria-hidden=true class=separator>•</span>10 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/machine-learning/>Machine Learning</a>, <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/continual-learning/>Continual Learning</a>, <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/fine-tuning/>Fine Tuning</a></ul><section class="e-content body"><blockquote><p>Original Paper: <a href=https://www.arxiv.org/abs/2509.04259>[2509.04259] RL's Razor: Why Online Reinforcement Learning Forgets Less</a><p>Posts: <a href=https://jyopari.github.io/posts/rl_razor>RL's Razor: Why On-Policy Reinforcement Learning Forgets Less</a></blockquote><hr><h2 id=introduction>Introduction</h2><p><strong>On-Policy RL</strong>: The agent learns only from experience <u>generated by its currect policy</u>. It follows its current strategy, collects data, updates its strategy, and then discards the old data.<p><strong>Off-Policy RL</strong>: The agent can learn from experience <u>generated by a different policy</u>. This could be data from an older version of itself, from a human demonstrator, or from another AI.<blockquote><p>On-Policy RL 只学习由【当前最新版本】的策略所产生的经验，Off-Policy 允许学习由【任何策略】（包括过去的自己、人类专家等）所产生的经验。</blockquote><p><strong>Online RL</strong>: The agent learns by <u>actively interacting with a live environment</u>. It takes an action, gets a result, and learns from it in real-time.<p><strong>Offline RL</strong>: The agent learns from a <u>fixed, pre-collected dataset of past interactions</u>. It has no ability to explore or gather new data.<blockquote><p>Online 和 On-Policy 的辨析<ul><li>Online 决定了经验是如何产生的：是通过与环境的实时互动，而非提前准备好<li>On-Policy 决定了采集到的经验是如何被使用的，是“用完即弃”（On-Policy），还是“存起来反复用”（Off-Policy）</ul><p>Offline 和 Off-Policy 的辨析：Offiline 一定是 Off-Policy 的，反之不一定</blockquote><p><strong>Previous Approaches to Catastrophic Forgetting</strong>: Previous approaches such as <u>constraining weight updates</u>, <u>preserving learned features</u>, or <u>regularizing shift in output distribution</u> focus on its effects rather than its underlying cause. Some prior work claimed that forgetting can be determined by <u>how much the model’s distribution shifts on past tasks</u>, but in practice this is infeasible because the set of prior tasks is vast or even unbounded.<p><strong>EWC</strong>: Elastic weight consolidation can be seen as approximations to KL minimization.<p><strong>SFT Versus RL</strong>: Prior comparisons between SFT and RL have focused on new task performance rather than the extent of forgetting. It is found that on-policy learning can <u>achieve stronger performance when the expert providing supervision is the same</u>.<p><strong>ParityMNIST</strong>: ParityMNIST is derived from MNIST, but reframes the task as predicting parity (even vs. odd).<hr><h2 id=contribution-of-this-work>Contribution of This Work</h2><p><strong>RL Forgets Less than SFT</strong>: Even when SFT and RL achieve the same performance on the new task, we observe that SFT often achieves new-task gains by erasing prior knowledge, while RL better preserves old skills.<figure><img alt=image src=assets/image-20250909102057-78oqku5.png><figcaption>Bias toward KL-minimal solutions reduces forgetting: (1) Left: RL converges to those closest in KL to the base model. (2) Right: RL preserves better prior-task performance compared to SFT.</figcaption></figure>​ <p><strong>Empirical Forgetting Law</strong>: When fine-tuning a model $\pi$ on a new task $\tau$, the degree of forgetting is accurately predicted by $\mathbb{E}_{x \sim \tau}[\operatorname{KL}(\pi_0 ||\pi)]$, where $\pi_0$ is the base policy. <strong>KL divergence is a reliable predictor of forgetting across settings</strong>.<p><strong>Difference between SFT and RL</strong>: On-policy methods such as RL are inherently <u>biased toward solutions that remain closer to the original policy in KL divergence</u>.<p><strong>KL Hypothesis Validation</strong>: We construct a “oracle SFT” that minimizes KL divergence while achieving perfect accuracy, which achieves even less forget than RL. This demonstrate that <u>RL’s advantage does not stem from being inherently different, but from its implicit KL minimization</u>.<blockquote><p>并不是 RL 好，而是 RL 中内含的 KL Minimizer 减少了遗忘，如果 SFT 能降低 KL Divergence，其也能减少遗忘。</blockquote><hr><h2 id=results>Results</h2><p><strong>Experimental Setup</strong>: We fine-tuned models using the same set of prompts. One group of models was trained with SFT, and another with RL using GRPO. In RL training, we used only a binary success indicator as the reward, <strong>without explicit KL regularization</strong>.<ul><li>LLM, Math Reasoning: Qwen 2.5 3B-Instruct on Open-Reasoner-Zero dataset.<li>LLM, Science Q&A: Qwen 2.5 3B-Instruct on Chemistry L-3 subset of SciKnowEval.<li>LLM, Tool use: Qwen 2.5 3B-Instruct on ToolAlpaca dataset.<li>Robotics, Pick and Place: OpenVLA 7B on SimplerEnv environment.</ul><p><strong>RL Forgets Less than SFT</strong>: RL is able to learn new tasks while incurring minimal forgetting, whereas SFT reaches similar new-task performance only by sacrificing prior knowledge.<figure><img alt=image src=assets/image-20250909112119-amhkwcj.png><figcaption>Pareto frontiers of RL and SFT: Comparing the performance of a fine-tuned model on the new task (x-axis) and prior task (y-axis). Each point corresponds to a model trained with a different set of hyperparameters.</figcaption></figure>​ <p><strong>Smaller KL Divergences Lead to Less Forgetting</strong>: We pretrained a MLP jointly on a subset of ParityMNIST and FashionMNIST, then fine-tuned only on ParityMNIST while measuring forgetting on FashionMNIST. We constructed an oracle SFT distribution (use the KL minimization answer as label instead of the true label).<figure><img alt=image src=assets/image-20250910102102-a77ljmv.png><figcaption>KL divergence predicts catastrophic forgetting: (1) SFT outperforms RL only when an oracle distribution is used. (2) Forgetting aligns a single curve when plotted against KL divergence. (3) RL improves new-task accuracy with much smaller KL shifts than SFT.</figcaption></figure>​ <ul><li>​<code>SFT on dist 1</code>​: All even digits mapped to label 0, all odd digits to label 1.<li>​<code>SFT on dist 2</code>​: Even digits randomly mapped to $\{0, 4\}$, odd digits to $\{1,5\}$.<li>​<code>SFT on optimal dist</code>​: Annotations drawn from the minimum-KL distribution consistent with task correctness. Concretely, for an input image $x$ we compute $\pi_0(\cdot | x) \in \mathbb{R}^{10}$ and the binary indicator vector $R \in \{0,1\}^{10}$ encoding which labels are correct given the digit’s parity. The oracle distribution $q^\ast$ is the solution to</ul><p>$$ q^*=\arg\min_qD_{\text{KL}}(\pi_0\|q)\quad\text{s.t.}\quad q^\top R=1. $$<blockquote><p>例如给定图片 $2$，因为其是偶数，$R$ 在所有偶数位置为 $1$​，那么上面的向量 <code>R = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]</code>​</blockquote><p><strong>On-Policy Methods Lead to Smaller KL Divergence</strong>: Here we consider the loss function of SFT and RL:<ul><li>SFT minimizes cross-entropy against a supervision distribution $\pi_\beta$ over a distribution of inputs $\mathcal{D}$​</ul><p>$$ {\mathcal{L}}_{\mathrm{SFT}}(\pi)=-\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\beta}}[\log\pi(y|x)] $$<ul><li>Let $A(x,y)$ be an advantage function. RL with policy gradient optimizes</ul><p>$$ \mathcal{L}_{\mathrm{RL}}(\pi)=-\mathbb{E}_{x\sim\mathcal{D},y\sim\pi}\left[A(x,y)\log\pi(y|x)\right] $$<p>. There are two features that distinguish RL from SFT:<ul><li>Sampling Distribution: While in RL the training was done on outputs <u>drawn from the model’s own distribution</u>, in SFT they <u>come from fixed external annotations</u>.<li>Negative Examples: While sampling from $\pi$, some of the responses will be incorrect. These are usually assigned a <u>negative coefficient</u> $A(x,y)$. This pushes probability mass away from poor outputs.</ul><p>Our hypothesis is that one of these two differences is what causes RL’s resistance to forgetting. So we perform experiments with four different objectives: “GRPO”, “1-0 Reinforce”, “SFT”, and “SimPO”. The results show that <u>the critical factor is the use of on-policy data</u>.<figure><img alt=image src=assets/image-20250910112852-pz5kjgm.png><figcaption>Comparison of algorithm classes: &amp;quot;Pos Examples&amp;quot; indicates that the dataset only contains positive examples while &amp;quot;Pos + Neg Examples&amp;quot; indicates that the dataset contains both positive and negative examples.</figcaption></figure>​ <p><strong>Theoretical Perspective</strong>: Sampling from the model’s own distribution keeps it close to the base model, while SFT pushes it toward arbitrary external distributions.<figure><img alt=image src=assets/image-20250910113142-i3xep9v.png><figcaption>KL-minimal path to optimality.</figcaption></figure>​ <blockquote><p>RL 的 policy 更新可以被视作来回的投影，源于 Information Geometry 的领域。假设空间中有我们的策略 $\pi_k$，可行空间 $\Pi$，最优空间 $P^\ast$。<ul><li>SFT 就像给定一个 $P^\ast$ 的一个坐标，让我们传送到那里，不管距离有多远<li>RL 则先用 Information Projection 选择 $P^\ast$ 中最近的位置进行投影，再用 Momentum Projection 投影到 $\Pi$ 中。最终 RL 可以视作一次在 $\Pi$ 中的更新。</ul></blockquote><hr><h2 id=alternative-hypothesis>Alternative Hypothesis</h2><p>We systematically evaluated alternative variables as potential predictors of catastrophic forgetting, grouped into four categories.<p><strong>Weight-Level Changes</strong>: Many prior work tried to mitigate forgetting by <u>constraining the change in parameter space</u>. We measured parameter changes under L1, Fisher-weighted L2, and spectral norm metrics. These metrics correlated only weakly with forgetting: <u>large parameter shifts could occur without forgetting, and conversely, forgetting sometimes occurred despite small parameter movement</u>.<p><strong>Representation/Activation-Level Changes</strong>: Some other papers focused on <u>maintaining the previous features</u>. We examined hidden activation shifts (L1 and L2 distances) as proxies for changes in internal representations. Although we found that there is representation drift during training, the <u>curves were distinct between training objectives</u> (不同训练方法的【Activation Change-Forgetting】曲线不同), meaning that it is not a good predictor.<figure><img alt=image src=assets/image-20250910105915-bchua9h.png><figcaption>CKA similarity to the base model during training.</figcaption></figure>​ <blockquote><p>CKA (Centered Kernel Alignment) 是一种数学工具，用于衡量两个分布的相似度。CKA 越接近 $1$ 说明与原始分布越接近。上图说明了 SFT 中模型的内部知识结构（表征）遭到了破坏。</blockquote><p><strong>Sparsity and Rank of Updates</strong>: Some argue that RL updates are sparse while SFT weight updates are dense. We found that the reason for the observed sparse updates was the use of <code>bfloat16</code>​, which may ignoring some small updates. Performing the same training with <code>float32</code>​ leads to identical performance without any sparsity. So we found that <u>all algorithms lead to full rank weight updates</u>.<p><strong>Distributional Distances</strong>: We considered multiple measures of output distribution change: (1) Forward KL $\mathbb{E}_{x \sim \tau}[\operatorname{KL}(\pi_0 || \pi)]$, (2) Reverse KL $\mathbb{E}_{x \sim \tau}[\operatorname{KL}(\pi || \pi_0)]$, (3) Total Variation, (4) $L_2$ distance between distributions.<figure><img alt=image src=assets/image-20250910104205-36souvi.png><figcaption>Predictive power of alternative variables compared to forward KL.</figcaption></figure>​ <blockquote><p>Distribution 在这里指最终模型输出的差别，因为衡量遗忘最直接的办法是衡量其对老问题的回答有没有变。这里 $R^2$ 是一种统计指标，$R^2 = 1$ 表示完美预测。</blockquote><hr><h2 id=additional-results>Additional Results</h2><p><strong>Gradient Similarity versus KL Change</strong>:<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/assets/image-20250910110729-fbb692h.png>​<p>‍</section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/#introduction>Introduction</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/#contribution-of-this-work>Contribution of This Work</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/#results>Results</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/#alternative-hypothesis>Alternative Hypothesis</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/#additional-results>Additional Results</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://reichtumqian.pages.dev/katex.min.css rel=stylesheet><script defer src=https://reichtumqian.pages.dev/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://reichtumqian.pages.dev/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://reichtumqian.pages.dev/atom.xml> <img alt=feed loading=lazy src=https://reichtumqian.pages.dev/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>