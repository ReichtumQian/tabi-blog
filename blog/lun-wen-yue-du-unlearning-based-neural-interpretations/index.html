<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://reichtumqian.pages.dev name=base><title>
Reichtum's Blog • 论文阅读：Unlearning-Based Neural Interpretations</title><link title="Reichtum's Blog - Atom Feed" href=https://reichtumqian.pages.dev/atom.xml rel=alternate type=application/atom+xml><link href="https://reichtumqian.pages.dev/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://reichtumqian.pages.dev/main.css?h=28b3fcbb58f2f22a8c44" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Reichtum's Blog" name=description><meta content="Reichtum's Blog" property=og:description><meta content="论文阅读：Unlearning-Based Neural Interpretations" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/ property=og:url><meta content="Reichtum's Blog" property=og:site_name><noscript><link href=https://reichtumqian.pages.dev/no_js.css rel=stylesheet></noscript><script src=https://reichtumqian.pages.dev/js/initializeTheme.min.js></script><script defer src=https://reichtumqian.pages.dev/js/themeSwitcher.min.js></script><script src="https://reichtumqian.pages.dev/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunr.zh.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://reichtumqian.pages.dev/>Reichtum's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/projects/>projects </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">论文阅读：Unlearning-Based Neural Interpretations</h1><a class="u-url u-uid" href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://reichtumqian.pages.dev rel=author title=Reichtum>Reichtum</a> </span><li><time class=dt-published datetime=2025-10-04>4th Oct 2025</time><li title="1972 words"><span aria-hidden=true class=separator>•</span>10 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/machine-learning/>Machine Learning</a></ul><section class="e-content body"><blockquote><p>Original Paper: <a href=https://arxiv.org/abs/2410.08069>[2410.08069] Unlearning-based Neural Interpretations</a></blockquote><p>‍<hr><h2 id=introduction>Introduction</h2><p><strong>Machine Unlearning</strong>: Unlearning concerns “forgetting” of a data-point for a trained model:<ul><li>Exact Unlearning: Seeks indistinguishability guarantees between a model not trained on a sample and one that has unlearned. Provable exact unlearning is <u>only achieved under full re-training</u>.<li>Approximate Unlearning: Unlearns with first/second order gradient updates, achieving max-divergence bounds for single unlearning samples.</ul><p><strong>Perturbative Methods</strong>: Perturb inputs to change and explain outputs. These methods fail to give a global insight of the model’s decision function and are <u>highly unstable due to the reliance on local perturbations</u>. Different methods give <u>incongruent explanations</u> which cannot be acted on.<blockquote><p>因为局部扰动而高度不稳定，不同解释方法给出完全不同的解释。</blockquote><p><strong>Backpropagative Methods</strong>: These methods leverage <u>gradients of the output w.r.t. the input</u> for some given neuron activity of interest. However, gradients are <u>highly noise and locally sensitive</u>, they can only <u>crudely localize salient feature regions</u>.<blockquote><p>Salient feature region 表示重要特征的部分，梯度噪声很大，且只能粗略定位</blockquote><p><strong>Path-Based Attribution</strong>: This family of post-hoc attributions rely on a <u>baseline</u> – a “vanilla” image devoid of features, and a <u>path</u> – an often linear path from the featureless baseline to the target image. However, it comes with difficulties of defining an <u>unambiguously featureless baseline</u> and a <u>reliable path of increasing label confidence</u> without intermediate inflection points.<blockquote><p>Post-hoc 表示事后的</blockquote><hr><h2 id=notation>Notation</h2><ul><li>Input/Feature Space: $\mathcal{X}\subset\mathbb{R}^{d_X}$, where $d_X$ is the number of pixels in an image.<li>Output/Label Space: $\mathcal{Y}\subset\mathbb{R}^{d_{Y}}$, where $d_Y$ is the number of classes.<li>Model Space: $\mathcal{F}\subset\mathcal{Y}^{\mathcal{X}}$, and $F:x\mapsto(F_{1}(x),…,F_{d_{Y}}(x))$, where $F_c(x)$ is the probability score of class $c$.<li>Attribution Methods: $\mathcal{A}: \{1,…,d_{X}\}\times\mathcal{F}\times\{1,…,d_{Y}\}\times\mathcal{X}\to\mathbb{R}$, where $\mathcal{A}(i,F,c,x)$ is the importance score of pixel $i$ of image $x$ for the prediction made by $F_c$. For convenience, we use the shorthand $\mathcal{A}_i(x)$ to refer to the attributed saliency score of a pixel $i$ for a specific class $c$.<li>Linear Path Feature: $\gamma(x^{\prime},x,\alpha):\mathbb{R}^{d_{\mathcal{X}}}\times\mathbb{R}^{d_{\mathcal{X}}}\times[0,1]\to\mathbb{R}^{d_{\mathcal{X}}}$, where $\gamma=(1-\alpha)x^{\prime}+\alpha x$ and employ shorthands $\gamma(0)=x^{\prime},\gamma(1)=x$.</ul><hr><h2 id=limitation-of-existing-gradient-based-methods>Limitation of Existing Gradient-Based Methods</h2><p><strong>Limitation of Simple Gradient</strong>: The simple gradient $\mathcal{A}_{i}^{\mathrm{SG}}(x)=\nabla_{x_{i}}F_{c}(x)$ can be effciently computed but it <u>encounters output saturation</u> when activation functions like ReLU and Sigmoid are used, leading to <u>zero gradients even for important features</u>.<p><strong>DeepLIFT</strong>: DeepLIFT reduces saturation by introducing a “reference state” and <u>comparing each neuron’s activations to that of the baseline</u>.<p><strong>Integrated Gradients (IG)</strong> : Integrated gradients similarly utilizes a reference, <u>black image</u> and computes the integral of gradients interpolated on a straight line between the image and the baseline:<p>$$ \mathcal{A}_{i}^{\mathrm{IG}}(x)=(x_{i}-x_{i}^{\prime})\int_{\alpha=0}^{1}\nabla_{x_{i}}F_{c}\left(x^{\prime}+\alpha(x-x^{\prime})\right)\mathrm{d}\alpha $$<p>where $(x_{i}-x_{i}^{\prime})$ is the difference between the real pixel and the baseline pixel, and $x^{\prime}+\alpha(x-x^{\prime})$ is the line interpolation.<p><strong>Limitation of IG</strong>: The soundness of IG depends on two assumptions–a <u>baseline represents the “absence” of predictive features</u>, and <u>has stable path features</u>.<ul><li>Research suggests that when a black image is used, the <u>path features are ambiguous</u>, where extrema of the model confidencs lie along the integration path instead of at the endpoints of the baseline.<li>Models trained with image augmentations (e.g., color jittering, rescaling, gaussian blur) yield equivalent or <u>even higher confidences for blurred and lightly-noised baselines</u>.</ul><blockquote><p>两个核心假设：（1）黑色图片真的什么特征都不表示（2）从黑图渐变到真实图片过程中，模型的信心是单调递增的</blockquote><p><strong>Post-Hoc Biased are Imposed</strong>: Static baseline functions (e.g., black, blurred, noised) implicitly assume that <u>similar features (e.g., dark, smooth, high-frequency) are irrelevant for model prediction</u>. For example, IG with a black baseline implies that “near-black features are unimportant”, since the term $(x_i-x_i^\prime)$ is small and requires a large gradient $\nabla_{x_i} F_c(\cdot)$.<p><strong>Experiments</strong>: The color biases are not present naturally in the pre-trained model but rather injected implicitly by a suboptimal choice of static baseline.<ul><li>Figures 2, 3: Darker features belonging to the object-of-interest cannot be reliably identified.<li>Figure 4: IG with a blurred baseline fails to attribute blurred inputs due to saturation and overly smoothed image textures.<li>Figure 5: Visualize how a noised IG baseline encounters high-frequency noise and outputs irregular, high-variance attribution scores.</ul><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251003085650-pz03jgr.png>​<figure><img alt=image src=assets/image-20251002153529-xyta6h9.png><figcaption>Figure 3: The left half is the experiment after brightening while the right after saturation. The middle includes the original images.</figcaption></figure>​ <hr><h2 id=uni-unlearning-based-neural-interpretations>UNI: Unlearning-Based Neural Interpretations</h2><p><strong>Desirable Baseline</strong>: For every given task-model-image triad, a well-chosen baseline should be:<ul><li>Image-Specific: Be connected via a <u>path feature of low curvature (smooth) to the original image</u>, rather than unchanged black images.<li>Reflect only the model’s Predictive Biases: <u>Salient image features should be excluded</u> from the baseline.<li>Less Task-Informative than the Original Image: Interpolating from the baseline towards the input image should yield <u>a path of increasing predictive confidence</u>.</ul><p><strong>UNI Pipeline</strong>:<ol><li>Unlearn predictive information in the model space<li>Use activation-matching between unlearned and trained models to mine a featureless baseline in the image space<li>Interpolate along the low-curvature, conformant and consistent path from baseline to image to compute reliable explanations in the attributions space</ol><blockquote><p>第一步：反学习，让模型忘掉怎么识别特定图片，生成一个针对特定图片的【失忆模型】。具体的方式是做【梯度上升】。<p>第二步：使用【激活值匹配】，生成一张图片，使得【原版模型的激活值】和【失忆模型的激活值】相同，作为 baseline。具体的方式是不断地寻找图片，计算【原模型和失忆模型的 KL 散度】，然后做【梯度下降】。<p>第三步：在 baseline 和原始图片中插值，获得光滑的路径。</blockquote><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251003085707-nbpqit1.png>​<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251003090156-u9mqbl7.png>​<p><strong>Desirable Path Features</strong>:<ul><li>Proximity (邻近性): We aim for a smooth transition between absence and presence of features, and this intuitively cannot be achieved if the baseline and input are too far away. In UNI, we bound the distant $\|x - x^\prime\|$ to a certain value $\epsilon$.<li>Low Curvature: Assume that the function $g:\alpha\in[0,1]\mapsto {\nabla}F_{c}\left(x^{{\prime}}+\alpha(x-x^{\prime})\right)$ is $\mathcal{C}^1$ on the segment $[x^\prime, x]$, the error in the Riemann approximation of the attribution is</ul><p>$$ \left|(x_i-x_i^{\prime})\int_{\alpha=0}^1g\left(\alpha\right)d\alpha-\frac{(x_i-x_i^{\prime})}{B}\sum_{k=1}^Bg\left(\frac{k}{B}\right)\right|\leq\frac{M||x-x^{\prime}||^2}{2B} $$<p>where the lhs indicates the numerical integration error, while $M$ is proportional to the curvature, and $B$ is number of steps. A lower value of the constant $M$ implies a lower error in the integration calculation.<ul><li>Monotonic: Intuitively, the path from the “featureless” baseline to the input image should be monotonically increasing in output class confidence. The unlearning mechanism naturally meets this condition.</ul><p>‍<figure><img alt=image src=assets/image-20251004092624-945zj5b.png><figcaption>Illustration of unlearning: The x-axis is the input and y-axis is the output. &amp;quot;Data points&amp;quot; are the objective to fit, the original model is mixed by three gaussians functions, &amp;quot;model&amp;quot; indicates the output of the model. (a) shows the model after one gradient ascent step on the datapoint (2,5). (b) shows the model trained on the three datapoints. The path between UNI baseline and the image is highlighted by arrows in (b).</figcaption></figure>​ <hr><h2 id=experiments>Experiments</h2><p><strong>Faithfulness</strong>:<ul><li>We report the MuFidelity scores, i.e., the faithfulness of an attribution function $\mathcal{A}$, to a model $F$, at a sample $x$, for a subset of features of size $|S|$, given by</ul><p>$$ \mu_{f}(F,\mathcal{A};x) = \mathrm{corr}_{S\in \begin{pmatrix} [d] \\ |S| \end{pmatrix}}\left(\sum_{i\in S}\mathcal{A}(i,F,c,x),F_{c}(x)-F_{c}(x_{[x_{s}=\bar{x}_{s}]})\right) $$<blockquote><p>$\sum_{i\in S}\mathcal{A}(i,F,c,x)$ 为归因方法的【预测重要性】，$F_{c}(x)-F_{c}(x_{[x_{s}=\bar{x}_{s}]})$ 为像素的【实际影响力】</blockquote><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251004093218-x8zylow.png>​<ul><li>We also report deletion and insertion scores, which measures the decrease (deletion) or increase (insertion) of a model’s output confidence as salient pixels are removed or inserted.</ul><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251004095016-nuafpb7.png>​<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251004095030-rh2b43l.png>​<p><strong>Robustness</strong>: We design norm-bounded attacks to miximize the disagreement in attributions while constraining the prediction label unchanged.<p>$$ \begin{gathered} \delta_{f}^{*}=\operatorname{arg}\operatorname*{max}_{\|\delta_{f}\|_{p}\leq\varepsilon_{f}}\frac{1}{d_{X}}\sum_{i=1}^{d_{X}}d\left(\mathcal{A}(i,F,c,x),\mathcal{A}(i,F,c,x+\delta_{f})\right) \\ \mathrm{subject~to}\quad\arg\operatorname*{max}_{c^{\prime}}F_{c^{\prime}}(x)=\arg\operatorname*{max}_{c}F_{c^{\prime}}(x+\delta_{f})=c \end{gathered} $$<blockquote><p>热力图的鲁棒性指的是当输入图片增加一个微小到人眼无法识别的变化时，好的解释方法的热力图保持稳定。这里采用了对抗攻击（adversarial attacks）的方法，在不改变模型最终预测结果的前提下，最大化改变归因热力图。<p>Table 5 展示了攻击前和攻击后两个热力图的相关性<p>Table 6 展示了攻击前最重要的 1000 个 pixel 和攻击后的重合比例</blockquote><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251004095304-j2uwyll.png>​<p><strong>Stability</strong>: We compare UNI and other method’s sensitivity to Riemann approximation noise<blockquote><p>IG 类方法从 baseline 出发通过多个 step 到达原始图片，理论上 step 越多越精确，但如果不稳定，则【走多少步】和【从哪个 baseline 出发】会影响最终热力图结果。</blockquote><p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/assets/image-20251004095538-9nd424y.png>​</section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/#introduction>Introduction</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/#notation>Notation</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/#limitation-of-existing-gradient-based-methods>Limitation of Existing Gradient-Based Methods</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/#uni-unlearning-based-neural-interpretations>UNI: Unlearning-Based Neural Interpretations</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-unlearning-based-neural-interpretations/#experiments>Experiments</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://reichtumqian.pages.dev/katex.min.css rel=stylesheet><script defer src=https://reichtumqian.pages.dev/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://reichtumqian.pages.dev/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://reichtumqian.pages.dev/atom.xml> <img alt=feed loading=lazy src=https://reichtumqian.pages.dev/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>