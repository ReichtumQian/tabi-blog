<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://reichtumqian.pages.dev name=base><title>
Reichtum's Blog • 论文阅读-Recall and Learn Fine Tuning with Less Forgetting</title><link title="Reichtum's Blog - Atom Feed" href=https://reichtumqian.pages.dev/atom.xml rel=alternate type=application/atom+xml><link href="https://reichtumqian.pages.dev/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://reichtumqian.pages.dev/main.css?h=28b3fcbb58f2f22a8c44" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Reichtum's Blog" name=description><meta content="Reichtum's Blog" property=og:description><meta content="论文阅读-Recall and Learn Fine Tuning with Less Forgetting" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://reichtumqian.pages.dev/blog/reading-recall-and-learn-fine-tuning-with-less-forgetting/ property=og:url><meta content="Reichtum's Blog" property=og:site_name><noscript><link href=https://reichtumqian.pages.dev/no_js.css rel=stylesheet></noscript><script src=https://reichtumqian.pages.dev/js/initializeTheme.min.js></script><script defer src=https://reichtumqian.pages.dev/js/themeSwitcher.min.js></script><script src="https://reichtumqian.pages.dev/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunr.zh.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://reichtumqian.pages.dev/>Reichtum's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/projects/>projects </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">论文阅读-Recall and Learn Fine Tuning with Less Forgetting</h1><a class="u-url u-uid" href=https://reichtumqian.pages.dev/blog/reading-recall-and-learn-fine-tuning-with-less-forgetting/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://reichtumqian.pages.dev rel=author title=Reichtum>Reichtum</a> </span><li><time class=dt-published datetime=2025-08-28>28th Aug 2025</time><li title="832 words"><span aria-hidden=true class=separator>•</span>5 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/fine-tuning/>Fine Tuning</a>, <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/catastrophic-forgetting/>Catastrophic Forgetting</a>, <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/machine-learning/>Machine Learning</a></ul><section class="e-content body"><blockquote><p>Original Paper: <a href=https://arxiv.org/abs/2004.12651>[2004.12651] Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting</a></blockquote><hr><h2 id=introduction>Introduction</h2><p><strong>Sequential Transfer Learning</strong>: Pretrain a language model on large-scale unlabeled data and then adapt it to downstream tasks. The adaptation step is usually conducted in two manners: <u>fine-tuning</u> or <u>freezing pretrained weights</u> (e.g., train an additional classification head).<p><strong>Catastrophic Forgetting</strong>: Such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem, where a model forgets previously learned knowledge and overfits to target domains.<p><strong>Multi-task Learning</strong>: Learns multiple tasks simultaneously to avoid catastrophic forgetting.<ul><li>Elastic Weight Consolidation (EWC): EWC evaluates the importance of each weight by <u>Fisher information matrix</u>, and punish the change on these important weights when adapting on subsequent tasks.</ul><p><strong>Challenge in LLM Fine-Tuning</strong>: Multi-task learning methods cannot be directly applied to the sequential transferring regime of deep pretrained LMs.<ul><li>Multi-task learning methods require to use data of pretraining tasks during adaptation.<li>We only care about the performance of the downstream task, while multi-task learning also aims to promote performance on pretraining tasks.</ul><hr><h2 id=contribution-of-this-work>Contribution of This Work</h2><p><strong>Recall and Learn Mechanism</strong>: We propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks.<ul><li>Pretraining Simulation: recall the knowledge from pretraining tasks <u>without data</u>.<li>Objective Shifting: focus the learning on downstream tasks gradually.</ul><p><strong>Recall Adam (RECADAM)</strong> : We provide RECADAM to integrate the recall and learn mechanism into Adam optimizer.<hr><h2 id=methodology>Methodology</h2><p><strong>Pretraining Simulation</strong>: We introduce Pretraining Simulation to approximate the optimization objective of source tasks as a quadratic penalty. The learning objective on the source tasks $\text{Loss}_S$ is approximately<p>$$ \text{Loss}_S \approx \frac{1}{2}\gamma \sum_i (\theta_i - \theta_i^\ast)^2, $$<p>where $\theta^\ast$ is the pretrained parameter, and $\gamma$ is a constant.<p><strong>Objective Shifting</strong>: We introduce Objective Shifting to allow the objective function to gradually shift to $\text{Loss}_T$ with the annealing coefficient. The loss function with annealing coefficient is<p>$$ \text{Loss} = \lambda(t) \text{Loss}_T + (1 - \lambda(t)) \text{Loss}_S. $$<p>Specifically, $\lambda(t)$ is calculated as the sigmoid annealing function:<p>$$ \lambda(t)=\frac{1}{1+\exp(-k\cdot(t-t_0))}. $$<figure><img alt=image src=assets/image-20250828151221-bcbjoaf.png><figcaption>At the beginning of the training process, the model mainly focuses on pretraining tasks. As training proceeds, the model gradually focuses on target tasks.</figcaption></figure>​ <p><strong>RecAdam Optimizer</strong>: We introduce RecAdam to integrate the quadratic penalty (Pretraining Simulation) and annealing coefficient (Objective Shifting). The difference between Adam and RecAdam lies in decoupling the quadratic penalty and annealing coefficient in Adam optimizer. In vanilla Adam, both the quadratic penalty and annealing coefficient would be adapted by the gradient update rules.<figure><img alt=image src=assets/image-20250828152052-woujadl.png><figcaption>The comparison between Adam and RecAdam, where SetScheduleMultiplier(t) refers to the preceduer (e.g., warm-up technique) to get the scaling factor of the step size.</figcaption></figure><hr><h2 id=experiments>Experiments</h2><p><strong>Set up</strong>​<ul><li><strong>Model</strong>: BERT and ALBERT;<li><strong>Data</strong>: General Language Understanding Evaluation (GLUE), it includes 9 tasks.<li><strong>Implementation</strong>: Our methods use <u>random initialization</u> (do not load the pretrained paramters) while vanilla fine-tuning initializes the fine-tuning model with pretrained parameters.<li><strong>Hyper-Parameters</strong>: We set $\gamma$ to $5000$, select the best $t_0$ and $k$ in $\{100, 250, 500, 1000\}$ and $\{0.05, 0.1, 0.2, 0.5, 1\}$ respectively for the annealing coefficient $\lambda(t)$.</ul><p><strong>Results on BERT-Base</strong>: We outperform the vanilla fine-tuning method on $7$ out of $8$ tasks, especially on the tasks with smaller training data (&lt;10k). It is interesting to find that compared to the median results with BERT-large model, we can also achieve better results on more than half of the tasks.<p><strong>Results on ALBERT-xxlarge</strong>: We outperform the vanilla fine-tuning method on 5 out of 8 tasks of the GLUE benchmark.<p><img alt=image src=https://reichtumqian.pages.dev/blog/reading-recall-and-learn-fine-tuning-with-less-forgetting/assets/image-20250828153601-zdnf97z.png><p><strong>Initialization Analysis</strong>: RECADAM, with both initialization strategies, can outperform the vanilla fine-tuning method on all the four tasks. Random initialization would be our choice because the model would benefit from a larger parameter search space.<figure><img alt=image src=assets/image-20250828155549-c5cefn1.png><figcaption>Comparison of different model initialization strategies: pre-trained initialization (PI) and Random Initialization (RI). We report median over 5 runs.</figcaption></figure><p><strong>Forgetting Analysis</strong>: The hyper-parameter $k$ controls the rate of the objective shifting<ul><li>Target Loss: With larger k, the model converges quickly on the target task.<li>Source Loss: We measure the pre-trained knowledge forgetting by the Euclidean distance between $\theta_0$ and $\theta$. At the very early stage, the distance drops sharply because of the random initialization and pre-trained knowledge recalling. As the objective rate $k$ decreases, we find that the model can achieve less forgetting at the end of the fine-tuning.</ul><figure><img alt=image src=assets/image-20250828155928-bpiioys.png><figcaption>Learning curves with different k values:</figcaption></figure><hr><p>‍</section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://reichtumqian.pages.dev/blog/reading-recall-and-learn-fine-tuning-with-less-forgetting/#introduction>Introduction</a><li><a href=https://reichtumqian.pages.dev/blog/reading-recall-and-learn-fine-tuning-with-less-forgetting/#contribution-of-this-work>Contribution of This Work</a><li><a href=https://reichtumqian.pages.dev/blog/reading-recall-and-learn-fine-tuning-with-less-forgetting/#methodology>Methodology</a><li><a href=https://reichtumqian.pages.dev/blog/reading-recall-and-learn-fine-tuning-with-less-forgetting/#experiments>Experiments</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://reichtumqian.pages.dev/katex.min.css rel=stylesheet><script defer src=https://reichtumqian.pages.dev/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://reichtumqian.pages.dev/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://reichtumqian.pages.dev/atom.xml> <img alt=feed loading=lazy src=https://reichtumqian.pages.dev/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>