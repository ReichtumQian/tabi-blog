<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://reichtumqian.pages.dev name=base><title>
Reichtum's Blog • 论文阅读-Benchmarking Optimizers for Large Language Model Pretraining</title><link title="Reichtum's Blog - Atom Feed" href=https://reichtumqian.pages.dev/atom.xml rel=alternate type=application/atom+xml><link href="https://reichtumqian.pages.dev/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://reichtumqian.pages.dev/main.css?h=28b3fcbb58f2f22a8c44" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Reichtum's Blog" name=description><meta content="Reichtum's Blog" property=og:description><meta content="论文阅读-Benchmarking Optimizers for Large Language Model Pretraining" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/ property=og:url><meta content="Reichtum's Blog" property=og:site_name><noscript><link href=https://reichtumqian.pages.dev/no_js.css rel=stylesheet></noscript><script src=https://reichtumqian.pages.dev/js/initializeTheme.min.js></script><script defer src=https://reichtumqian.pages.dev/js/themeSwitcher.min.js></script><script src="https://reichtumqian.pages.dev/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunr.zh.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://reichtumqian.pages.dev/>Reichtum's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/projects/>projects </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">论文阅读-Benchmarking Optimizers for Large Language Model Pretraining</h1><a class="u-url u-uid" href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://reichtumqian.pages.dev rel=author title=Reichtum>Reichtum</a> </span><li><time class=dt-published datetime=2025-09-13>13th Sep 2025</time><li title="1082 words"><span aria-hidden=true class=separator>•</span>6 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/llm/>LLM</a>, <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/machine-learning/>Machine Learning</a></ul><section class="e-content body"><blockquote><p>Original Paper: <a href=https://arxiv.org/abs/2509.01440>[2509.01440] Benchmarking Optimizers for Large Language Model Pretraining</a></blockquote><hr><h2 id=introduction>Introduction</h2><p><strong>Chinchilla Scaling Law</strong>: The optimal amount of training data for a given model size that yields the best performance under a fixed computational budget. To be more specific, we <u>need around 20 text tokens per parameter</u> (see <a href=https://arxiv.org/pdf/2203.15556>2203.15556</a>)<p><strong>Overview</strong>: We discuss the algorithms according to their logical grouping:<ul><li>Adam-like methods: <code>AdamW</code>​, <code>ADOPT</code>​, <code>AdEMAMix</code>​<li>Sign-based methods: <code>Lion</code>​, <code>Signum</code>​<li>Approximate second-order optimizers: <code>Muon</code>​, <code>SOAP</code>​, <code>Sophia</code>​<li>Learning rate/ scheduler-free learning algorithms: <code>Schedule-Free AdamW</code>​, <code>Prodigy</code>​<li>MARS methods: <code>MARS</code>​</ul><hr><h2 id=results-at-small-scale-124m-models>Results at Small Scale: 124M Models</h2><p><strong>Results with Small and Large Batches</strong> and <strong>Stability across Training Horizons</strong><figure><img alt=image src=assets/image-20250912181322-uddtshy.png><figcaption>Comparing optimizers for training a 124M parameter LLM: (a) &amp;quot;small&amp;quot; batch size (b) &amp;quot;large&amp;quot; batch size.</figcaption></figure>​ <figure><img alt=image src=assets/image-20250908175428-l9hiovd.png><figcaption>Ranking of optimizers for 124M models with &amp;quot;small&amp;quot; and &amp;quot;large&amp;quot; batch sizes.</figcaption></figure>​ <blockquote><p><strong>Takeaway (Batch Size)</strong><ul><li><code>AdEMAMix</code>​ consistently achieves state-of-the-art performance and robust scaling with training duration.<li>Sign-based methods (<code>Signum</code>​, <code>Lion</code>​) and <code>MARS</code>​ greatly benefit from the <u>increased batch size</u>.<li><code>Sophia</code>​ diverges in small-batch setting, when trained beyond the Chinchilla optimal horizon, even with sufficiently small learning rate;<li><code>SOAP</code>​ show a consistent performance in both settings.</ul><p><strong>Takeaway (Stability)</strong> : Once optimizers are properly re-tuned for the maximal length of training considered, doubling of number of iterations does not affect the ranking of methods.</blockquote><hr><p><strong>Increasing the Batch Size Further</strong>:<figure><img alt=image src=assets/image-20250912182035-3u788qq.png><figcaption>Scaling batch size vs. scaling the number of iterations</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>: Many methods, especially <code>MARS</code>​, <code>Prodigy</code>​, and <code>sign-based</code>​ ones, can outperform <code>AdamW</code>​ while trained on a sufficiently large batches.</blockquote><hr><p><strong>Weight Decay Ablation</strong>:<figure><img alt=image src=assets/image-20250912182611-33p1mpd.png><figcaption>Larger weight decay achieves significantly better results when training on fewer tokens: (a) AdamW, Signum, Lion with large weight decay outperform baseline AdamW with weight decay of 0.1 for short training duration. (b) the setting without weight decay is suboptimal. (c) Smaller weight decay leads to larger L2 norm of the model parameter.</figcaption></figure>​ <figure><img alt=image src=assets/image-20250912182642-ieebifg.png><figcaption>Importance of weight decay for Muon. (1) D-Muon uses a weight decay for all parameters, (2) Muon uses weight decay only on embeddings, scalar parameters, and the final layer. We can see that D-Muon greatly outperforms the basic Muon.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>:<ul><li>The use of weight decay (particularly a large weight decay term 0.5 and above), can significantly impact the final loss and optimizer behavior.<li>The setting of weight decay to be $0$ is suboptimal.<li>For extended training horizons, non-zero weight of $0.1$ proves to be a robust option.</ul></blockquote><hr><p><strong>Learning Rate Sensitivity</strong>:<figure><img alt=image src=assets/image-20250912194544-c9ym1ai.png><figcaption>Optimal learning rate stability across optimizers. The optimal learning rate determined during tuning on 2.1B tokens remains consistent after a learning rate sweep on 16.8B tokens for most optimizers.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>:<ul><li>For most optimizer, the learning rate $\gamma_{\max}$ selected near the Chinchilla optimal horizon transfers smoothly to $8 \times$longer run.<li>Sign-based methods and <code>Sophia</code>​ diverge with $\gamma_{\max} = 2e^{-3}$.<li><code>MARS</code>​ demonstrates a very consistent performance across $\gamma$ sweep.</ul></blockquote><hr><p><strong>Warmup Ablation</strong>:<figure><img alt=image src=assets/image-20250912195833-rxrm1vz.png><figcaption>Warmup ablation: sign-based optimizers, Sophia and SF-AdamW benefit from the increased warmup.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>: We reveal that the warmup duration is optimizer-dependent and should be tuned: for <code>SF-AdamW</code>​, <code>Sophia</code>​, and <code>Signum</code>​, longer warmup results in improved final performance.</blockquote><hr><p><strong>Warmup Types of WSD, cosine, and linear</strong> **$\gamma$**​ <strong>-scheduler</strong>:<figure><img alt=image src=assets/image-20250913155616-x3m2l3m.png><figcaption>Comparisons between cosine, WSD, and the linear schedulers.</figcaption></figure>​ <figure><img alt=image src=assets/image-20250913160100-ljc3uwl.png><figcaption>Gradient norm patterns for different schedulers: (b) the gradient evolution for majority of optimizers resembles the SF-AdamW pattern (a,c) Exceptions are sign-based methods: Signum and Lion.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>: A choice of the learning rate scheduler is also optimizer-related<ul><li>For most methods, the cosine scheduler dominates.<li>Linear scheduler outperforms or matches cosine and WSD for sign-based methods, <code>SOAP</code>​ and <code>MARS</code>​.<li>WSD appears to be the best option for <code>Muon</code>​</ul></blockquote><hr><h2 id=results-at-medium-scale-210m-models>Results at Medium Scale: 210M Models</h2><p><strong>Results</strong>​<figure><img alt=image src=assets/image-20250913160454-797022m.png><figcaption>Ranking of optimizers for 210M models with the batch size of 256*512 tokens.</figcaption></figure>​ <figure><img alt=image src=assets/image-20250913160605-3jwtcqf.png><figcaption>Comparing optimizers for training a 219M parameter LLM.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>:<ul><li>We do not observe a much of a change in ranking of optimizers for 210M model, compared to benchmarking on 124M.<li>We replicated almost identical hyperparameters for all optimizers, except for the learning rate for sign-based methods (which is more sensitive to the learning rate while scaling the model size)</ul></blockquote><hr><p><strong>Decay the learning rate sufficiently</strong><figure><img alt=image src=assets/image-20250913161002-rhf7rcr.png><figcaption>Decaying the learning rate down to 0.01 and beyond, instead of only to 0.1</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>: Decaying the learning rate further than $10\%$ of the maximal significantly improves the results. However, for different schedulers, the best final learning rate is different.</blockquote><hr><h2 id=results-at-large-scale-583m-and-720m-parameters>Results at Large Scale: 583M and 720M Parameters</h2><p><strong>Results</strong><figure><img alt=image src=assets/image-20250913161558-oyap0pm.png><figcaption>Ranking of optimizers for 720M Llama-based models.</figcaption></figure>​ <figure><img alt=image src=assets/image-20250913161416-9a93emu.png><figcaption>Comparing optimizers for training a 720M parameter LLM.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>:<ul><li>At larger scale of model and batch size, <code>AdEMAMix</code>​ and <code>MARS</code>​ dominate.<li>Despite training with large batches, <code>Signum</code>​ and <code>Lion</code>​ scale poorly.<li><code>D-Muon</code>​ is consistent across all our benchmarking setups.</ul></blockquote><hr><p><strong>Wall-clock time comparison</strong>​<figure><img alt=image src=assets/image-20250913161744-b9b1nwq.png><figcaption>Wall-clock time comparison.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>: Most optimizers exhibit similar wall-time performance, with sign-based methods being slightly faster. <code>SOAP</code>​ is the main exception.</blockquote><hr><h2 id=extension-to-moes>Extension to MoEs</h2><figure><img alt=image src=assets/image-20250913162006-wshll9d.png><figcaption>Ranking optimizers for 520M MoE models with 256*512 batch size.</figcaption></figure>​ <figure><img alt=image src=assets/image-20250913162059-zjiwg4o.png><figcaption>Comparing optimizers for training a 520M parameter MoE.</figcaption></figure>​ <blockquote><p><strong>Takeaway</strong>: Benchmarking results obtained for dense models transfer to corresponding MoEs.</blockquote><p>‍</section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/#introduction>Introduction</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/#results-at-small-scale-124m-models>Results at Small Scale: 124M Models</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/#results-at-medium-scale-210m-models>Results at Medium Scale: 210M Models</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/#results-at-large-scale-583m-and-720m-parameters>Results at Large Scale: 583M and 720M Parameters</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/#extension-to-moes>Extension to MoEs</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://reichtumqian.pages.dev/katex.min.css rel=stylesheet><script defer src=https://reichtumqian.pages.dev/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://reichtumqian.pages.dev/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://reichtumqian.pages.dev/atom.xml> <img alt=feed loading=lazy src=https://reichtumqian.pages.dev/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>