<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="default-src 'self';font-src 'self' data:;img-src 'self' https://* data:;media-src 'self';style-src 'self';frame-src player.vimeo.com https://www.youtube-nocookie.com;connect-src 'self';script-src 'self' 'self'" http-equiv=Content-Security-Policy><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://reichtumqian.pages.dev name=base><title>
Reichtum's Blog • 论文阅读：The Super Weight in Large Language Models</title><link title="Reichtum's Blog - Atom Feed" href=https://reichtumqian.pages.dev/atom.xml rel=alternate type=application/atom+xml><link href="https://reichtumqian.pages.dev/custom_subset.css?h=0b9535a28bc3d5bf2321" rel=stylesheet><link href="https://reichtumqian.pages.dev/main.css?h=28b3fcbb58f2f22a8c44" rel=stylesheet><meta content="light dark" name=color-scheme><meta content="Reichtum's Blog" name=description><meta content="Reichtum's Blog" property=og:description><meta content="论文阅读：The Super Weight in Large Language Models" property=og:title><meta content=article property=og:type><meta content=en_GB property=og:locale><meta content=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/ property=og:url><meta content="Reichtum's Blog" property=og:site_name><noscript><link href=https://reichtumqian.pages.dev/no_js.css rel=stylesheet></noscript><script src=https://reichtumqian.pages.dev/js/initializeTheme.min.js></script><script defer src=https://reichtumqian.pages.dev/js/themeSwitcher.min.js></script><script src="https://reichtumqian.pages.dev/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunrStemmerSupport.min.js></script><script defer src=https://reichtumqian.pages.dev/js/lunr/lunr.zh.min.js></script><body><header><nav class=navbar><div class=nav-title><a class=home-title href=https://reichtumqian.pages.dev/>Reichtum's Blog</a></div><div class=nav-navs><ul><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/blog/>blog </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/archive/>archive </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/tags/>tags </a><li><a class="nav-links no-hover-padding" href=https://reichtumqian.pages.dev/projects/>projects </a><li class=menu-icons-container><ul class=menu-icons-group><li class="js menu-icon"><div aria-label="Press $SHORTCUT to open search" class="search-icon interactive-icon" title="Press $SHORTCUT to open search" id=search-button role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><li class="theme-switcher-wrapper js"><div aria-label="Toggle dark mode" title="Toggle dark/light mode" aria-pressed=false class=theme-switcher role=button tabindex=0></div><div aria-label="Reset mode to default" class="theme-resetter arrow" title="Reset mode to default" aria-hidden=true role=button tabindex=0></div></ul></ul></div></nav></header><div class=content><main><article class=h-entry><h1 class="p-name article-title">论文阅读：The Super Weight in Large Language Models</h1><a class="u-url u-uid" href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/></a><ul class=meta><span class="hidden p-author h-card"> <a class=u-url href=https://reichtumqian.pages.dev rel=author title=Reichtum>Reichtum</a> </span><li><time class=dt-published datetime=2025-09-06>6th Sep 2025</time><li title="1823 words"><span aria-hidden=true class=separator>•</span>10 min read<li class=tag><span aria-hidden=true class=separator>•</span>Tags: <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/llm/>LLM</a>, <li class=tag><a class=p-category href=https://reichtumqian.pages.dev/tags/machine-learning/>Machine Learning</a></ul><section class="e-content body"><blockquote><p>Original Paper: <a href=https://arxiv.org/abs/2411.07191>[2411.07191] The Super Weight in Large Language Models</a></blockquote><hr><h2 id=introduction>Introduction</h2><p><strong>Large Outliers in Large Models</strong>: Once LLMs reach a certain scale, a small set of hidden state features contains outliers of exceptionally large magnitude. These outliers account for a small percentage of all activations but are crucial for preserving the compressed model’s quality.<p><strong>Super Weights</strong>: Not all large outliers are equally important. In this paper, we study a tiny yet important set of outliers in LLMs, termed <em>super weights</em>. In Llama-7B, pruning the super weight, a single scalar, completely destroys the model’s ability to generate text.<figure><img alt=image src=assets/image-20250906104045-xl09xfx.png><figcaption>Super Weight Phenomenon: Pruning a single super weight can completely destroy a LLM's ability to generate text. On the left, the original Llama-7B produces a reasonable completion. On the right, after pruning the super weight, Llama-7B generates complete gibberish.</figcaption></figure>​ <p><strong>Super Activations</strong>: <em>Super activations</em> are exceptionally massive activations. They persist across many layers, feature constant magnitude, and always exist at the same position regardless of input.<figure><img alt=image src=assets/image-20250906111713-x3jh79f.png><figcaption>Super activations: Exceptionally massive activations</figcaption></figure>​ <p><strong>Super Weights behave similarly across model families and sizes</strong>:<ul><li>They are always found in the <code>mlp.down_proj</code>​ weight.<li>They produce exceptionally large magnitude activation–the <em>super activation</em>.<li>They suppress stopword likelihood.<li>Pruning the super weight destroys quality by <u>dampening the super activation</u>（super activation 几乎消失了） and <u>shifting almost all logit probability mass to stopwords</u>（几乎只输出 stopwords）.</ul><figure><img alt=image src=assets/image-20250906111816-u8ibtvm.png><figcaption>How Super Weights behave: (1) Super weights are often found in an early layer's down projection (2) Super activations are propagated through skip connections. (3) This has a effect of suppressing stopword likelihoods in the final logits.</figcaption></figure>​ <blockquote><p><code>mlp.down_proj</code>​：在 Transformer 架构中，Feed Forward 层（也就是 MLP 层）一般是两层神经网络，表示为 $\operatorname{FFN}(x) = W_2(\operatorname{ReLU}(W_1x + b_1)) + b_2)$，即先经过一个 <code>up_proj</code>​ 进行升维，一个 <code>activation</code>​ 进行非线性变换，再经过一个 <code>down_proj</code>​ 进行降维。<p><img alt=image src=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/assets/image-20250906103003-ltmazal.png>​<p>Stopword 指的是那些非常常见，但是信息量很低的词，例如 <code>the</code>​、<code>a</code>​、<code>is</code>​、<code>of</code>​ 等。</blockquote><p><strong>Super Outliers</strong>: We refer to both super weights and super activations as <em>super outliers</em>, which are critical to model quality. Fortunately, there are no more than a handful of scalar super outliers per tensor.<hr><h2 id=contribution-of-this-work>Contribution of This Work</h2><p><strong>Super Weights</strong>: We discover a tiny subset of outliers in LLMs, at most six scalars, that are disproportionately important; pruning these super weights destroys model quality.<p><strong>Identifying Super Weights</strong>: We present a data-free way to identify super weights using only a single forward pass and provide an index of super weights for existing, open LLMs.<p><strong>Super Activations</strong>: We analyze how super weights influence inference and relate them to the activation outliers observed in prior work.<p><strong>Compression</strong>: By preserving super outliers, we show that round-to-nearest quantization increases effectiveness noticeably; preserving super outliers improves compression quality.<hr><h2 id=identification-of-super-weights>Identification of Super Weights</h2><p><strong>Super Weights Create Super Activations</strong>: The super activations’ channel (the position in vector) aligns with the super weights’, and the activation first appears right after the super weights.<figure><img alt=image src=assets/image-20250906105604-xtl5z4e.png><figcaption>Pruning the super weight decreases the super activation's magnitude by 75%.</figcaption></figure>​ <p><strong>The Mechanism behind Super Activations</strong>: The Hadamard product of the <code>gate</code>​ and <code>up</code>​ projection creates a relatively large activation. The super weights further amplify it and create super activations.<blockquote><p>Gate 指的是对输入信息进行过滤和放缩的操作，假设 <code>A</code>​ 是输入向量；<code>g</code>​ 是与 <code>A</code>​ 等长的门向量，其元素在 <code>0~1</code>​ 之间。那么 gate 操作后的输出为 <code>B = A ⊙ g</code>​，其中 <code>⊙</code>​ 是逐元素（Hadamard）乘法。<p>虽然经典 Transformer 架构中的 FFN 层没有 gate 层，但是现代 Transformer，如 Llama、Mistral 等模型中有。</blockquote><p><strong>Identifying Super Weight by Activation Spikes</strong>: Super weights can be located by detecting the spikes in the <code>down_proj</code>​ inputs and outputs distributions across the layers. This dectection only requires a single input prompt, rather than a set of validation data or use-case examples.<p><strong>Identifying Steps</strong>: Let $W \in \mathbb{R}^{D \times H}$ be the <code>down_proj</code>​ weight matrix, where $D$ is the dimension of the activation feature and $H$ is the intermediate hidden dimension. Let $X \in \mathbb{R}^{L \times H}$ be the input matrix, where $L$ is the sequence length. Then the output of <code>down_proj</code>​ is<p>$$ Y = XW^\top, \quad \text{where} \quad Y_{ij} = \sum_{k=1}^d X_{ik}W_{jk}. $$<p>Suppose $Y_{ij}$ is a super activation, $X_{ik}$ and $W_{jk}$ are outliers, then $Y_{ij} \approx X_{ik}W_{jk}$.<ul><li>Plot extreme outliers in the input and output activations of <code>mlp.down_proj</code>​.<li>Determine the layer and coordinates of the super weights.<li>Remove detected super weights and repeat the above process, until the magnitudes of large maximum activations are greatly suppressed.</ul><figure><img alt=image src=assets/image-20250906111713-x3jh79f.png><figcaption>How to identify the super weights: The input has a large activation on layer 2. The value's channel index tells the row of super weight. The output has a large activation at layer 2. This value's channel index gives us the column of the super weight.</figcaption></figure>​ <hr><h2 id=mechanisms-of-super-weights>Mechanisms of Super Weights</h2><p><strong>Super Weights (partially) Operate via Super Activations</strong>: We want to assess the super weight’s impact on model quality is solely mediated by super activations or other factors. We conduct experiments under three conditions:<ul><li>Original model<li>Remove super weights (Prune SW): Set the weight scalar as zero.<li>Remove super weights and restore super activation (Prune SW, +SA): Set the weight scalar as zero, and restore super activation at the layer where it first appears.</ul><p>The results show that super activations contribute substantially to the model’s performance, they do not fully account for the super weight’s overall influence on quality.<figure><img alt=image src=assets/image-20250906151402-b1r818u.png><figcaption>Super Weight Importance: &amp;quot;Prune SW&amp;quot; indicates pruning single super weight, &amp;quot;Prune Non-SW&amp;quot; indicates pruning other 7,000 largest-magnitude weights, &amp;quot;Prune SW,+SA&amp;quot; indicates pruning super weight but restoring super activation. The experiment is conducted to assess the model's accuracy on seven zero-shot datasets and perplexity on C4 and Wiki-2.</figcaption></figure>​ <p><strong>Super Weights Affect Output Token Probability Distributions</strong>:<figure><img alt=image src=assets/image-20250906153301-pkr83su.png><figcaption>Super Weights Suppress Stopwords: Removing super weights results in 2 to 5 times larger stopword probabilities, while non-stopwords decrease by 2 to 3 times.</figcaption></figure>​ <p><strong>Sensitivity of Super Weights</strong>: We investigate how does increasing the magnitude of super weights affect model quality.<figure><img alt=image src=assets/image-20250906154045-bid5jgo.png><figcaption>Amplifying Super Weight Improves Quality: There exists some scaling where quality is improved.</figcaption></figure>​ <hr><h2 id=super-outlier-aware-quantization>Super-Outlier Aware Quantization</h2><p><strong>Quantization</strong>: The presence of outliers significantly degrade quantization quality. However, super outliers carry significant importance for model quality, making their preservation during<br> quantization critical.<p><strong>Round-to-Nearest Quantization</strong>: Here we consider the asymmetric round-to-nearest quantization<p>$$ Q(\mathbf{X})=\mathrm{Round}\left(\frac{\mathbf{X}-\mathrm{MIN}(\mathbf{X})}{\Delta}\right),Q^{-1}(\mathbf{\hat{X}})=\Delta\cdot\mathbf{\hat{X}}+\mathrm{MIN}(\mathbf{X}) $$<p>where $\mathbf{X}$ is the tensor to be quantized, $\mathrm{MIN}(\mathbf{X})$ is the smallest element in $\mathbf{X}$, and $\Delta=\frac{\mathrm{MAX}(\mathbf{X})-\mathrm{MIN}(\mathbf{X})}{2^{N-1}-1}$ is the quantization step with $N$ being the number of bits. So super outliers in $\mathbf{X}$ drastically increase the step size, increasing the quantization error.<blockquote><p>量化步长 $\Delta$ 衡量了量化后两个相邻离散值之间的距离，可以类比于图像的分辨率。假设 $\mathbf{X}$ 中大部分值都在 $[-1.0, 1.0]$ 区间内，突然出现了一个超大的离群值（outlier），这会导致量化步长 $\Delta$ 激增，从而导致精度下降严重。</blockquote><p><strong>Solution to Outlier Quantization</strong>:<ul><li>Hold out the super outlier to prevent adverse effects on inlier quantization.<li>Restore the super outlier’s value after dequantization.</ul><p><strong>Activation Quantization</strong>: We replace the super activation with the median, then quantize, dequantize and restore it.<p>$$ \hat{A}=\mathrm{RESTORE}(Q^{-1}(Q(\mathrm{REPLACE}(A))) $$<p><strong>Weight Quantization</strong>: First, we identify super weights. Second, we clip the outlier weights, quantize, and dequantize the clipped weights. Third, restore the half-precision super weights after dequantization.<p>$$ \hat{W}=\mathrm{RESTORE}(Q^{-1}(Q(\mathrm{CLIP}_z(W))) $$<p>We parameterize clipping using a z-score.<blockquote><p>z-score 方法来自于统计学，给定一个阈值 <code>z</code>​，其衡量每个元素偏离平均值的程度。一旦某个元素的偏离值超过阈值 <code>z</code>​，则会被视作离群值。上面的 <code>CLIP</code>​ 操作即将离群值裁剪掉。</blockquote><figure><img alt=image src=assets/image-20250906175810-cc0bc8e.png><figcaption>Round-to-nearest with super-activation handling is competitive. Here, &amp;quot;Naive W8A8&amp;quot; indicates the naive round-to-nearest quantification, &amp;quot;SmoothQuant&amp;quot; is an advanced quantification method.</figcaption></figure>​ <hr><h2 id=experiments>Experiments</h2><p>We first evaluate the perplexity (PPL) of different quantization method for Wiki-2 and C4.<figure><img alt=image src=assets/image-20250906181755-c29ddf6.png><figcaption>Handling the super activation improves activation quantization: FP16 indicates the un-quantized model.</figcaption></figure>​ <p>We also evaluate the accuracy on zero-shot benchmarks.<figure><img alt=image src=assets/image-20250906182114-jbxzt7u.png><figcaption>Restoring super weight improves block scaling: Here &amp;quot;RTN&amp;quot; refers to &amp;quot;round-to-nearest&amp;quot;.</figcaption></figure>​ <blockquote><p>Block size 是量化中的一个概念，指的是为了更好地适应 weight tensor 的局部变化，将大的 weight tensor 切割为多个小块独立进行量化<ul><li>大 Block Size：量化粗超，但是高效<li>小 Block Size：量化精细，但是计算和存储开销大</ul></blockquote></section></article></main><div id=button-container><div id=toc-floating-container><input class=toggle id=toc-toggle type=checkbox><label class=overlay for=toc-toggle></label><label title="Toggle Table of Contents" class=button for=toc-toggle id=toc-button><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M414.82-193.094q-18.044 0-30.497-12.32-12.453-12.319-12.453-30.036t12.453-30.086q12.453-12.37 30.497-12.37h392.767q17.237 0 29.927 12.487 12.69 12.486 12.69 30.203 0 17.716-12.69 29.919t-29.927 12.203H414.82Zm0-244.833q-18.044 0-30.497-12.487Q371.87-462.9 371.87-480.45t12.453-29.92q12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.511 12.69 12.512 12.69 29.845 0 17.716-12.69 30.086-12.69 12.37-29.927 12.37H414.82Zm0-245.167q-18.044 0-30.497-12.32t-12.453-30.037q0-17.716 12.453-30.086 12.453-12.369 30.497-12.369h392.767q17.237 0 29.927 12.486 12.69 12.487 12.69 30.203 0 17.717-12.69 29.92-12.69 12.203-29.927 12.203H414.82ZM189.379-156.681q-32.652 0-55.878-22.829t-23.226-55.731q0-32.549 23.15-55.647 23.151-23.097 55.95-23.097 32.799 0 55.313 23.484 22.515 23.484 22.515 56.246 0 32.212-22.861 54.893-22.861 22.681-54.963 22.681Zm0-245.167q-32.652 0-55.878-23.134-23.226-23.135-23.226-55.623 0-32.487 23.467-55.517t56.12-23.03q32.102 0 54.721 23.288 22.62 23.288 22.62 55.775 0 32.488-22.861 55.364-22.861 22.877-54.963 22.877Zm-.82-244.833q-32.224 0-55.254-23.288-23.03-23.289-23.03-55.623 0-32.333 23.271-55.364 23.272-23.03 55.495-23.03 32.224 0 55.193 23.288 22.969 23.289 22.969 55.622 0 32.334-23.21 55.364-23.21 23.031-55.434 23.031Z"/></svg></label><div class=toc-content><div class=toc-container><ul><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/#introduction>Introduction</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/#contribution-of-this-work>Contribution of This Work</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/#identification-of-super-weights>Identification of Super Weights</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/#mechanisms-of-super-weights>Mechanisms of Super Weights</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/#super-outlier-aware-quantization>Super-Outlier Aware Quantization</a><li><a href=https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/#experiments>Experiments</a></ul></div></div></div><a title="Go to the top of the page" class=no-hover-padding href=# id=top-button> <svg viewbox="0 0 20 20" fill=currentColor><path d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z"/></svg> </a></div><link href=https://reichtumqian.pages.dev/katex.min.css rel=stylesheet><script defer src=https://reichtumqian.pages.dev/js/katex.min.js></script><span class=hidden id=copy-success> Copied! </span><span class=hidden id=copy-init> Copy code to clipboard </span><script defer src=https://reichtumqian.pages.dev/js/copyCodeToClipboard.min.js></script></div><footer><section><nav class="socials nav-navs"><ul><li><a class="nav-links no-hover-padding social" href=https://reichtumqian.pages.dev/atom.xml> <img alt=feed loading=lazy src=https://reichtumqian.pages.dev/social_icons/rss.svg title=feed> </a></ul></nav><nav class=nav-navs></nav><div class=credits><small> Powered by <a href=https://www.getzola.org>Zola</a> & <a href=https://github.com/welpo/tabi>tabi</a> </small></div></section><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><h1 class=visually-hidden id=modalTitle>Search</h1><div id=modal-content><div id=searchBar><div aria-hidden=true class=search-icon><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="M784-120 532-372q-30 24-69 38t-83 14q-109 0-184.5-75.5T120-580q0-109 75.5-184.5T380-840q109 0 184.5 75.5T640-580q0 44-14 83t-38 69l252 252-56 56ZM380-400q75 0 127.5-52.5T560-580q0-75-52.5-127.5T380-760q-75 0-127.5 52.5T200-580q0 75 52.5 127.5T380-400Z"/></svg></div><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search… role=combobox spellcheck=false><div class="close-icon interactive-icon" title="Clear search" id=clear-search role=button tabindex=0><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></div></div><div id=results-container><div id=results-info><span id=zero_results> No results</span><span id=one_results> 1 result</span><span id=many_results> $NUMBER results</span><span id=two_results> $NUMBER results</span><span id=few_results> $NUMBER results</span></div><div id=results role=listbox></div></div></div></div></footer>