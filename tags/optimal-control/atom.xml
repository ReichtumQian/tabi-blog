<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Optimal Control</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - Optimal Control</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/optimal-control/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/optimal-control/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-08-22T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/optimal-control/atom.xml</id><entry xml:lang="zh">
        <title>论文阅读：Method of Successive Approximations</title>
        <published>2025-08-22T00:00:00+00:00</published>
        <updated>2025-08-22T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-method-of-successive-approximations/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-method-of-successive-approximations/</id>
        
            <content type="html">&lt;blockquote&gt;
&lt;p&gt;Original Paper: &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.01299&quot;&gt;[1803.01299] An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-optimal-control-viewpoint&quot;&gt;The Optimal Control Viewpoint&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Neural Networks as Dynamical System&lt;&#x2F;strong&gt;: Let $T \in \mathbb{Z}_+$ denote the number layers and $\{x_{s,0} \in \mathbb{R}^{d_0}: s = 0, 1, \cdots, S\}$ represent $S + 1$ inputs (images, time-series). Consider the dynamical system&lt;&#x2F;p&gt;
&lt;p&gt;$$
x_{s,t+1}=f_t(x_{s,t},\theta_t),\quad t=0,1,\ldots,T-1,
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $f_{t}:\mathbb{R}^{d_{t}}\times\Theta_{t}\to\mathbb{R}^{d_{t+1}}$ is a transformation on the state, $\Theta_t$ is the trainable parameter set.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Objective of Training&lt;&#x2F;strong&gt;: The goal of training is to adjust the weights $\bm{\theta} := \{\theta_t: t = 0, 1, \cdots, T-1\}$ to minimize some loss function between final output $x_{s, T}$ and true targets $y_s$ of $x_{s,0}$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Statement of Problem&lt;&#x2F;strong&gt;: Define $\Phi_s: \mathbb{R}^{d_T} \to \mathbb{R}$ that measures the loss, and the average loss function is&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{1}{S}\sum_s \Phi_s(x_{s,T})
$$&lt;&#x2F;p&gt;
&lt;p&gt;We also consider some regularization terms $L_t: \mathbb{R}^{d_t} \times \Theta_t \to \mathbb{R}$, thus the problem is&lt;&#x2F;p&gt;
&lt;p&gt;$$
\min_{\boldsymbol{\theta}\in\boldsymbol{\Theta}}J(\boldsymbol{\theta}):=\frac{1}{S}\sum_{s=1}^{S}\Phi_{s}(x_{s,T})+\frac{1}{S}\sum_{s=1}^{S}\sum_{t=0}^{T-1}L_{t}(x_{s,t},\theta_{t}),
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
x_{s,t+1}=f_t(x_{s,t},\theta_t), \quad t=0,\cdots,T-1, \quad s\in \{1,2,\cdots,S\}
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $\bm{\Theta} := \{\Theta_0 \times \cdots \times \Theta_{T-1}\}$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-pontryagin-s-maximum-principle&quot;&gt;The Pontryagin’s Maximum Principle&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Hamiltonian Function&lt;&#x2F;strong&gt;: Let $\bm{\theta}^\ast = \{\theta_0, \cdots, \theta_{T-1}\} \in \bm{\Theta}$ be a solution of the problem. For each $t$, define the Hamiltonian function $H_{t}:\mathbb{R}^{d_{t}}\times\mathbb{R}^{d_{t+1}}\times\Theta_{t}\to\mathbb{R}$​&lt;&#x2F;p&gt;
&lt;p&gt;$$
H_t(x,p,\theta):=p\cdot f_t(x,\theta)-\frac{1}{S}L_t(x,\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $p \in \mathbb{R}^{d_{t+1}}$ is the co-state vector.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Discrete PMP, Informal Statement&lt;&#x2F;strong&gt;: Let $f_t$ and $\Phi_s, s = 1,2,\cdots, S$ be sufficiently smooth in $x$. Assume for each $t$ and $x \in \mathbb{R}^{d_t}$, the sets $\{f_t(x,\theta): \theta \in \Theta_t\}$ and $\{L_t(x,\theta): \theta \in \Theta_t\}$ are convex. Then there exists $\boldsymbol{p}_{s}^{*}:=\{p_{s,t}^{*}:t=0,\ldots,{T}\},$ such that&lt;&#x2F;p&gt;
&lt;p&gt;$$
x_{s,t+1}^* = \nabla_p H_t(x_{s,t}^*, p_{s,t+1}^*, \theta_t^*), \quad x_{s,0}^* = x_{s,0}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
p_{s,t}^* = \nabla_x H_t(x_{s,t}^*, p_{s,t+1}^*, \theta_t^*), \quad p_{s,T}^* = -\frac{1}{S} \nabla \Phi_s(x_{s,T}^*)
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\sum_{s=1}^S H_t(x_{s,t}^*, p_{s,t+1}^*, \theta_t^*) \geq \sum_{s=1}^S H_t(x_{s,t}^*, p_{s,t+1}^*, \theta), \forall \theta \in \Theta_t
$$&lt;&#x2F;p&gt;
&lt;p&gt;for $t = 0, 1, \cdots, T-1$ and $s = 1,2,\cdots, S$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-method-of-successive-approximations-msa&quot;&gt;The Method of Successive Approximations (MSA)&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Statement of MSA Algorithm&lt;&#x2F;strong&gt;: Start from an initial guess $\boldsymbol{\theta}^{0}=\{\theta_{t}^{0}\in\Theta_{t}:t=0\ldots,T-1\}$,&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;State Equation: $x_{s, t}$ means the state of the $s$-th sample at the $t$-th layer, $f_t$ is the transformation function at the $t$-th layer, $\theta_t$ is the control at the $t$-th layer&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$
x_{s,t+1}^{\boldsymbol{\theta}^0}=f_t(x_{s,t}^{\boldsymbol{\theta}^0},\theta_t^0),\quad x_{s,0}^{\boldsymbol{\theta}^0}=x_{s,0},
$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Co-State Equation: $p_{s,t}$ means the co-state of the $s$-th sample at the $t$-th layer, $\Phi_s$ measures the loss of the $s$-th sample, $H_t$ is the Hamiltonian function&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$
p_{s,t}^{\boldsymbol{\theta}^0}=\nabla_xH_t(x_{s,t}^{\boldsymbol{\theta}^0},p_{s,t+1}^{\boldsymbol{\theta}^0},\theta_t^0),\quad p_{s,T}^{\boldsymbol{\theta}^0}=-\frac{1}{S}\nabla\Phi_s(x_{s,T}^{\boldsymbol{\theta}^0}),
$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Maximization of the Hamiltonian:&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$
\theta_t^1=\arg\max_{\theta\in\Theta_t}\sum_{s=1}^SH_t(x_{s,t}^{\boldsymbol{\theta}^0},p_{s,t+1}^{\boldsymbol{\theta}^0},\theta),
\quad t=0,\ldots,T-1.
$$&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;MSA Algorithm&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initialize&lt;&#x2F;strong&gt;: $\boldsymbol{\theta}^{0}=\{\theta_{t}^{0}\in\Theta_{t}:t=0\ldots,T-1\};$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;For&lt;&#x2F;strong&gt; $k = 0$ to $K$ &lt;strong&gt;do&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$x_{s,t+1}^{\boldsymbol{\theta}^k}=f_t(x_{s,t}^{\boldsymbol{\theta}^k},\theta_t^k)$ and $x_{s,0}^{\boldsymbol{\theta}^k}=x_{s,0}$ for all $s$ and $t$;&lt;&#x2F;li&gt;
&lt;li&gt;$p_{s,t}^{\boldsymbol{\theta}^k}=\nabla_xH_t(x_{s,t}^{\boldsymbol{\theta}^k},p_{s,t+1}^{\boldsymbol{\theta}^k},\theta_t^k),p_{s,T}^{\boldsymbol{\theta}^k}=-\frac{1}{S}\nabla\Phi_s(x_{s,T})$ for all $s$ and $t$;&lt;&#x2F;li&gt;
&lt;li&gt;$\theta_t^{k+1}=\arg\max_{\theta\in\Theta_t}\sum_{s=1}^SH_t(x_{s,t}^{\boldsymbol{\theta}^k},p_{s,t+1}^{\boldsymbol{\theta}^k},\theta)$ for $t = 0, \cdots, T-1$;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;End for&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>论文阅读：连续学习的最优控制方式</title>
        <published>2025-08-15T00:00:00+00:00</published>
        <updated>2025-08-15T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi/</id>
        
            <content type="html">&lt;blockquote&gt;
&lt;p&gt;Original Paper: &lt;a href=&quot;https:&#x2F;&#x2F;www.123865.com&#x2F;s&#x2F;plj7Vv-fvR23&quot;&gt;Optimal Protocols for Continual Learning via Statistical Physics and Control Theory&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;概述：本篇文章介绍了如何使用最优控制理论调整连续学习中的 Replay 任务、学习率使得最终的 generalization error 最低。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Multi-Task Learning&lt;&#x2F;strong&gt;: Training a neural network on a series of tasks.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Catastrophic Forgetting&lt;&#x2F;strong&gt;: Multi-task learning can lead to catastrophic forgetting, where learning new tasks degrades performance on older ones.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Replay&lt;&#x2F;strong&gt;: Present the network with examples from the old tasks while training on the new one to minimize forgetting.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;a-teacher-student-framework&quot;&gt;A Teacher-Student Framework&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Teacher-Student Framework&lt;&#x2F;strong&gt;: Here we consider a teacher-student framework&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Student: The student network is trained on synthetic inputs $\boldsymbol{x} \in \mathbb{R}^N$, drawn i.i.d. from a standard Gaussian distribution $x_i \sim \mathcal{N}(0, 1)$. It is a two-layer neural network with $K$ hidden units, first-layer weight $\boldsymbol{W} = (\boldsymbol{w}_1,\cdots,\boldsymbol{w}_K)^{\top} \in \mathbb{R}^{K \times N}$, activation function $g$, and second-layer weights $\boldsymbol{v} \in \mathbb{R}^K$. It outputs the prediction&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{y}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{v}) = \sum\limits_{k = 1}^K g \left( \frac{\boldsymbol{x} \cdot \boldsymbol{w}_k}{\sqrt{N}} \right).
$$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Teacher: The labels for each task $t = 1,2,\cdots, T$ are generated by the teacher networks, $y^{(t)} = g_{\ast}(\boldsymbol{x} \cdot \boldsymbol{w}_{\ast}^{(t)}&#x2F;\sqrt{N})$, where $\boldsymbol{W}_{\ast} = (\boldsymbol{w}_{\ast}^{(1)},\cdots,\boldsymbol{w}_{\ast}^{(T)})^{\top} \in \mathbb{R}^{T \times N}$ denote the corresponding teacher vectors, and $g_{\ast}$ the activation function.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Task-Dependent Weights: We allow for task-dependent readout weights $\boldsymbol{V} = (\boldsymbol{v}^{(1)},\cdots,\boldsymbol{v}^{(T)})^{\top} \in \mathbb{R}^{T \times K}$. Specifically, when the task $t$ is presented, the readout is switched to the corresponding task, the first-layer weights are shared across tasks.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250815095756-eyuazzc.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Representation of the continual learning task in the teacher-student setting: (a) A student network is trained on i.i.d. inputs from two teacher networks, defining two different tasks; (b) Sequential training results in catastrophic forgetting.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;
&lt;p&gt;&lt;strong&gt;Generalization Error&lt;&#x2F;strong&gt;: The generalization error of the student on task $t$ is given by&lt;&#x2F;p&gt;
&lt;p&gt;$$
\varepsilon_t(\boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_*) := \frac{1}{2} \left\langle \left( y^{(t)} - \hat{y}^{(t)} \right)^2 \right\rangle = \frac{1}{2} \mathbb{E}_{\boldsymbol{x}} \left[ \left( g_* \left( \frac{\boldsymbol{w}_*^{(t)} \cdot \boldsymbol{x}}{\sqrt{N}} \right) - \hat{y}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{v}^{(t)}) \right)^2 \right].
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $(\boldsymbol{x}, y^{(t)})$ is a sample, $\hat{y}^{(t)}$ is the prediction, the angular brackets $\langle \cdot \rangle$ denote the expectation over the input distribution.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Overlaps Variables&lt;&#x2F;strong&gt;: The above generalization error depends only through the preactivations&lt;&#x2F;p&gt;
&lt;p&gt;$$
\lambda_k := \frac{\boldsymbol{x} \cdot \boldsymbol{w}_k}{\sqrt{N}}, \quad k = 1, \ldots, K, \qquad \text{and} \qquad \lambda_*^{(t)} := \frac{\boldsymbol{x} \cdot \boldsymbol{w}_*^{(t)}}{\sqrt{N}}, \quad t = 1, \ldots, T.
$$&lt;&#x2F;p&gt;
&lt;p&gt;They define jointly Gaussian variables with zero mean and second moments given by&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;M_{kt} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_k \lambda_*^{(t)} \right] = \frac{\boldsymbol{w}_k \cdot \boldsymbol{w}_*^{(t)}}{N} , \\
&amp;amp;Q_{kh} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_k \lambda_h \right] = \frac{\boldsymbol{w}_k \cdot \boldsymbol{w}_h}{N} , \\
&amp;amp;S_{tt’} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_*^{(t)} \lambda_*^{(t’)} \right] = \frac{\boldsymbol{w}_*^{(t)} \cdot \boldsymbol{w}_*^{(t’)}}{N} ,
\end{aligned}
$$&lt;&#x2F;p&gt;
&lt;p&gt;called &lt;em&gt;overlaps&lt;&#x2F;em&gt; in the statistical physics literature. Therefore, the dynamics of the generalization error is entirely captured by the evolution of the readouts $\boldsymbol{V}$ and the overlaps.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Forward Training Dynamics&lt;&#x2F;strong&gt;: We use the shorthand notation $\mathbb{Q} = (\operatorname{vec}(\boldsymbol{Q}), \operatorname{vec}(\boldsymbol{M}), \operatorname{vec}(\boldsymbol{V})) \in \mathbb{R}^{K^2 + 2KT}$. The training dynamics is described by a set of ODEs&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\mathrm{d}\mathbb{Q}(\alpha)}{\mathrm{d}\alpha} = f_{\mathbb{Q}}(\mathbb{Q}(\alpha), \boldsymbol{u}(\alpha)), \quad \alpha \in (0, \alpha_F].
$$&lt;&#x2F;p&gt;
&lt;p&gt;The parameter $\alpha$ denotes the effective training time, and $\boldsymbol{u}$ is the control variable.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-optimal-control-framework&quot;&gt;The Optimal Control Framework&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Optimal Control Framework&lt;&#x2F;strong&gt;: Our goal is to derive training strategies that are optimal with respect to the generalization performance &lt;em&gt;at the end of the training&lt;&#x2F;em&gt; and on &lt;em&gt;all tasks&lt;&#x2F;em&gt;. In practice, we minimize a linear combination of the generalization errors on different tasks&lt;&#x2F;p&gt;
&lt;p&gt;$$
h(\mathbb{Q}(\alpha_F)) := \sum\limits_{t = 1}^T c_t \epsilon_t(\mathbb{Q}(\alpha_F)), \quad \text{with} \quad c_t \geq 0, \sum\limits_{t = 1}^T c_t = 1.
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $\alpha_F$ is the final training time, the coefficients $c_t$ identify the relative importance of different tasks and $\epsilon_t$ denotes the infinite-dimensional limit of the average generalization error on task $t$. We define the cost functional&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{F}[\mathbb{Q}, \hat{\mathbb{Q}}, \bm{u}] = h\left(\mathbb{Q}(\alpha_F)\right) + \int_0^{\alpha_F} \mathrm{d}\alpha , \hat{\mathbb{Q}}(\alpha)^\top \left[ -\frac{\mathrm{d}\mathbb{Q}(\alpha)}{\mathrm{d}\alpha} + f_{\mathbb{Q}}\left(\mathbb{Q}(\alpha), \bm{u}(\alpha)\right) \right],
$$&lt;&#x2F;p&gt;
&lt;p&gt;Here $\hat{\mathbb{Q}} = (\operatorname{vec}(\hat{\boldsymbol{Q}}), \operatorname{vec}(\hat{\boldsymbol{M}}), \hat{\operatorname{vec}(\boldsymbol{V}}))$ is the conjugate order parameters (We can consider it as a Lagrange multiplier that incorperates the dynamics of $\mathbb{Q}$).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Backward Conjugate Dynamics&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi&#x2F;assets&#x2F;image-20250815150435-10kdupk.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>最优控制 Optimal Control 概览</title>
        <published>2025-08-15T00:00:00+00:00</published>
        <updated>2025-08-15T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/zui-you-kong-zhi-optimal-control-gai-lan/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/zui-you-kong-zhi-optimal-control-gai-lan/</id>
        
            <content type="html">&lt;p&gt;近期在学习一些最优控制 Optimal Control 相关的理论，想先学习一下大致的问题，因此整理此笔记。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;wen-ti-miao-shu-problem-formulation&quot;&gt;问题描述 Problem Formulation&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;状态方程 State Equation&lt;&#x2F;strong&gt;：Optimal Control 中考虑描述系统动态的常微分方程&lt;&#x2F;p&gt;
&lt;p&gt;$$
\dot{x}(t) = f(t, x(t), u(t)),
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $t \in [t_0, t_f]$ 是时间，$x(t) \in \mathbb{R}^n$ 是状态向量 state vector，$u(t) \in \mathcal{U} \subset \mathbb{R}^m$ 是控制向量，$\mathcal{U}$ 是 set of admissible controls。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;我们希望通过调整 $u$ 的控制信号来控制 $x(t)$ 的行为。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;边界条件 Boundary Conditions&lt;&#x2F;strong&gt;：往往我们考虑的问题会有一些限制条件，例如&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Initial Condition：初始状态往往是固定的 $x(t_0) = x_0$；&lt;&#x2F;li&gt;
&lt;li&gt;Terminal Condition：一般是一种约束条件 $\Psi(t_f, x(t_f)) = \mathbf{0}$，其中 $\Psi$ 是一个向量函数&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Cost Functional&lt;&#x2F;strong&gt;：通过控制动力系统，我们希望最小化目标函数 $J$，其一般由两个部分组成：&lt;&#x2F;p&gt;
&lt;p&gt;$$
J[u(\cdot)] = \phi(t_f, x(t_f)) + \int_{t_0}^{t_f} L(t, x(t), u(t))\mathrm{d}t.
$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$\phi(t_f, x(t_f))$ 是 terminal cost，与系统在终点的状态有关；&lt;&#x2F;li&gt;
&lt;li&gt;$L(t, x(t), u(t))$ 是 running cost，表示整个过程中的积累成本。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;我们的目标 Objective&lt;&#x2F;strong&gt;：找到一个控制函数 $u^\ast(\cdot): [t_0, t_f] \to \mathcal{U}$，是的对应的状态轨迹 trajectory 能够最小化指标 $J$。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;pontryagin-s-minimum-principle-pmp&quot;&gt;Pontryagin’s Minimum Principle - PMP&lt;&#x2F;h2&gt;
&lt;p&gt;PMP 提供了一组最优控制所必须满足的必要条件，其是经典变分法 Calculus of Variations 的推广。但我们这里暂时不管什么是经典变分法，先把 PMP 的定理叙述搞明白。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;哈密顿量 The Hamiltonian&lt;&#x2F;strong&gt;：我们引入一个辅助函数，称为&lt;em&gt;哈密顿量 Hamiltonian&lt;&#x2F;em&gt;：&lt;&#x2F;p&gt;
&lt;p&gt;$$
H(t,x,u,\lambda) = L(t, x, u) + \lambda(t)^\top f(t, x, u),
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $\lambda(t) \in \mathbb{R}^n$ 是一个向量函数，称为 co-state vector 或 adjoint vector。哈密顿量将用于描述 PMP 定理。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;PMP 定理&lt;&#x2F;strong&gt;：设 $u^\ast(\cdot)$ 是最优控制，$x^\ast(\cdot)$ 是对应的最优轨迹，那么必然存在一个非零的 co-state vector $\lambda^\ast(\cdot)$，对于所有 $t \in [t_0, t_f]$ 满足&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;状态方程 State Equation：系统原始动态的重新表述&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$
\dot{x}^\ast(t) = \frac{\partial H}{\partial \lambda}(t, x^\ast, u^\ast, \lambda^\ast) = f(t, x^\ast, u^\ast).
$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;协态方程 Co-state Equation&#x2F;Adjoint Equation：$\lambda^\ast(t)$ 需要满足的方程&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$
\dot{\lambda}^\ast(t) = - \frac{\partial H}{\partial x}(t, x^\ast, u^\ast, \lambda^\ast).
$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;哈密顿量最小化条件 Minimization of the Hamiltonian：对于所有容许的控制 $v \in \mathcal{U}$，最优控制 $u^\ast(t)$ 必须使哈密顿量在任意时刻 $t$ 取最小值&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$
H(t, x^\ast(t), u^\ast(t), \lambda^\ast(t)) \leq H(t, x^\ast(t), v, \lambda^\ast(t)).
$$&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果控制 $u$ 没有约束（即 $\mathcal{U} = \mathbb{R}^m$）且 $H$ 对 $u$ 可微，则该条件可以简化为&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\partial H}{\partial u} (t, x^\ast, u^\ast, \lambda^\ast) = 0
$$&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;横截条件 Transversality Condition：如果 $x(t_f)$ 固定，则 $\lambda(t_f)$ 没有约束；如果 $x(t_f)$ 自由，则&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$
\lambda^\ast(t_f) = \frac{\partial \phi}{\partial x} (t_f, x^\ast(t_f)).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;&#x2F;strong&gt;：PMP 将复杂的泛函最小化问题转化为两点边值问题 Two-Point Boundary Value Problem&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;dong-tai-gui-hua-yu-hamilton-jacobi-bellman-hjb-fang-cheng&quot;&gt;动态规划与 Hamilton-Jacobi-Bellman (HJB) 方程&lt;&#x2F;h2&gt;
&lt;p&gt;动态规划是解决最优控制问题的另一种方法，其提供了最优性的&lt;strong&gt;充分条件&lt;&#x2F;strong&gt;，并且能得到&lt;strong&gt;闭环 closed-loop&lt;&#x2F;strong&gt; 或者&lt;strong&gt;反馈 feedback&lt;&#x2F;strong&gt; 控制策略。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;最优值函数 Optimal Value Function&lt;&#x2F;strong&gt;：定义 &lt;em&gt;Optimal Value Function&lt;&#x2F;em&gt; $V(t, x)$ 为从时间 $t$、状态 $x$ 出发，能得到的最小成本&lt;&#x2F;p&gt;
&lt;p&gt;$$
V(t, x) = \min_{u(\cdot) \in \mathcal{U}} \left[\phi(t_f, x(t_f)) + \int_t^{t_f} L(\tau, x(\tau), u(\tau) \mathrm{d} \tau\right],
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中轨迹 $x(\tau)$ 满足 $\dot{x} = f(\tau, x, u)$ 和 $x(t) = x$。根据其定义，我们有 terminal condition&lt;&#x2F;p&gt;
&lt;p&gt;$$
V(t_f, x) = \phi(t_f, x).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Bellman’s Principle of Optimality&lt;&#x2F;strong&gt;：无论过去的状态和决策如何，余下的决策对于由过去决策所形成的状态而言，也必须构成一个最优策略。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;HJB 方程&lt;&#x2F;strong&gt;：根据 Bellman’s Priciple of Optimality，考虑一个极小的时间段 $[t, t+\delta t]$，&lt;&#x2F;p&gt;
&lt;p&gt;$$
V(t, x) = \min_{u(t)} [L(t,x,u) \delta t + V(t + \delta t, x(t + \delta t)) + o(\delta t)]
$$&lt;&#x2F;p&gt;
&lt;p&gt;对 $V(t + \delta t, x(t+\delta t))$ 进行泰勒展开&lt;&#x2F;p&gt;
&lt;p&gt;$$
V(t + \delta t, x+ \delta x) \approx V(t,x) + \frac{\partial V}{\partial t} \delta t + \left( \frac{\partial V}{\partial x}\right) \dot{x} \delta t.
$$&lt;&#x2F;p&gt;
&lt;p&gt;将展开式代入，并用 $\dot{x} = f(t, x, u)$ 替换，整理后两侧同时除以 $\delta t$，并令 $\delta t \to 0$，得到 &lt;strong&gt;HJB 方程&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;$$- \frac{\partial V}{\partial t} (t, x) = \min_{u \in \mathcal{U}} \left[ L(t, x, u) + \left( \frac{\partial V}{\partial x}(t, x) \right)^{\top} f(t,x,u) \right].
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;与哈密顿量的关系&lt;&#x2F;strong&gt;：如果我们将 $\frac{\partial V}{\partial x}$ 视作一个整体，则 HJB 方程可以更紧凑地写为：&lt;&#x2F;p&gt;
&lt;p&gt;$$- \frac{\partial V}{\partial t} = \min_{u \in \mathcal{U}} H \left(t, x, u, \frac{\partial V}{\partial x}\right),
$$&lt;&#x2F;p&gt;
&lt;p&gt;变为一个 PDE，边界条件为 $V(t_f, x)  = \phi(t_f, x)$。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;求解与应用&lt;&#x2F;strong&gt;：如果能求解上面的 PDE 得到 $V(t,x)$，则最优控制可以通过最小化哈密顿量在每个时刻 $(t,x)$ 得到&lt;&#x2F;p&gt;
&lt;p&gt;$$
u^\ast(t, x) = \arg\min_{u \in \mathcal{U}} H \left( t, x, u, \frac{\partial V}{\partial x}(t,x) \right).
$$&lt;&#x2F;p&gt;
&lt;p&gt;但是在高位 state space 中，HJB 方程的求解非常困难，这也被称为维数灾难 curse of dimensionality。&lt;&#x2F;p&gt;
</content>
        </entry>
</feed>
