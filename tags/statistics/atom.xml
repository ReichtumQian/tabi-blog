<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Statistics</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - Statistics</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/statistics/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/statistics/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-09-01T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/statistics/atom.xml</id><entry xml:lang="zh">
        <title>先验、后验概率、贝叶斯公式</title>
        <published>2025-09-01T00:00:00+00:00</published>
        <updated>2025-09-01T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-xian-yan-hou-yan-gai-lu-bei-xie-si-gong-shi/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-xian-yan-hou-yan-gai-lu-bei-xie-si-gong-shi/</id>
        
            <content type="html">&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;xian-yan-hou-yan-de-ding-yi&quot;&gt;先验、后验的定义&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;先验概率 Prior Probability&lt;&#x2F;strong&gt;：假设我们观测到数据 $D$，其由事物背后看不见的性质（参数 $\theta$）控制。&lt;em&gt;先验概率&lt;&#x2F;em&gt;指的是在未观测任何数据前，我们对一个参数 $\theta$ 的信念（表现为概率）&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;以抛硬币为例，我们让 $\theta$ 表示&lt;u&gt;该硬币朝上的概率&lt;&#x2F;u&gt;（这个是硬币本身的性质，类似于神经网络参数）。我们在没抛任何一枚硬币前，若我们认为该硬币是均匀的概率是 $80\%$​，则我们的先验表达为：&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta = 0.5) = 0.8,
$$&lt;&#x2F;p&gt;
&lt;p&gt;即对于事件&lt;u&gt;硬币朝上概率的概率为 &lt;&#x2F;u&gt;​&lt;u&gt;$50\%$&lt;&#x2F;u&gt;（也就是均匀的）的信任度为 $80\%$​。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;似然 Likelihood&lt;&#x2F;strong&gt;：似然表示我们假设某个参数 $\theta$ 为真的情况下，能观测到数据 $D$ 的概率&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D | \theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;例如我们假设硬币是均匀的，即 $\theta = 0.5$，我们观察到抛 $10$ 次出现 $7$ 次正面的现象 $D$ 的概率为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D | \theta = 0.5).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;后验概率 Posterior Probability&lt;&#x2F;strong&gt;：后验概率表示观测到新数据 $D$ 后，我们对参数 $\theta$ 更新后的信念（从先验的主观臆断到后验的数据观测）&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta | D).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;证据 Evidence &#x2F; 边缘似然 Marginal Likelihood&lt;&#x2F;strong&gt;：对于数据 $D$，我们定义其在所有可能的 $\theta$ 下出现的总概率为 &lt;em&gt;Evidence&lt;&#x2F;em&gt;。在离散情况下&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D) = \sum P(D|\theta_i) P(\theta_i)
$$&lt;&#x2F;p&gt;
&lt;p&gt;在连续情况下&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D) = \int P(D|\theta) P(\theta) \mathrm{d}\theta.
$$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;bei-xie-si-gong-shi&quot;&gt;贝叶斯公式&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯公式&lt;&#x2F;strong&gt;：贝叶斯公式将先验概率、似然、后验概率联系在了一起：&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta|D)=\frac{P(D|\theta)\cdot P(\theta)}{P(D)}
$$&lt;&#x2F;p&gt;
&lt;p&gt;左侧 $P(\theta | D)$ 是后验概率，$P(D|\theta)$ 是似然，$P(\theta)$ 是先验概率，$P(D)$ 是边缘似然。在实际计算中，$P(D)$ 非常难计算，因此我们一般认为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta|D)\propto P(D|\theta)\cdot P(\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;投硬币例子&lt;&#x2F;strong&gt;：例如我们对硬币的公平性有所怀疑，提出了两个假设 $\theta_1 = 0.5$ 和 $\theta_2 = 0.8$（表示正面概率）。在没有证据前，我们认为它们都有可能，即&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta = 0.5) = 0.5, \quad P(\theta = 0.8) = 0.5.
$$&lt;&#x2F;p&gt;
&lt;p&gt;然后我们观测到数据 $D$ 为&lt;u&gt;10次抛掷，7次正面&lt;&#x2F;u&gt;。如果 $\theta = 0.5$，那么观测到 $D$ 的概率为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D|\theta_1=0.5)=C(10,7)\cdot(0.5)^7(0.5)^3\approx0.117
$$&lt;&#x2F;p&gt;
&lt;p&gt;如果 $\theta = 0.8$，那么观测到 $D$ 的概率为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D|\theta_2=0.8)=C(10,7)\cdot(0.8)^7(0.2)^3\approx0.201
$$&lt;&#x2F;p&gt;
&lt;p&gt;因此我们可以得到后验概率&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta_1=0.5|D)\propto0.117\times0.5=0.0585
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta_2=0.8|D)\propto0.201\times0.5=0.1005
$$&lt;&#x2F;p&gt;
&lt;p&gt;由于只有这两种情况，我们不妨做个归一化：$P(D) = 0.0585 + 0.1005 = 0.159$，得到&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta_1=0.5|D)=\frac{0.0585}{0.159}\approx0.368, \quad
P(\theta_2=0.8|D)=\frac{0.1005}{0.159}\approx0.632.
$$&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>KL 散度、交叉熵与对数似然</title>
        <published>2025-08-21T00:00:00+00:00</published>
        <updated>2025-08-21T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-kl-san-du-jiao-cha-shang-yu-dui-shu-si-ran/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-kl-san-du-jiao-cha-shang-yu-dui-shu-si-ran/</id>
        
            <content type="html">&lt;h2 id=&quot;kl-san-du&quot;&gt;KL 散度&lt;&#x2F;h2&gt;
&lt;p&gt;KL 散度（Kullback-Leibler Divergence）是一种非对称的度量，用于量化一个概率分布和另一个参考概率分布的差异。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;KL 散度的定义&lt;&#x2F;strong&gt;：给定概率分布 $P(x)$ 和 $Q(x)$，若它们是离散概率分布，则 &lt;em&gt;KL 散度&lt;&#x2F;em&gt;定义为&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL} (P ||Q) := \sum_x P(x) \log \frac{P(x)}{Q(x)}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;若 $P(x)$ 和 $Q(x)$ 是连续概率分布，对应概率密度函数为 $p(x)$ 和 $q(x)$，则 KL 散度为&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL} (P ||Q) := \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} \mathrm{d} x.
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;KL 散度的性质&lt;&#x2F;strong&gt;：KL 散度满足除了对称性外的度量性质&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;非负性：$D_{KL}(P || Q) \geq 0$，且 $D_{KL}(P || Q) = 0$ 当且仅当 $P(x) \equiv Q(x)$。&lt;&#x2F;li&gt;
&lt;li&gt;非对称性：$D_{KL}(P || Q) \neq D_{KL}(Q || P)$。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;KL 散度的信息论解释&lt;&#x2F;strong&gt;：$D_{KL}(P || Q)$ 表示使用 $Q$ 来编码 $P$ 时，平均每个样本所需的额外信息量（比特数）。KL 散度越小，说明 $Q$ 对 $P$ 的近似程度越好，因此&lt;strong&gt;最小化 KL 散度&lt;&#x2F;strong&gt;也是机器学习中常用的优化目标。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;xin-xi-shang-yu-jiao-cha-shang&quot;&gt;信息熵与交叉熵&lt;&#x2F;h2&gt;
&lt;p&gt;信息熵（Information Entropy）用于衡量一个随机变量的不确定性，一个系统越混乱，越不可预测，则其信息熵越高。交叉熵（Cross Entropy）衡量着我们使用&lt;strong&gt;错误的&lt;&#x2F;strong&gt;分布 $Q$ 去衡量真实分布 $P$ 时所需要的平均编码长度。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;信息熵的定义&lt;&#x2F;strong&gt;：信息熵定义为一个随机变量所包含信息的平均期望，给定离散随机变量 $X$，其可取 $x_1, x_2, \cdots, x_n$，对应概率 $P(x_1), P(x_2), \cdots, P(x_n)$，则&lt;em&gt;信息熵&lt;&#x2F;em&gt;定义为&lt;&#x2F;p&gt;
&lt;p&gt;$$
H(x) = - \sum_{i = 1}^n P(x_i) \log_bP(x_i),
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $b$ 决定了熵的单位，$b = 2$ 表示 bit，$b=e$ 表示 nat，$b = 10$ 表示 hart。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;交叉熵的定义&lt;&#x2F;strong&gt;：给定概率分布 $P(x)$ 和 $Q(x)$，&lt;em&gt;交叉熵&lt;&#x2F;em&gt;函数为&lt;&#x2F;p&gt;
&lt;p&gt;$$
H(P, Q) := - \sum_xP(x)\log Q(x).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;交叉熵的信息论理解&lt;&#x2F;strong&gt;：如果 $Q$ 能完美预测真实分布 $P$，那么交叉熵就等于真实分布的信息熵 $H(P)$，此时编码成本最低。如果 $Q$ 的预测非常不准，那么交叉熵就会远大于信息熵。因此在机器学习中我们往往希望&lt;strong&gt;最小化交叉熵&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dui-shu-si-ran&quot;&gt;对数似然&lt;&#x2F;h2&gt;
&lt;p&gt;似然函数在统计学中用于&lt;strong&gt;评估模型参数对观测数据的拟合程度&lt;&#x2F;strong&gt;，似然函数越大说明当前模型的参数对观测数据的拟合程度越高。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;似然函数 Likelihood Function&lt;&#x2F;strong&gt;：给定数据集 $X = \{x_1, x_2, \cdots, x_n\}$ 和由参数 $\theta$ 控制的概率模型 $P(X|\theta)$，&lt;em&gt;似然函数&lt;&#x2F;em&gt;定义为：&lt;&#x2F;p&gt;
&lt;p&gt;$$
L(\theta|X) := P(X|\theta) = \prod_{i = 1}^n P(x_i|\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;即表示着在 $\theta$ 参数情况下，从概率模型中抽取得到数据集 $X$ 的概率。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;最大化似然估计 MLE&lt;&#x2F;strong&gt;：显然 $L(\theta|X)$ 越大，那么模型就越可能生成当前的数据集，也就是更加贴合数据集。因此一般我们的目标是寻找一组参数 $\hat{\theta}$，使得似然函数最大化&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{\theta}_{MLE} = \arg\max_\theta L(\theta|X).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;对数似然函数 Log-Likelihood Function&lt;&#x2F;strong&gt;：由于似然函数的格式是连乘，在计算上既复杂又容易数值下溢，因此通常会使用&lt;em&gt;对数似然函数 Log-Likelihood Function&lt;&#x2F;em&gt;：&lt;&#x2F;p&gt;
&lt;p&gt;$$
\log L(\theta|X) = \sum_{i = 1}^n \log P(x_i|\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;可以看出最大化似然估计也等价于最大化对数似然估计，因此实际优化中一般我们都使用最大对数似然。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;san-zhe-de-guan-xi&quot;&gt;三者的关系&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;KL 散度与交叉熵&lt;&#x2F;strong&gt;：KL 散度为交叉熵与真实分布熵的差，即&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL}(P||Q) = H(P,Q) - H(P).
$$&lt;&#x2F;p&gt;
&lt;p&gt;因此&lt;strong&gt;最小化 KL 散度等价于最小化 Cross-Entropy&lt;&#x2F;strong&gt;，而在计算时由于交叉熵更容易计算，因此通常使用交叉熵函数。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;KL 散度与对数似然&lt;&#x2F;strong&gt;：这里我们假设数据集由数据分布 $P_{\text{data}}(x)$ 生成，分布模型为 $P_{\text{model}}(x|\theta)$，此时&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL}(P_{\text{data}} || P_{\text{model}}) = \sum_{x} P_{\text{data}}(x) \log P_{\text{data}}(x) - \sum_{x} P_{\text{data}}(x)\log P_{\text{model}}(x)
$$&lt;&#x2F;p&gt;
&lt;p&gt;右侧第一项即 $P_{\text{data}}$ 的信息熵，由于 $P_{\text{data}}$ 是固定的，因此此项是一个常数，而第二项即 $P_{\text{data}}$ 和 $P_{\text{model}}$ 的交叉熵。因此&lt;strong&gt;最小化 KL 散度等价于最大化对数似然&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry>
</feed>
