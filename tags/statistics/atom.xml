<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Statistics</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - Statistics</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/statistics/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/statistics/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-08-21T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/statistics/atom.xml</id><entry xml:lang="zh">
        <title>KL 散度、交叉熵与对数似然</title>
        <published>2025-08-21T00:00:00+00:00</published>
        <updated>2025-08-21T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-kl-san-du-jiao-cha-shang-yu-dui-shu-si-ran/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-kl-san-du-jiao-cha-shang-yu-dui-shu-si-ran/</id>
        
            <content type="html">&lt;h2 id=&quot;kl-san-du&quot;&gt;KL 散度&lt;&#x2F;h2&gt;
&lt;p&gt;KL 散度（Kullback-Leibler Divergence）是一种非对称的度量，用于量化一个概率分布和另一个参考概率分布的差异。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;KL 散度的定义&lt;&#x2F;strong&gt;：给定概率分布 $P(x)$ 和 $Q(x)$，若它们是离散概率分布，则 &lt;em&gt;KL 散度&lt;&#x2F;em&gt;定义为&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL} (P ||Q) := \sum_x P(x) \log \frac{P(x)}{Q(x)}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;若 $P(x)$ 和 $Q(x)$ 是连续概率分布，对应概率密度函数为 $p(x)$ 和 $q(x)$，则 KL 散度为&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL} (P ||Q) := \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} \mathrm{d} x.
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;KL 散度的性质&lt;&#x2F;strong&gt;：KL 散度满足除了对称性外的度量性质&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;非负性：$D_{KL}(P || Q) \geq 0$，且 $D_{KL}(P || Q) = 0$ 当且仅当 $P(x) \equiv Q(x)$。&lt;&#x2F;li&gt;
&lt;li&gt;非对称性：$D_{KL}(P || Q) \neq D_{KL}(Q || P)$。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;KL 散度的信息论解释&lt;&#x2F;strong&gt;：$D_{KL}(P || Q)$ 表示使用 $Q$ 来编码 $P$ 时，平均每个样本所需的额外信息量（比特数）。KL 散度越小，说明 $Q$ 对 $P$ 的近似程度越好，因此&lt;strong&gt;最小化 KL 散度&lt;&#x2F;strong&gt;也是机器学习中常用的优化目标。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;xin-xi-shang-yu-jiao-cha-shang&quot;&gt;信息熵与交叉熵&lt;&#x2F;h2&gt;
&lt;p&gt;信息熵（Information Entropy）用于衡量一个随机变量的不确定性，一个系统越混乱，越不可预测，则其信息熵越高。交叉熵（Cross Entropy）衡量着我们使用&lt;strong&gt;错误的&lt;&#x2F;strong&gt;分布 $Q$ 去衡量真实分布 $P$ 时所需要的平均编码长度。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;信息熵的定义&lt;&#x2F;strong&gt;：信息熵定义为一个随机变量所包含信息的平均期望，给定离散随机变量 $X$，其可取 $x_1, x_2, \cdots, x_n$，对应概率 $P(x_1), P(x_2), \cdots, P(x_n)$，则&lt;em&gt;信息熵&lt;&#x2F;em&gt;定义为&lt;&#x2F;p&gt;
&lt;p&gt;$$
H(x) = - \sum_{i = 1}^n P(x_i) \log_bP(x_i),
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $b$ 决定了熵的单位，$b = 2$ 表示 bit，$b=e$ 表示 nat，$b = 10$ 表示 hart。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;交叉熵的定义&lt;&#x2F;strong&gt;：给定概率分布 $P(x)$ 和 $Q(x)$，&lt;em&gt;交叉熵&lt;&#x2F;em&gt;函数为&lt;&#x2F;p&gt;
&lt;p&gt;$$
H(P, Q) := - \sum_xP(x)\log Q(x).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;交叉熵的信息论理解&lt;&#x2F;strong&gt;：如果 $Q$ 能完美预测真实分布 $P$，那么交叉熵就等于真实分布的信息熵 $H(P)$，此时编码成本最低。如果 $Q$ 的预测非常不准，那么交叉熵就会远大于信息熵。因此在机器学习中我们往往希望&lt;strong&gt;最小化交叉熵&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dui-shu-si-ran&quot;&gt;对数似然&lt;&#x2F;h2&gt;
&lt;p&gt;似然函数在统计学中用于&lt;strong&gt;评估模型参数对观测数据的拟合程度&lt;&#x2F;strong&gt;，似然函数越大说明当前模型的参数对观测数据的拟合程度越高。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;似然函数 Likelihood Function&lt;&#x2F;strong&gt;：给定数据集 $X = \{x_1, x_2, \cdots, x_n\}$ 和由参数 $\theta$ 控制的概率模型 $P(X|\theta)$，&lt;em&gt;似然函数&lt;&#x2F;em&gt;定义为：&lt;&#x2F;p&gt;
&lt;p&gt;$$
L(\theta|X) := P(X|\theta) = \prod_{i = 1}^n P(x_i|\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;即表示着在 $\theta$ 参数情况下，从概率模型中抽取得到数据集 $X$ 的概率。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;最大化似然估计 MLE&lt;&#x2F;strong&gt;：显然 $L(\theta|X)$ 越大，那么模型就越可能生成当前的数据集，也就是更加贴合数据集。因此一般我们的目标是寻找一组参数 $\hat{\theta}$，使得似然函数最大化&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{\theta}_{MLE} = \arg\max_\theta L(\theta|X).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;对数似然函数 Log-Likelihood Function&lt;&#x2F;strong&gt;：由于似然函数的格式是连乘，在计算上既复杂又容易数值下溢，因此通常会使用&lt;em&gt;对数似然函数 Log-Likelihood Function&lt;&#x2F;em&gt;：&lt;&#x2F;p&gt;
&lt;p&gt;$$
\log L(\theta|X) = \sum_{i = 1}^n \log P(x_i|\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;可以看出最大化似然估计也等价于最大化对数似然估计，因此实际优化中一般我们都使用最大对数似然。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;san-zhe-de-guan-xi&quot;&gt;三者的关系&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;KL 散度与交叉熵&lt;&#x2F;strong&gt;：KL 散度为交叉熵与真实分布熵的差，即&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL}(P||Q) = H(P,Q) - H(P).
$$&lt;&#x2F;p&gt;
&lt;p&gt;因此&lt;strong&gt;最小化 KL 散度等价于最小化 Cross-Entropy&lt;&#x2F;strong&gt;，而在计算时由于交叉熵更容易计算，因此通常使用交叉熵函数。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;KL 散度与对数似然&lt;&#x2F;strong&gt;：这里我们假设数据集由数据分布 $P_{\text{data}}(x)$ 生成，分布模型为 $P_{\text{model}}(x|\theta)$，此时&lt;&#x2F;p&gt;
&lt;p&gt;$$
D_{KL}(P_{\text{data}} || P_{\text{model}}) = \sum_{x} P_{\text{data}}(x) \log P_{\text{data}}(x) - \sum_{x} P_{\text{data}}(x)\log P_{\text{model}}(x)
$$&lt;&#x2F;p&gt;
&lt;p&gt;右侧第一项即 $P_{\text{data}}$ 的信息熵，由于 $P_{\text{data}}$ 是固定的，因此此项是一个常数，而第二项即 $P_{\text{data}}$ 和 $P_{\text{model}}$ 的交叉熵。因此&lt;strong&gt;最小化 KL 散度等价于最大化对数似然&lt;&#x2F;strong&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry>
</feed>
