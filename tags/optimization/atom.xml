<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Optimization</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - Optimization</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/optimization/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/optimization/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-09-04T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/optimization/atom.xml</id><entry xml:lang="zh">
        <title>拉格朗日乘子法</title>
        <published>2025-09-04T00:00:00+00:00</published>
        <updated>2025-09-04T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/la-ge-lang-ri-cheng-zi-fa/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/la-ge-lang-ri-cheng-zi-fa/</id>
        
            <content type="html">&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;wen-ti-miao-shu&quot;&gt;问题描述&lt;&#x2F;h2&gt;
&lt;p&gt;拉格朗日乘子法（Method of Lagrange multiplier）用于解决&lt;u&gt;等式约束的优化问题&lt;&#x2F;u&gt;（Equality-Constrained Optimization）。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;等式约束的优化算法&lt;&#x2F;strong&gt;：给定目标函数 $f: \mathbb{R}^m \to \mathbb{R}$ 和等式约束条件 $\mathbf{g}(\mathbf{x}) = \mathbf{c}$，其中 $\mathbf{x} \in \mathbb{R}^n$，$\mathbf{c} \in \mathbb{R}^m$​。问题是找到 $\mathbf{x}^\ast$ 满足&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathbf{x}^\ast = \arg\min_\mathbf{x} f(\mathbf{x}), \quad \operatorname{subject~to} \quad
\mathbf{g}(\mathbf{x}) = \mathbf{c}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $\mathbf{g}(\mathbf{x}^\ast) = \mathbf{c}$ 为多个等式约束条件 $g_1(\mathbf{x}^\ast) = c_1$，$g_2(\mathbf{x}^\ast) = c_2$ 等。&lt;&#x2F;p&gt;
&lt;p&gt;注：为了简单起见，我们先考虑一个限制条件 $g(\mathbf{x}) = c$。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;he-xin-si-xiang&quot;&gt;核心思想&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;等高线 Level Sets&lt;&#x2F;strong&gt;：对于 $d \in \mathbb{R}$，集合 $\{\mathbf{x}: f(\mathbf{x}) = d\}$ 构成了 $f$ 的一条等高线。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;梯度与等高线正交：函数在某一点的梯度 $\nabla f(\mathbf{x})$ 指向函数值增长最快的方向，且与该点的等高线&lt;strong&gt;正交&lt;&#x2F;strong&gt;。&lt;&#x2F;li&gt;
&lt;li&gt;约束曲面是等高线：约束 $g(\mathbf{x}) = c$ 本身也定义了一个曲面。由于其是一个等式，其可以被视作等高线，因此 $\nabla g(\mathbf{x})$ 与该曲面正交。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;拉格朗日乘子法的直观理解&lt;&#x2F;strong&gt;：假设我们沿着约束曲面 $g(\mathbf{x}) = c$ 行走，寻找 $f(\mathbf{x})$ 的最高点。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;如果我们能沿着约束曲面 $g(\mathbf{x}) = c$ 移动优化 $f(\mathbf{x})$，那我们肯定还没到最优点。&lt;&#x2F;li&gt;
&lt;li&gt;当我们达到 $\mathbf{x}^\ast$ 时，我们无法再沿着 $g(\mathbf{x}) = c$ 优化 $f(\mathbf{x})$，因此 $\mathbf{x}^\ast$ 处约束曲线的方向与 $f(\mathbf{x})$ 的等高线方向是相切的。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;核心思想&lt;&#x2F;strong&gt;：拉格朗日乘子法的核心思想为&lt;u&gt;在最优点 &lt;&#x2F;u&gt;​&lt;u&gt;$\mathbf{x}^\ast$&lt;&#x2F;u&gt;​&lt;u&gt;处，目标函数 &lt;&#x2F;u&gt;​&lt;u&gt;$f(\mathbf{x})$&lt;&#x2F;u&gt;​&lt;u&gt; 的等高线&#x2F;面与约束条件 &lt;&#x2F;u&gt;​&lt;u&gt;$g(\mathbf{x}) = c$&lt;&#x2F;u&gt;​&lt;u&gt; 的曲线&#x2F;面相切&lt;&#x2F;u&gt;。用数学语言来说即两个曲面 $f(\mathbf{x}) = f(\mathbf{x}^\ast)$ 和 $g(\mathbf{x}) = c$ 相切，那么它们的法向量必然是平行的，也就是&lt;&#x2F;p&gt;
&lt;p&gt;$$
\nabla f(\mathbf{x}^*)=-\lambda\nabla g(\mathbf{x}^*)
\quad \Rightarrow \quad
\nabla f(\mathbf{x}^*)+\lambda\nabla g(\mathbf{x}^*)=\mathbf{0}
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $\lambda$ 是一个待定系数，表示两个向量的模长比。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;la-ge-lang-ri-cheng-zi-fa&quot;&gt;拉格朗日乘子法&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;拉格朗日乘子法叙述&lt;&#x2F;strong&gt;：上述思想引出了一个巧妙的解决方案，我们引入一个辅助函数，称为&lt;em&gt;拉格朗日函数（Lagrangian Function）&lt;&#x2F;em&gt; ：&lt;&#x2F;p&gt;
&lt;p&gt;$$
L(\mathbf{x},\lambda)=f(\mathbf{x})+\lambda(g(\mathbf{x})-c)
$$&lt;&#x2F;p&gt;
&lt;p&gt;这样我们就把前面的有约束问题&lt;u&gt;转换为了对拉格朗日函数 &lt;&#x2F;u&gt;​&lt;u&gt;$L(\mathbf{x}, \lambda)$&lt;&#x2F;u&gt;​&lt;u&gt; 的无约束优化问题&lt;&#x2F;u&gt;：&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{cases}
\nabla_\mathbf{x}L(\mathbf{x},\lambda)=\nabla f(\mathbf{x})+\lambda\nabla g(\mathbf{x})=\mathbf{0} \\
\frac{\partial L}{\partial\lambda}=g(\mathbf{x})-c=0
\end{cases}
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中第一个式子是前一节中导出的条件，第二个式子施加了原始的约束条件 $g(\mathbf{x}) = c$。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;多等式约束拉格朗日乘子法&lt;&#x2F;strong&gt;：对于多个等式约束的优化问题，我们定义拉格朗日函数为&lt;&#x2F;p&gt;
&lt;p&gt;$$
L(\mathbf{x},\boldsymbol{\lambda})=f(\mathbf{x})+\sum_{i=1}^m\lambda_i(g_i(\mathbf{x})-c_i).
$$&lt;&#x2F;p&gt;
&lt;p&gt;对应的无约束优化问题为：&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{cases}
\nabla_\mathbf{x}L(\mathbf{x},\boldsymbol{\lambda})=\nabla f(\mathbf{x})+\sum_{i=1}^m\lambda_i\nabla g_i(\mathbf{x})=\mathbf{0} \\
\frac{\partial L}{\partial\lambda_i}=g_i(\mathbf{x})-c_i=0, \quad i = 1,2,\cdots, m
\end{cases}
$$&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>AdamW: Adam with Decoupled Weight Decay</title>
        <published>2025-08-27T00:00:00+00:00</published>
        <updated>2025-08-27T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-adamw/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-adamw/</id>
        
            <content type="html">&lt;p&gt;AdamW 是对 Adam 的改进版，Adam 在处理 Weight Decay 时存在一些问题，而 AdamW 正解决了该问题。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;l2-regularization-yu-weight-decay&quot;&gt;L2 Regularization 与 Weight Decay&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;L2 Regularization 的定义&lt;&#x2F;strong&gt;：给定原始损失函数 $f$，使用 L2 Regularization 的优化器&lt;u&gt;修改了损失函数&lt;&#x2F;u&gt;为&lt;&#x2F;p&gt;
&lt;p&gt;$$
f_{\text{total}} (\theta) = f(\theta) + \frac{\lambda}{2}\|\theta\|_2^2.
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $\theta$ 表示模型中的权重参数（一般不包含偏置项 biases），$\|\theta\|_2^2$ 是 $\theta$ 的 L2 范数平方。当我们对其求梯度可以得到其在 SGD 中的更新格式&lt;&#x2F;p&gt;
&lt;p&gt;$$
\nabla f_{\text{total}}(\theta)=\nabla f(\theta)+\lambda\theta
\quad \Rightarrow \quad
\theta_{t+1}=(1-\eta\lambda)\theta_t-\eta \nabla f(\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Weight Decay 的定义&lt;&#x2F;strong&gt;：带 Weight Decay 的优化器&lt;u&gt;修改了梯度更新规则&lt;&#x2F;u&gt;为&lt;&#x2F;p&gt;
&lt;p&gt;$$
g = \nabla f(\theta) + \lambda \theta,
$$&lt;&#x2F;p&gt;
&lt;p&gt;同理我们可以得到其在 SGD 中的参数更新格式：&lt;&#x2F;p&gt;
&lt;p&gt;$$
\theta_{t+1}=(1-\eta\lambda)\theta_t-\eta \nabla f(\theta)
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;L2 Regularization 与 Weight Decay 的等价性&lt;&#x2F;strong&gt;：从上面的推导可以看出，在 SGD 中 L2 Regularization 与 Weight Decay 是等价的。但是这一结论在 Adam 中并不成立。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;L2 Regularization 与 Weight Decay 的目的&lt;&#x2F;strong&gt;：两个方法的根本目的是为了防止&lt;u&gt;模型参数过大而导致过拟合&lt;&#x2F;u&gt;（因为 L2 Regularization 损失函数中对大参数进行了惩罚），很多时候当模型参数过大时，其在训练数据上能表现得很好，但是在验证&#x2F;测试数据上表现很差。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;L2 Regularization 与 Weight Decay 的区别&lt;&#x2F;strong&gt;：虽然两者在 SGD 中等价，但是对大参数的惩罚的最根本原因是 L2 Regularization 中修改的损失函数。这也为 Adam 中的错误处理埋下了伏笔。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;adam-zhong-de-weight-decay&quot;&gt;Adam 中的 Weight Decay&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Adam 中的 Weight Decay 处理方式&lt;&#x2F;strong&gt;：Adam 中 Weight Decay 的处理放在了梯度的计算上，即&lt;&#x2F;p&gt;
&lt;p&gt;$$
g_t \leftarrow \nabla_\theta f_t(\theta_{t-1}) + \lambda \theta_{t-1}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Adam 中 Weight Decay 处理的问题&lt;&#x2F;strong&gt;：Adam with L2 regularization 的最大错误就是&lt;&#x2F;p&gt;
&lt;p&gt;$$
\text{忽略了修改损失函数和修改更新规则的等价性}
$$&lt;&#x2F;p&gt;
&lt;p&gt;我们本质上是想要惩罚大参数，也就是实现 L2 Regularization 中的损失函数。而 Adam 延续了 SGD 的定式思维，认为 Weight Decay 中的更新格式就能实现 L2 Regularization 中的损失函数，而事实上 Adam 的自适应机制导致 Weight Decay 的格式被自适应步长所影响，达不到 L2 Regularization 格式的目标。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;blog-adamw&#x2F;assets&#x2F;image-20250827111241-t0nwupm.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;adamw-de-ge-shi&quot;&gt;AdamW 的格式&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;AdamW 的改进&lt;&#x2F;strong&gt;：AdamW 重新实现了原本 L2 Regularization 的目标，将 Weight Decay 从梯度中解耦，放到最后一步计算，从而提高了泛化性。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;blog-adamw&#x2F;assets&#x2F;image-20250827114029-328jm7k.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>Adam: Adaptive Moment Estimation</title>
        <published>2025-08-25T00:00:00+00:00</published>
        <updated>2025-08-25T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-adam/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-adam/</id>
        
            <content type="html">&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;blog-adam&#x2F;assets&#x2F;image-20250825161145-yxo8sbr.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;blog-adam&#x2F;assets&#x2F;image-20250825161155-ft1cy96.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;blog-adam&#x2F;assets&#x2F;image-20250825161214-ns3zjlt.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;blog-adam&#x2F;assets&#x2F;image-20250825161225-zib63jo.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;blog-adam&#x2F;assets&#x2F;image-20250825161233-000qesq.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>Heavy-ball 与 Momentum 算法等价性</title>
        <published>2025-08-16T00:00:00+00:00</published>
        <updated>2025-08-16T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/heavy-ball-yu-momentum-suan-fa-deng-jia-xing/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/heavy-ball-yu-momentum-suan-fa-deng-jia-xing/</id>
        
            <content type="html">&lt;p&gt;依稀记得 Heavy-Ball 动量格式和 Momentum 动量格式是完全等价的，但是总是记不清楚，因此重新写一遍。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Heavy-Ball 动量格式&lt;&#x2F;strong&gt;：Heavy-Ball 动量格式为&lt;&#x2F;p&gt;
&lt;p&gt;$$
\theta_{t+1} - \theta_t = \gamma (\theta_{t} - \theta_{t-1}) - \eta \nabla f(\theta_t)
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;现代 Momentum 格式&lt;&#x2F;strong&gt;：现代 Momentum 动量格式为&lt;&#x2F;p&gt;
&lt;p&gt;$$
v_{t+1} = \beta v_t + \nabla f(\theta_t), \quad \theta_{t+1} = \theta_t - \eta v_{t+1}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Heavy-Ball 与 Momentum 格式的等价性&lt;&#x2F;strong&gt;：Heavy-Ball 和 Momentum 在数学上是完全等价的，且 $\gamma = \beta$。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;证明：这里只从 Momentum 出发推导 Heavy-Ball，将 Momentum 第一个式子两侧同时乘上学习率 $\eta$ 得到&lt;&#x2F;p&gt;
&lt;p&gt;$$
\eta v_{t+1} = \beta \eta v_t + \eta \nabla f(\theta_t),
$$&lt;&#x2F;p&gt;
&lt;p&gt;而根据第二个式子得到 $-\eta v_{t+1} = \theta_{t+1} - \theta_t$，因此带入上面的等式得到结论。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry>
</feed>
