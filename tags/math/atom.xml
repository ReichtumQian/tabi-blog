<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Math</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - Math</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/math/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/math/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-09-01T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/math/atom.xml</id><entry xml:lang="zh">
        <title>先验、后验概率、贝叶斯公式</title>
        <published>2025-09-01T00:00:00+00:00</published>
        <updated>2025-09-01T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-xian-yan-hou-yan-gai-lu-bei-xie-si-gong-shi/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-xian-yan-hou-yan-gai-lu-bei-xie-si-gong-shi/</id>
        
            <content type="html">&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;xian-yan-hou-yan-de-ding-yi&quot;&gt;先验、后验的定义&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;先验概率 Prior Probability&lt;&#x2F;strong&gt;：假设我们观测到数据 $D$，其由事物背后看不见的性质（参数 $\theta$）控制。&lt;em&gt;先验概率&lt;&#x2F;em&gt;指的是在未观测任何数据前，我们对一个参数 $\theta$ 的信念（表现为概率）&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;以抛硬币为例，我们让 $\theta$ 表示&lt;u&gt;该硬币朝上的概率&lt;&#x2F;u&gt;（这个是硬币本身的性质，类似于神经网络参数）。我们在没抛任何一枚硬币前，若我们认为该硬币是均匀的概率是 $80\%$​，则我们的先验表达为：&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta = 0.5) = 0.8,
$$&lt;&#x2F;p&gt;
&lt;p&gt;即对于事件&lt;u&gt;硬币朝上概率的概率为 &lt;&#x2F;u&gt;​&lt;u&gt;$50\%$&lt;&#x2F;u&gt;（也就是均匀的）的信任度为 $80\%$​。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;似然 Likelihood&lt;&#x2F;strong&gt;：似然表示我们假设某个参数 $\theta$ 为真的情况下，能观测到数据 $D$ 的概率&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D | \theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;例如我们假设硬币是均匀的，即 $\theta = 0.5$，我们观察到抛 $10$ 次出现 $7$ 次正面的现象 $D$ 的概率为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D | \theta = 0.5).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;后验概率 Posterior Probability&lt;&#x2F;strong&gt;：后验概率表示观测到新数据 $D$ 后，我们对参数 $\theta$ 更新后的信念（从先验的主观臆断到后验的数据观测）&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta | D).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;证据 Evidence &#x2F; 边缘似然 Marginal Likelihood&lt;&#x2F;strong&gt;：对于数据 $D$，我们定义其在所有可能的 $\theta$ 下出现的总概率为 &lt;em&gt;Evidence&lt;&#x2F;em&gt;。在离散情况下&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D) = \sum P(D|\theta_i) P(\theta_i)
$$&lt;&#x2F;p&gt;
&lt;p&gt;在连续情况下&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D) = \int P(D|\theta) P(\theta) \mathrm{d}\theta.
$$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;bei-xie-si-gong-shi&quot;&gt;贝叶斯公式&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;贝叶斯公式&lt;&#x2F;strong&gt;：贝叶斯公式将先验概率、似然、后验概率联系在了一起：&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta|D)=\frac{P(D|\theta)\cdot P(\theta)}{P(D)}
$$&lt;&#x2F;p&gt;
&lt;p&gt;左侧 $P(\theta | D)$ 是后验概率，$P(D|\theta)$ 是似然，$P(\theta)$ 是先验概率，$P(D)$ 是边缘似然。在实际计算中，$P(D)$ 非常难计算，因此我们一般认为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta|D)\propto P(D|\theta)\cdot P(\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;投硬币例子&lt;&#x2F;strong&gt;：例如我们对硬币的公平性有所怀疑，提出了两个假设 $\theta_1 = 0.5$ 和 $\theta_2 = 0.8$（表示正面概率）。在没有证据前，我们认为它们都有可能，即&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta = 0.5) = 0.5, \quad P(\theta = 0.8) = 0.5.
$$&lt;&#x2F;p&gt;
&lt;p&gt;然后我们观测到数据 $D$ 为&lt;u&gt;10次抛掷，7次正面&lt;&#x2F;u&gt;。如果 $\theta = 0.5$，那么观测到 $D$ 的概率为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D|\theta_1=0.5)=C(10,7)\cdot(0.5)^7(0.5)^3\approx0.117
$$&lt;&#x2F;p&gt;
&lt;p&gt;如果 $\theta = 0.8$，那么观测到 $D$ 的概率为&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(D|\theta_2=0.8)=C(10,7)\cdot(0.8)^7(0.2)^3\approx0.201
$$&lt;&#x2F;p&gt;
&lt;p&gt;因此我们可以得到后验概率&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta_1=0.5|D)\propto0.117\times0.5=0.0585
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta_2=0.8|D)\propto0.201\times0.5=0.1005
$$&lt;&#x2F;p&gt;
&lt;p&gt;由于只有这两种情况，我们不妨做个归一化：$P(D) = 0.0585 + 0.1005 = 0.159$，得到&lt;&#x2F;p&gt;
&lt;p&gt;$$
P(\theta_1=0.5|D)=\frac{0.0585}{0.159}\approx0.368, \quad
P(\theta_2=0.8|D)=\frac{0.1005}{0.159}\approx0.632.
$$&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>Fisher 矩阵与弹性权重巩固 Elastic Weight Consolidation, EWC</title>
        <published>2025-09-01T00:00:00+00:00</published>
        <updated>2025-09-01T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/fisher-ju-zhen-yu-dan-xing-quan-zhong-gong-gu-elastic-weight-consolidation-ewc/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/fisher-ju-zhen-yu-dan-xing-quan-zhong-gong-gu-elastic-weight-consolidation-ewc/</id>
        
            <content type="html">&lt;p&gt;‍&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;yu-bei-zhi-shi-si-ran-han-shu&quot;&gt;预备知识：似然函数&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;似然函数&lt;&#x2F;strong&gt;：似然函数 $L(\theta | D)$ 表示在参数为 $\theta$ 的模型下，观测到数据 $D$ 的概率，即&lt;&#x2F;p&gt;
&lt;p&gt;$$
L(\theta|D)=p(D|\theta)=\prod_{i=1}^Np(x_i|\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;对数似然&lt;&#x2F;strong&gt;：由于似然函数的计算是乘法，有各种各样的问题。我们希望将其转换为加法或者减法，而取对数是一种很自然的想法&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{L}(\theta|D)=\log L(\theta|D)=\sum_{i=1}^N\log p(x_i|\theta).
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;最大化似然估计 Maximum Likelihood Estimation, MLE&lt;&#x2F;strong&gt;：如果我们能够调整 $\theta$，那么我们肯定希望在 $\theta$ 下观测到 $D$ 的概率越高越好，也就是最大化似然函数&lt;&#x2F;p&gt;
&lt;p&gt;$$
\theta_{MLE}=\arg\max_\theta\mathcal{L}(\theta|D).
$$&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;yu-bei-zhi-shi-score-han-shu&quot;&gt;预备知识：Score 函数&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Score 函数的定义&lt;&#x2F;strong&gt;：定义 Score 函数为对数似然函数关于 $\theta$ 的梯度，其表示着在&lt;u&gt;当前 &lt;&#x2F;u&gt;​&lt;u&gt;$\theta$&lt;&#x2F;u&gt;​&lt;u&gt; 位置如何调整参数能最快地提高模型对数据的解释能力&lt;&#x2F;u&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;$$
s(\theta) := \nabla_\theta \mathcal{L}(\theta|D)
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Score 函数的期望&lt;&#x2F;strong&gt;：假设数据 $D$ 是由一个真实的参数 $\theta^\ast$ 生成的，那么 Score 函数在该点的期望为零：&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathbb{E}_{D\sim p(D|\theta^*)}[s(\theta^*)]=0,
$$&lt;&#x2F;p&gt;
&lt;p&gt;需要注意的是，这个期望为 $0$ 是一个关于所有可能数据集的&lt;u&gt;理论平均性质&lt;&#x2F;u&gt;。对于我们手中任何一个具体的数据集 $D$，由于采样的随机性，计算出的 $s(\theta^*)$ 的值几乎总是不为 $0$ 的。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;fisher-ju-zhen&quot;&gt;Fisher 矩阵&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Fisher 信息矩阵&lt;&#x2F;strong&gt;：Fisher 信息矩阵量化了&lt;u&gt;数据 &lt;&#x2F;u&gt;​&lt;u&gt;$D$&lt;&#x2F;u&gt;​&lt;u&gt; 中包含了多少模型参数 &lt;&#x2F;u&gt;​&lt;u&gt;$\theta$&lt;&#x2F;u&gt;​&lt;u&gt; 的信息&lt;&#x2F;u&gt;，其有两种等价的定义。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;定义 1. Score 函数的方差&lt;&#x2F;strong&gt;：定义 Fisher 信息矩阵为 Score 函数的协方差矩阵&lt;&#x2F;p&gt;
&lt;p&gt;$$
F(\theta)=\mathbb{E}_{x\sim p(x|\theta)}[s(\theta)s(\theta)^\top]
$$&lt;&#x2F;p&gt;
&lt;p&gt;在实际问题中，对于每个样本的梯度向量 $s_k(\theta)$，我们通过以下公式计算 Fisher 信息矩阵&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{F}(\theta)=\frac{1}{N}\sum_{k=1}^Ns_k(\theta)s_k(\theta)^\top
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;定义 2. 对数似然函数 Hessian 矩阵的负期望&lt;&#x2F;strong&gt;：我们也可以用对数似然函数的 Hessian 矩阵的负期望来定义 Fisher 矩阵&lt;&#x2F;p&gt;
&lt;p&gt;$$
F(\theta)=-\mathbb{E}_{x\sim p(x|\theta)}[\nabla_{\theta}^{2}\mathcal{L}(\theta|D)]
$$&lt;&#x2F;p&gt;
&lt;p&gt;在实际计算中，对数据集中每个样本 $x_k$ 计算对数似然的 Hessian 矩阵 $\nabla_\theta^2\mathcal{L}(\theta|x_k)=\nabla_\theta^2\log p(x_k|\theta)$，取负号后求和：&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{F}(\theta)=-\frac{1}{N}\sum_{k=1}^{N}\nabla_{\theta}^{2}\log p(x_{k}|\theta)
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Fisher 矩阵元素含义&lt;&#x2F;strong&gt;：Fisher 矩阵的对角元素 $F_{ii}$ 衡量了数据中关于某个参数 $\theta_i$ 的信息量，$F_{ii}$ 越大说明 $\theta_i$ 对模型越重要。非对角元素 $F_{ij}$ 表示了不同参数 $\theta_i$ 和 $\theta_j$ 估计值之间的相关性&#x2F;耦合度。&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;dan-xing-quan-zhong-gong-gu-elastic-weight-consolidation-ewc&quot;&gt;弹性权重巩固 Elastic Weight Consolidation, EWC&lt;&#x2F;h2&gt;
&lt;p&gt;EWC 是一种持续学习算法，其目标是在学习新任务（任务B）时，减缓对旧任务（任务A）知识的遗忘。这里介绍其数学基础与基本想法。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;EWC 核心思想&lt;&#x2F;strong&gt;：对于旧任务（任务A）中越重要的参数，在学习新任务（任务B）时就越要&lt;u&gt;限制它的改动&lt;&#x2F;u&gt;。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;EWC 损失函数&lt;&#x2F;strong&gt;：假设已经完成了任务 A 的训练得到了最优参数 $\theta_A^\ast$。现在要学习任务 B，则 EWC 的损失函数表示为&lt;&#x2F;p&gt;
&lt;p&gt;$$
L(\theta)=L_B(\theta)+\frac{\lambda}{2}\sum_iF_i(\theta_i-\theta_{A,i}^*)^2,
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $L_B(\theta)$ 是任务 B 的标准损失函数，$\frac{\lambda}{2}\sum_iF_i(\theta_i-\theta_{A,i}^*)^2$ 是 EWC 增加的&lt;u&gt;正则化惩罚项&lt;&#x2F;u&gt;，$\lambda$ 是正则化参数，$(\theta_i-\theta_{A,i}^*)^2$ 是当前参数 $\theta_i$ 与任务 A 最优参数 $\theta_{A,i}^\ast$ 的二次距离，$F_i$ 是&lt;u&gt;在 &lt;&#x2F;u&gt;​&lt;u&gt;$\theta_{A}^\ast$&lt;&#x2F;u&gt;​&lt;u&gt; 处的 Fisher 矩阵&lt;&#x2F;u&gt;的对角元素，充当了&lt;u&gt;重要性权重&lt;&#x2F;u&gt;。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;如果参数 $\theta_i$ 对任务 A 很重要（$F_i$ 很大），那么对它的任何改动 $(\theta_i - \theta_{A,i})^2$ 都会被放大，从而产生巨大的惩罚，迫使模型不要轻易改动它。&lt;&#x2F;li&gt;
&lt;li&gt;如果参数 $\theta_i$ 对任务 A 不重要（$F_i$ 很小），惩罚就小，模型可以自由调整它来适应任务 $B$。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;一些疑问和注意点&lt;&#x2F;strong&gt;：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;为什么只用对角元素：由于完整的 Fisher 矩阵过于庞大，EWC 中&lt;u&gt;只使用了 Fisher 矩阵的对角元素&lt;&#x2F;u&gt;以表示重要性，忽略了参数之间的相关性。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Fisher 矩阵是固定的吗：注意 EWC 使用的 Fisher 矩阵是 $\theta_A^\ast$ 处的，而&lt;u&gt;非每次参数更新时更新&lt;&#x2F;u&gt;的，这样能准确捕捉对 A 任务重要的参数。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;为什么用 Fisher 矩阵对角而非 Score 函数分量：理想情况下 $\theta_A^\ast$ 处的 Score 函数为 $0$（极值点），而 Fisher 矩阵衡量了&lt;u&gt;不同样本对某个参数的需求&lt;&#x2F;u&gt;，如果 $F_i$ 很大，则稍微一改动，便会影响多个样本的拟合结果。&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>LaSalle&#x27;s Invariance Principle</title>
        <published>2025-08-18T00:00:00+00:00</published>
        <updated>2025-08-18T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/blog-lasalle-s-invariance-principle/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/blog-lasalle-s-invariance-principle/</id>
        
            <content type="html">&lt;p&gt;LaSalle’s Invariance Principle 是 Lyapunov’s Second Method 的延伸和推广。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lasalle-s-invariance-principle-de-he-xin-si-xiang&quot;&gt;LaSalle’s Invariance Principle 的核心思想&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;回顾 Lyapunov’s Second Method&lt;&#x2F;strong&gt;：Lyapunov’s Second Method 告诉我们，如果能为一个 Dynamical System 找到一个能量函数（Lyapunov 函数） $V(x)$，并且这个能量函数沿着系统的轨迹是&lt;strong&gt;严格递减&lt;&#x2F;strong&gt;的（$\dot{V}(x) &amp;lt; 0$），那么系统会渐进稳定（Asymptotically Stable）于能量最低点（通常指原点）。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;LaSalle’s Invariance Principle 的思想&lt;&#x2F;strong&gt;：很多时候 $V(x)$ 不一定是严格递减的，例如 $\dot{V}(x) \leq 0$，这意味着系统的能量在某些地方可能暂时不变，LaSalle’s Invariance Principle 就是为了解决这个问题。其核心思想为：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;能量不增且有下界&lt;&#x2F;strong&gt;：$V(x)$ 永不增加且有下界，所以它一定收敛到某个常数；&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;寻找能量不变的区域&lt;&#x2F;strong&gt;：既然能量最终不再变化，那么 $x(t)$ 一定趋近哪些让能量不再变化的点，即 $E = \{x: \dot{V}(x) = 0\}$；&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;在不变区域中寻找归宿&lt;&#x2F;strong&gt;：系统轨迹 $x(t)$ 虽然会趋于 $E$，但是其不能在 $E$ 中乱跑，其最终肯定趋于 $E$ 内部的一个&lt;em&gt;不变集&lt;&#x2F;em&gt; $M$。即一旦 $x(t)$ 进入了 $M$，其就再也出不来了；&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;结论&lt;&#x2F;strong&gt;：即使 $\dot{V}(x)$ 只是半负定，我们也能断定系统轨迹 $x(t)$ 最终收敛到 $E$ 中的最大不变集 $M$。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;yu-bei-zhi-shi-bu-bian-ji&quot;&gt;预备知识：不变集&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;正不变集 Positively Invariant Set&lt;&#x2F;strong&gt;：对于集合 $\Omega \subset \mathbb{R}^n$ 和 Dynamical System $\dot{x} = f(x)$，若对于任何从集合内部出发的 $x(0) \in \Omega$，其后的整个轨迹 $x(t)$ 都停留在 $\Omega$ 内部，则称其是 &lt;em&gt;positively invariant&lt;&#x2F;em&gt; 的。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;最大不变集 Largest Invariant Set&lt;&#x2F;strong&gt;：给定集合 $E$，则 $M \subset E$ 是 $E$ 的&lt;em&gt;最大不变集&lt;&#x2F;em&gt;是所有完全包含于 $E$ 的系统轨迹的并。即如果一条轨迹始终在 $E$ 内部，则其属于 $M$，一旦其出过 $E$，则其就不属于 $M$ 了。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lasalle-s-invariance-principle-ding-li&quot;&gt;LaSalle’s Invariance Principle 定理&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;LaSalle’s Invariance Principle&lt;&#x2F;strong&gt;：考虑 $\dot{x} = f(x)$，其中 $x \in D \subset \mathbb{R}^n$，$f:D \to \mathbb{R}^n$ 满足局部 Lipschitz 条件。若存在紧集 $\Omega \subset D$ 关于该系统 positively invariant，以及连续可微的 $V: \Omega \to \mathbb{R}$ 满足&lt;&#x2F;p&gt;
&lt;p&gt;$$
\dot{V}(x) = \nabla V(x) \cdot f(x) \leq 0, \quad \forall x \in \Omega.
$$&lt;&#x2F;p&gt;
&lt;p&gt;那么对于任意初始条件 $x(0) \in \Omega$，系统轨迹 $x(t)$ 都会收敛到 $E = \{x \in \Omega: \dot{V}(x) = 0\}$ 的最大不变集 $M$。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;上述定理要求我们预先找到一个紧集 $\Omega$，但是在实际问题中往往很难构造出 $\Omega$，因此我们可以考虑下面更通用的版本。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LaSalle’s Invariance Principle (Generalized)&lt;&#x2F;strong&gt; ：考虑 $\dot{x} = f(x)$，其中 $x \in D \subset \mathbb{R}^n$，$f:D \to \mathbb{R}^n$ 满足局部 Lipschitz 条件。若存在连续可微的 $V: D \to \mathbb{R}$ 满足&lt;&#x2F;p&gt;
&lt;p&gt;$$
\dot{V}(x) = \nabla V(x) \cdot f(x) \leq 0, \quad \forall x \in \Omega.
$$&lt;&#x2F;p&gt;
&lt;p&gt;且对于初始条件 $x(0) \in D$，若其轨迹 $x(t)$ 是&lt;strong&gt;有界的&lt;&#x2F;strong&gt;且始终保持在 $D$ 内，则 $x(t)$ 收敛到 $E = \{x \in \Omega: \dot{V}(x) = 0\}$ 的最大不变集 $M$。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;如何保证轨迹有界？&lt;&#x2F;strong&gt; 若 $V(x)$ 是 Radially Unbounded 的，且 $\dot{V}(x) \leq 0$，则 $x(t)$ 是有界的。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;径向无界 Radially Unbounded&lt;&#x2F;strong&gt;：如果当 $\|{x}\| \to \infty$ 时，$V({x}) \to \infty$，则称 $V({x})$ 是 radially unbounded 的。&lt;&#x2F;p&gt;
&lt;p&gt;证明：因为当 $\|x\| \to \infty$ 时，$V(x) \to \infty$，这与 $\dot{V}(x) \leq 0$ 相矛盾。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>动力系统稳定性概览</title>
        <published>2025-08-16T00:00:00+00:00</published>
        <updated>2025-08-16T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/dong-li-xi-tong-wen-ding-xing-gai-lan/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/dong-li-xi-tong-wen-ding-xing-gai-lan/</id>
        
            <content type="html">&lt;h2 id=&quot;dong-li-xi-tong-yu-ping-heng-dian&quot;&gt;动力系统与平衡点&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;动力系统&lt;&#x2F;strong&gt;：在动力学系统理论中，我们一般考虑一个连续时间的动力系统&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\mathrm{d} \mathbf{x}}{\mathrm{d} t} = {f}(\mathbf{x},t).
$$&lt;&#x2F;p&gt;
&lt;p&gt;其中 $\mathbf{x}(t)$ 是系统的状态 state，$t$ 是时间，${f}: D \to \mathbb{R}^n$ 是连续可微的向量场。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;自治动力系统&lt;&#x2F;strong&gt;：通常更常见的动力系统是&lt;em&gt;自治 autonomous&lt;&#x2F;em&gt; 的，表示其行为只受状态 $\mathbf{x}(t)$ 的直接影响，而不受时间 $t$ 的直接影响，其形式为&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\mathrm{d} \mathbf{x}}{\mathrm{d} t} = {f}(\mathbf{x}), \quad f(\mathbf{0}) = \mathbf{0}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;不失一般性，我们这里直接令系统的初值为 $\mathbf{0}$。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;平衡点 Equilibrium Point&lt;&#x2F;strong&gt;：若系统一旦到达某点 $\mathbf{x}_e \in D$，它将永远保持在该点，则该点被称为&lt;em&gt;平衡点 equilibrium point&lt;&#x2F;em&gt;，满足&lt;&#x2F;p&gt;
&lt;p&gt;$$
f(\mathbf{x}_e) = \mathbf{0}
$$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;wen-ding-xing-de-gai-nian&quot;&gt;稳定性的概念&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;稳定性的思想&lt;&#x2F;strong&gt;：稳定性并不是一个单一的概念，而是有一个层次结构递进的。其核心思想是，当系统状态受到一个小的扰动 (perturbation) 偏离平衡点后，系统将如何响应。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Lyapunov 稳定性：最基本的稳定性，直观含义为如果系统初始状态足够接近平衡点，那么它未来的所有状态都将保持在平衡点附近的一个任意小的邻域内&lt;&#x2F;li&gt;
&lt;li&gt;Asymptotic Stability 渐进稳定：比 Lyapunov 稳定性更强，如果系统初始状态足够接近平衡点，系统不仅保持在平衡点附近，而且随着时间推移，最终会收敛到平衡点。&lt;&#x2F;li&gt;
&lt;li&gt;Exponential Stability 指数稳定：比渐近稳定更强的稳定性，如果系统初始状态足够接近平衡点，系统不仅收敛到平衡点，而且其收敛速度至少像指数函数 $e^{-\lambda t}$ 一样快，其中 $\lambda &amp;gt; 0$。&lt;&#x2F;li&gt;
&lt;li&gt;Global Stability 全局稳定：前述稳定性都是局部的，全局稳定性将这一范围扩展至整个状态空间 $\mathbb{R}^n$。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Lyapunov 稳定性&lt;&#x2F;strong&gt;：系统在平衡点 $\mathbf{x} = \mathbf{0}$ 是 Lyapunov 稳定的如果对于任意 $\epsilon &amp;gt; 0$，存在 $\delta &amp;gt; 0$，若 $\|\mathbf{x}(t_0)\| &amp;lt; \delta$ 满足&lt;&#x2F;p&gt;
&lt;p&gt;$$
\|\mathbf{x}(t) \| &amp;lt; \epsilon, \quad t \geq t_0.
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Asymptotic Stability&lt;&#x2F;strong&gt;：系统在平衡点 $\mathbf{x} = \mathbf{0}$ 是 Asymptotic 稳定的如果其是 Lyapunov 稳定的，且具有吸引性，即存在 $\delta &amp;gt; 0$，若 $\| \mathbf{x}(t_0) \| &amp;lt; \delta$，有&lt;&#x2F;p&gt;
&lt;p&gt;$$
\lim\limits_{t \to \infty} \mathbf{x}(t) = \mathbf{0}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Exponential Stability&lt;&#x2F;strong&gt;：系统在平衡点 $\mathbf{x} = \mathbf{0}$ 是指数稳定的如果存在 $\alpha &amp;gt; 0$，$\lambda &amp;gt; 0$，$\delta &amp;gt; 0$ 使得 $\|\mathbf{x}(t_0)\| &amp;lt; \delta$ 时，&lt;&#x2F;p&gt;
&lt;p&gt;$$
\|\mathbf{x}(t)\| \leq \alpha \|\mathbf{x}(t_0)\|e^{-\lambda (t - t_0)}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;这里 $\lambda$ 称为 rate of convergence，$\alpha$ 反映系统的瞬态响应特性。&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于线性时不变系统 $\dot{\mathbf{x}} = A \mathbf{x}$​，&lt;strong&gt;渐近稳定&lt;&#x2F;strong&gt;等价于&lt;strong&gt;指数稳定&lt;&#x2F;strong&gt;，且当且仅当矩阵 A 的所有特征值都具有负实部。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Global Stability&lt;&#x2F;strong&gt;：系统在平衡点 $\mathbf{x} = \mathbf{0}$ 是&lt;em&gt;全局渐进稳定&lt;&#x2F;em&gt;​的如果其是 Lyapunov 稳定的，且对任意初始状态  $\mathbf{x}(t_0) \in \mathbb{R}^n$，有&lt;&#x2F;p&gt;
&lt;p&gt;$$
\lim\limits_{t \to \infty} \mathbf{x}(t) = \mathbf{0}
$$&lt;&#x2F;p&gt;
&lt;p&gt;类似的，也可以定义全局指数稳定。&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lyapunov-s-second-method&quot;&gt;Lyapunov’s Second Method&lt;&#x2F;h2&gt;
&lt;p&gt;李雅普诺夫第二方法（也称直接法）允许我们不求解微分方程本身来分析稳定性。该方法的核心是寻找一个标量函数，即李雅普诺夫函数。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;径向无界 Radially Unbounded&lt;&#x2F;strong&gt;：如果当 $\|\mathbf{x}\| \to \infty$ 时，$V(\mathbf{x}) \to \infty$，则称 $V(\mathbf{x})$ 是 radially unbounded 的。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Lyapunov 稳定性定理&lt;&#x2F;strong&gt;：考虑自治系统&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\mathrm{d} \mathbf{x}}{\mathrm{d} t} = {f}(\mathbf{x}), \quad f(\mathbf{0}) = \mathbf{0}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;设 $V(\mathbf{x})$ 是在包含原点的区域 $D \subseteq \mathbb{R}^n$ 上连续可微的标量函数。定义 $V(\mathbf{x})$ 沿系统轨迹的导数为&lt;&#x2F;p&gt;
&lt;p&gt;$$
\dot{V} = \frac{\mathrm{d}}{\mathrm{d} t}V(\mathbf{x}(t)) = \nabla V(\mathbf{x}) \cdot f(\mathbf{x}) = \frac{\partial V}{\partial \mathbf{x}} \dot{\mathbf{x}}.
$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;如果 $D$ 内 $V(\mathbf{x})$ 是正定，$\dot{V}(\mathbf{x})$ 是半负定的，则 $\mathbf{x} = \mathbf{0}$ 是 Lyapunov 稳定的；&lt;&#x2F;li&gt;
&lt;li&gt;如果 $D$ 内 $V(\mathbf{x})$ 是正定，$\dot{V}(\mathbf{x})$ 是负定的，则 $\mathbf{x} = \mathbf{0}$ 是渐进稳定的；&lt;&#x2F;li&gt;
&lt;li&gt;如果 $\mathbb{R}^n$ 内 $V(\mathbf{x})$ 是正定且径向无界，$\dot{V}(\mathbf{x})$ 是负定的，则 $\mathbf{x} = \mathbf{0}$ 是全局渐进稳定的；&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry>
</feed>
