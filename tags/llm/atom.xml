<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>LLM</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - LLM</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/llm/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/llm/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-09-13T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/llm/atom.xml</id><entry xml:lang="zh">
        <title>论文阅读-Benchmarking Optimizers for Large Language Model Pretraining</title>
        <published>2025-09-13T00:00:00+00:00</published>
        <updated>2025-09-13T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md/</id>
        
            <content type="html">&lt;blockquote&gt;
&lt;p&gt;Original Paper: &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2509.01440&quot;&gt;[2509.01440] Benchmarking Optimizers for Large Language Model Pretraining&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Chinchilla Scaling Law&lt;&#x2F;strong&gt;: The optimal amount of training data for a given model size that yields the best performance under a fixed computational budget. To be more specific, we &lt;u&gt;need around 20 text tokens per parameter&lt;&#x2F;u&gt; (see &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2203.15556&quot;&gt;2203.15556&lt;&#x2F;a&gt;)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Overview&lt;&#x2F;strong&gt;: We discuss the algorithms according to their logical grouping:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Adam-like methods: &lt;code&gt;AdamW&lt;&#x2F;code&gt;​, &lt;code&gt;ADOPT&lt;&#x2F;code&gt;​, &lt;code&gt;AdEMAMix&lt;&#x2F;code&gt;​&lt;&#x2F;li&gt;
&lt;li&gt;Sign-based methods: &lt;code&gt;Lion&lt;&#x2F;code&gt;​, &lt;code&gt;Signum&lt;&#x2F;code&gt;​&lt;&#x2F;li&gt;
&lt;li&gt;Approximate second-order optimizers: &lt;code&gt;Muon&lt;&#x2F;code&gt;​, &lt;code&gt;SOAP&lt;&#x2F;code&gt;​, &lt;code&gt;Sophia&lt;&#x2F;code&gt;​&lt;&#x2F;li&gt;
&lt;li&gt;Learning rate&#x2F; scheduler-free learning algorithms: &lt;code&gt;Schedule-Free AdamW&lt;&#x2F;code&gt;​, &lt;code&gt;Prodigy&lt;&#x2F;code&gt;​&lt;&#x2F;li&gt;
&lt;li&gt;MARS methods: &lt;code&gt;MARS&lt;&#x2F;code&gt;​&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;ADOPT&lt;&#x2F;strong&gt;: Remove the current gradient $g_t$ from the second moment estimate $v_t$ and alter the order of the momentum update $m_t$ and normalization.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916123516-1ogt8zb.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;AdEMAMix (Dual EMA)&lt;&#x2F;strong&gt; : This work argues that using a single EMA to accumulate past gradients in the first moment estimate $m$ can be suboptimal, as it cannot simultaneously prioritize both immediate past and older gradients.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916123659-e0xo7hc.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Lion&lt;&#x2F;strong&gt;: Lion is a sign-based method, which determines the update direction by taking the sign of an interpolation between the previous momentum and the current gradient.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916124348-zffip98.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Signum&lt;&#x2F;strong&gt;: This method differs from Lion in the interpolation term between the EMA of momentum and the current gradient.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916124534-axihdsz.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Muon and D-Muon&lt;&#x2F;strong&gt;: In Muon’s original code, weight decay does not apply to the matrix parameters in &lt;code&gt;MuonNon1D&lt;&#x2F;code&gt;​. This weight decay issue is addressd in &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2502.16982&quot;&gt;[2502.16982] Muon is Scalable for LLM Training&lt;&#x2F;a&gt;, in which the authors present a scheme for sharing the learning rate and weight decay between the matrix and non-matrix parameters of the model.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916124916-2mip0jk.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916124938-os749zg.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;SOAP&lt;&#x2F;strong&gt;: SOAP improves Shampoo and reduces the computational overhead by optimizing only two-dimensional layers while running AdamW for 1D layers.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916125252-l4h5uac.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Sophia&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916125310-aw2os2i.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Schedule-Free AdamW&lt;&#x2F;strong&gt;: The idea of &lt;code&gt;Shcedule-Free AdamW&lt;&#x2F;code&gt;​ is to eliminate learning rate schedulers by replacing them with iterate averaging.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916130502-zuppq93.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Prodigy&lt;&#x2F;strong&gt;: Prodigy removes the need for hand-tuned learning rates through an intrinsic, adaptive step-size scheme.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916130520-t4awlkj.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;MARS&lt;&#x2F;strong&gt;: MARS incorporates modern adaptive and approximate second-order methods with a variance reduction update style.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916130536-nh961yf.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;results-at-small-scale-124m-models&quot;&gt;Results at Small Scale: 124M Models&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Notation&lt;&#x2F;strong&gt;: Hereafter, “$A \times B$ tokens” indicates the batch size is $A$, and each batch contains $B$ tokens.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Results with Small and Large Batches&lt;&#x2F;strong&gt; and &lt;strong&gt;Stability across Training Horizons&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250912181322-uddtshy.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Comparing optimizers for training a 124M parameter LLM: (a) &amp;amp;quot;small&amp;amp;quot; batch size (b) &amp;amp;quot;large&amp;amp;quot; batch size.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250908175428-l9hiovd.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Ranking of optimizers for 124M models with &amp;amp;quot;small&amp;amp;quot; and &amp;amp;quot;large&amp;amp;quot; batch sizes.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway (Batch Size)&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;AdEMAMix&lt;&#x2F;code&gt;​ consistently achieves state-of-the-art performance and robust scaling with training duration.&lt;&#x2F;li&gt;
&lt;li&gt;Sign-based methods (&lt;code&gt;Signum&lt;&#x2F;code&gt;​, &lt;code&gt;Lion&lt;&#x2F;code&gt;​) and &lt;code&gt;MARS&lt;&#x2F;code&gt;​ greatly benefit from the &lt;u&gt;increased batch size&lt;&#x2F;u&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;Sophia&lt;&#x2F;code&gt;​ diverges in small-batch setting, when trained beyond the Chinchilla optimal horizon, even with sufficiently small learning rate;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;SOAP&lt;&#x2F;code&gt;​ show a consistent performance in both settings.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Takeaway (Stability)&lt;&#x2F;strong&gt; : Once optimizers are properly re-tuned for the maximal length of training considered, doubling of number of iterations does not affect the ranking of methods.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Increasing the Batch Size Further&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250912182035-3u788qq.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Scaling batch size&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;: Many methods, especially &lt;code&gt;MARS&lt;&#x2F;code&gt;​, &lt;code&gt;Prodigy&lt;&#x2F;code&gt;​, and &lt;code&gt;sign-based&lt;&#x2F;code&gt;​ ones, can outperform &lt;code&gt;AdamW&lt;&#x2F;code&gt;​ while trained on a sufficiently large batches.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Weight Decay Ablation&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;Here the baseline &lt;code&gt;AdamW&lt;&#x2F;code&gt;​ uses a weight decay of $\lambda = 0.1$.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250912182611-33p1mpd.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Larger weight decay achieves significantly better results when training on fewer tokens: (a) AdamW, Signum, Lion with large weight decay outperform baseline AdamW with weight decay of 0.1 for short training duration. (b) the setting without weight decay is suboptimal. (c) Smaller weight decay leads to larger L2 norm of the model parameter.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250912182642-ieebifg.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Importance of weight decay for Muon. (1) D-Muon uses a weight decay for all parameters, (2) Muon uses weight decay only on embeddings, scalar parameters, and the final layer. We can see that D-Muon greatly outperforms the basic Muon.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The use of weight decay (particularly a large weight decay term 0.5 and above), can significantly impact the final loss and optimizer behavior.&lt;&#x2F;li&gt;
&lt;li&gt;The setting of weight decay to be $0$ is suboptimal.&lt;&#x2F;li&gt;
&lt;li&gt;For extended training horizons, non-zero weight of $0.1$ proves to be a robust option.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Learning Rate Sensitivity&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250912194544-c9ym1ai.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Optimal learning rate stability across optimizers. The optimal learning rate determined during tuning on 2.1B tokens remains consistent after a learning rate sweep on 16.8B tokens for most optimizers.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;For most optimizer, the learning rate $\gamma_{\max}$ selected near the Chinchilla optimal horizon transfers smoothly to $8 \times$longer run.&lt;&#x2F;li&gt;
&lt;li&gt;Sign-based methods and &lt;code&gt;Sophia&lt;&#x2F;code&gt;​ diverge with $\gamma_{\max} = 2e^{-3}$.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;MARS&lt;&#x2F;code&gt;​ demonstrates a very consistent performance across $\gamma$ sweep.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Warmup Ablation&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250912195833-rxrm1vz.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Warmup ablation: sign-based optimizers, Sophia and SF-AdamW benefit from the increased warmup.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;: We reveal that the warmup duration is optimizer-dependent and should be tuned: for &lt;code&gt;SF-AdamW&lt;&#x2F;code&gt;​, &lt;code&gt;Sophia&lt;&#x2F;code&gt;​, and &lt;code&gt;Signum&lt;&#x2F;code&gt;​, longer warmup results in improved final performance.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Warmup Types of WSD(Warmup-Stable-Decay), cosine, and linear&lt;&#x2F;strong&gt; **$\gamma$**​ &lt;strong&gt;-scheduler&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913155616-x3m2l3m.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Comparisons between cosine, WSD, and the linear schedulers.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913160100-ljc3uwl.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Gradient norm patterns for different schedulers: (b) the gradient evolution for majority of optimizers resembles the SF-AdamW pattern (a,c) Exceptions are sign-based methods: Signum and Lion.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;: A choice of the learning rate scheduler is also optimizer-related&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;For most methods, the cosine scheduler dominates.&lt;&#x2F;li&gt;
&lt;li&gt;Linear scheduler outperforms or matches cosine and WSD for sign-based methods, &lt;code&gt;SOAP&lt;&#x2F;code&gt;​ and &lt;code&gt;MARS&lt;&#x2F;code&gt;​.&lt;&#x2F;li&gt;
&lt;li&gt;WSD appears to be the best option for &lt;code&gt;Muon&lt;&#x2F;code&gt;​&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;results-at-medium-scale-210m-models&quot;&gt;Results at Medium Scale: 210M Models&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;&#x2F;strong&gt;​&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913160454-797022m.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Ranking of optimizers for 210M models with the batch size of 256*512 tokens.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913160605-3jwtcqf.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Comparing optimizers for training a 219M parameter LLM.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;We do not observe a much of a change in ranking of optimizers for 210M model, compared to benchmarking on 124M.&lt;&#x2F;li&gt;
&lt;li&gt;We replicated almost identical hyperparameters for all optimizers, except for the learning rate for sign-based methods (which is more sensitive to the learning rate while scaling the model size)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Decay the learning rate sufficiently&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913161002-rhf7rcr.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Decaying the learning rate down to 0.01 and beyond, instead of only to 0.1&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;: Decaying the learning rate further than $10\%$ of the maximal significantly improves the results. However, for different schedulers, the best final learning rate is different.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;results-at-large-scale-583m-and-720m-parameters&quot;&gt;Results at Large Scale: 583M and 720M Parameters&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913161558-oyap0pm.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Ranking of optimizers for 720M Llama-based models.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913161416-9a93emu.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Comparing optimizers for training a 720M parameter LLM.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;At larger scale of model and batch size, &lt;code&gt;AdEMAMix&lt;&#x2F;code&gt;​ and &lt;code&gt;MARS&lt;&#x2F;code&gt;​ dominate.&lt;&#x2F;li&gt;
&lt;li&gt;Despite training with large batches, &lt;code&gt;Signum&lt;&#x2F;code&gt;​ and &lt;code&gt;Lion&lt;&#x2F;code&gt;​ scale poorly.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;D-Muon&lt;&#x2F;code&gt;​ is consistent across all our benchmarking setups.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Wall-clock time comparison&lt;&#x2F;strong&gt;​&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913161744-b9b1nwq.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Wall-clock time comparison.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;: Most optimizers exhibit similar wall-time performance, with sign-based methods being slightly faster. &lt;code&gt;SOAP&lt;&#x2F;code&gt;​ is the main exception.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;extension-to-moes&quot;&gt;Extension to MoEs&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;MoE&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-benchmarking-optimizers-for-large-language-model-pretraining-md&#x2F;assets&#x2F;image-20250916132019-0l4mkul.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Results&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913162006-wshll9d.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Ranking optimizers for 520M MoE models with 256*512 batch size.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250913162059-zjiwg4o.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Comparing optimizers for training a 520M parameter MoE.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Takeaway&lt;&#x2F;strong&gt;: Benchmarking results obtained for dense models transfer to corresponding MoEs.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>论文阅读：The Super Weight in Large Language Models</title>
        <published>2025-09-06T00:00:00+00:00</published>
        <updated>2025-09-06T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/lun-wen-yue-du-the-super-weight-in-large-language-models/</id>
        
            <content type="html">&lt;blockquote&gt;
&lt;p&gt;Original Paper: &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2411.07191&quot;&gt;[2411.07191] The Super Weight in Large Language Models&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Large Outliers in Large Models&lt;&#x2F;strong&gt;: Once LLMs reach a certain scale, a small set of hidden state features contains outliers of exceptionally large magnitude. These outliers account for a small percentage of all activations but are crucial for preserving the compressed model’s quality.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Super Weights&lt;&#x2F;strong&gt;: Not all large outliers are equally important. In this paper, we study a tiny yet important set of outliers in LLMs, termed &lt;em&gt;super weights&lt;&#x2F;em&gt;. In Llama-7B, pruning the super weight, a single scalar, completely destroys the model’s ability to generate text.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906104045-xl09xfx.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Super Weight Phenomenon: Pruning a single super weight can completely destroy a LLM&amp;#x27;s ability to generate text. On the left, the original Llama-7B produces a reasonable completion. On the right, after pruning the super weight, Llama-7B generates complete gibberish.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;Super Activations&lt;&#x2F;strong&gt;: &lt;em&gt;Super activations&lt;&#x2F;em&gt; are exceptionally massive activations. They persist across many layers, feature constant magnitude, and always exist at the same position regardless of input.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906111713-x3jh79f.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Super activations: Exceptionally massive activations&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;Super Weights behave similarly across model families and sizes&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;They are always found in the &lt;code&gt;mlp.down_proj&lt;&#x2F;code&gt;​ weight.&lt;&#x2F;li&gt;
&lt;li&gt;They produce exceptionally large magnitude activation–the &lt;em&gt;super activation&lt;&#x2F;em&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;They suppress stopword likelihood.&lt;&#x2F;li&gt;
&lt;li&gt;Pruning the super weight destroys quality by &lt;u&gt;dampening the super activation&lt;&#x2F;u&gt;（super activation 几乎消失了） and &lt;u&gt;shifting almost all logit probability mass to stopwords&lt;&#x2F;u&gt;（几乎只输出 stopwords）.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906111816-u8ibtvm.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;How Super Weights behave: (1) Super weights are often found in an early layer&amp;#x27;s down projection (2) Super activations are propagated through skip connections. (3) This has a effect of suppressing stopword likelihoods in the final logits.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;mlp.down_proj&lt;&#x2F;code&gt;​：在 Transformer 架构中，Feed Forward 层（也就是 MLP 层）一般是两层神经网络，表示为 $\operatorname{FFN}(x) = W_2(\operatorname{ReLU}(W_1x + b_1)) + b_2)$，即先经过一个 &lt;code&gt;up_proj&lt;&#x2F;code&gt;​ 进行升维，一个 &lt;code&gt;activation&lt;&#x2F;code&gt;​ 进行非线性变换，再经过一个 &lt;code&gt;down_proj&lt;&#x2F;code&gt;​ 进行降维。&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-the-super-weight-in-large-language-models&#x2F;assets&#x2F;image-20250906103003-ltmazal.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;Stopword 指的是那些非常常见，但是信息量很低的词，例如 &lt;code&gt;the&lt;&#x2F;code&gt;​、&lt;code&gt;a&lt;&#x2F;code&gt;​、&lt;code&gt;is&lt;&#x2F;code&gt;​、&lt;code&gt;of&lt;&#x2F;code&gt;​ 等。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Super Outliers&lt;&#x2F;strong&gt;: We refer to both super weights and super activations as &lt;em&gt;super outliers&lt;&#x2F;em&gt;, which are critical to model quality. Fortunately, there are no more than a handful of scalar super outliers per tensor.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;contribution-of-this-work&quot;&gt;Contribution of This Work&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Super Weights&lt;&#x2F;strong&gt;: We discover a tiny subset of outliers in LLMs, at most six scalars, that are disproportionately important; pruning these super weights destroys model quality.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Identifying Super Weights&lt;&#x2F;strong&gt;: We present a data-free way to identify super weights using only a single forward pass and provide an index of super weights for existing, open LLMs.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Super Activations&lt;&#x2F;strong&gt;: We analyze how super weights influence inference and relate them to the activation outliers observed in prior work.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Compression&lt;&#x2F;strong&gt;: By preserving super outliers, we show that round-to-nearest quantization increases effectiveness noticeably; preserving super outliers improves compression quality.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;identification-of-super-weights&quot;&gt;Identification of Super Weights&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Super Weights Create Super Activations&lt;&#x2F;strong&gt;: The super activations’ channel (the position in vector) aligns with the super weights’, and the activation first appears right after the super weights.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906105604-xtl5z4e.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Pruning the super weight decreases the super activation&amp;#x27;s magnitude by 75%.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;The Mechanism behind Super Activations&lt;&#x2F;strong&gt;: The Hadamard product of the &lt;code&gt;gate&lt;&#x2F;code&gt;​ and &lt;code&gt;up&lt;&#x2F;code&gt;​ projection creates a relatively large activation. The super weights further amplify it and create super activations.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Gate 指的是对输入信息进行过滤和放缩的操作，假设 &lt;code&gt;A&lt;&#x2F;code&gt;​ 是输入向量；&lt;code&gt;g&lt;&#x2F;code&gt;​ 是与 &lt;code&gt;A&lt;&#x2F;code&gt;​ 等长的门向量，其元素在 &lt;code&gt;0~1&lt;&#x2F;code&gt;​ 之间。那么 gate 操作后的输出为 &lt;code&gt;B = A ⊙ g&lt;&#x2F;code&gt;​，其中 &lt;code&gt;⊙&lt;&#x2F;code&gt;​ 是逐元素（Hadamard）乘法。&lt;&#x2F;p&gt;
&lt;p&gt;虽然经典 Transformer 架构中的 FFN 层没有 gate 层，但是现代 Transformer，如 Llama、Mistral 等模型中有。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Identifying Super Weight by Activation Spikes&lt;&#x2F;strong&gt;: Super weights can be located by detecting the spikes in the &lt;code&gt;down_proj&lt;&#x2F;code&gt;​ inputs and outputs distributions across the layers. This dectection only requires a single input prompt, rather than a set of validation data or use-case examples.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Identifying Steps&lt;&#x2F;strong&gt;: Let $W \in \mathbb{R}^{D \times H}$ be the &lt;code&gt;down_proj&lt;&#x2F;code&gt;​ weight matrix, where $D$ is the dimension of the activation feature and $H$ is the intermediate hidden dimension. Let $X \in \mathbb{R}^{L \times H}$ be the input matrix, where $L$ is the sequence length. Then the output of &lt;code&gt;down_proj&lt;&#x2F;code&gt;​ is&lt;&#x2F;p&gt;
&lt;p&gt;$$
Y = XW^\top, \quad
\text{where} \quad Y_{ij} = \sum_{k=1}^d X_{ik}W_{jk}.
$$&lt;&#x2F;p&gt;
&lt;p&gt;Suppose $Y_{ij}$ is a super activation, $X_{ik}$ and $W_{jk}$ are outliers, then $Y_{ij} \approx X_{ik}W_{jk}$.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Plot extreme outliers in the input and output activations of &lt;code&gt;mlp.down_proj&lt;&#x2F;code&gt;​.&lt;&#x2F;li&gt;
&lt;li&gt;Determine the layer and coordinates of the super weights.&lt;&#x2F;li&gt;
&lt;li&gt;Remove detected super weights and repeat the above process, until the magnitudes of large maximum activations are greatly suppressed.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906111713-x3jh79f.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;How to identify the super weights: The input has a large activation on layer 2. The value&amp;#x27;s channel index tells the row of super weight. The output has a large activation at layer 2. This value&amp;#x27;s channel index gives us the column of the super weight.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;mechanisms-of-super-weights&quot;&gt;Mechanisms of Super Weights&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Super Weights (partially) Operate via Super Activations&lt;&#x2F;strong&gt;: We want to assess the super weight’s impact on model quality is solely mediated by super activations or other factors. We conduct experiments under three conditions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Original model&lt;&#x2F;li&gt;
&lt;li&gt;Remove super weights (Prune SW): Set the weight scalar as zero.&lt;&#x2F;li&gt;
&lt;li&gt;Remove super weights and restore super activation (Prune SW, +SA): Set the weight scalar as zero, and restore super activation at the layer where it first appears.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The results show that super activations contribute substantially to the model’s performance, they do not fully account for the super weight’s overall influence on quality.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906151402-b1r818u.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Super Weight Importance: &amp;amp;quot;Prune SW&amp;amp;quot; indicates pruning single super weight, &amp;amp;quot;Prune Non-SW&amp;amp;quot; indicates pruning other 7,000 largest-magnitude weights, &amp;amp;quot;Prune SW,+SA&amp;amp;quot; indicates pruning super weight but restoring super activation. The experiment is conducted to assess the model&amp;#x27;s accuracy on seven zero-shot datasets and perplexity on C4 and Wiki-2.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;Super Weights Affect Output Token Probability Distributions&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906153301-pkr83su.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Super Weights Suppress Stopwords: Removing super weights results in 2 to 5 times larger stopword probabilities, while non-stopwords decrease by 2 to 3 times.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;Sensitivity of Super Weights&lt;&#x2F;strong&gt;: We investigate how does increasing the magnitude of super weights affect model quality.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906154045-bid5jgo.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Amplifying Super Weight Improves Quality: There exists some scaling where quality is improved.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;super-outlier-aware-quantization&quot;&gt;Super-Outlier Aware Quantization&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Quantization&lt;&#x2F;strong&gt;: The presence of outliers significantly degrade quantization quality. However, super outliers carry significant importance for model quality, making their preservation during&lt;br &#x2F;&gt;
quantization critical.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Round-to-Nearest Quantization&lt;&#x2F;strong&gt;: Here we consider the asymmetric round-to-nearest quantization&lt;&#x2F;p&gt;
&lt;p&gt;$$
Q(\mathbf{X})=\mathrm{Round}\left(\frac{\mathbf{X}-\mathrm{MIN}(\mathbf{X})}{\Delta}\right),Q^{-1}(\mathbf{\hat{X}})=\Delta\cdot\mathbf{\hat{X}}+\mathrm{MIN}(\mathbf{X})
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $\mathbf{X}$ is the tensor to be quantized, $\mathrm{MIN}(\mathbf{X})$ is the smallest element in $\mathbf{X}$, and $\Delta=\frac{\mathrm{MAX}(\mathbf{X})-\mathrm{MIN}(\mathbf{X})}{2^{N-1}-1}$ is the quantization step with $N$ being the number of bits. So super outliers in $\mathbf{X}$ drastically increase the step size, increasing the quantization error.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;量化步长 $\Delta$ 衡量了量化后两个相邻离散值之间的距离，可以类比于图像的分辨率。假设 $\mathbf{X}$ 中大部分值都在 $[-1.0, 1.0]$ 区间内，突然出现了一个超大的离群值（outlier），这会导致量化步长 $\Delta$ 激增，从而导致精度下降严重。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Solution to Outlier Quantization&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Hold out the super outlier to prevent adverse effects on inlier quantization.&lt;&#x2F;li&gt;
&lt;li&gt;Restore the super outlier’s value after dequantization.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Activation Quantization&lt;&#x2F;strong&gt;: We replace the super activation with the median, then quantize, dequantize and restore it.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{A}=\mathrm{RESTORE}(Q^{-1}(Q(\mathrm{REPLACE}(A)))
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Weight Quantization&lt;&#x2F;strong&gt;: First, we identify super weights. Second, we clip the outlier weights, quantize, and dequantize the clipped weights. Third, restore the half-precision super weights after dequantization.&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{W}=\mathrm{RESTORE}(Q^{-1}(Q(\mathrm{CLIP}_z(W)))
$$&lt;&#x2F;p&gt;
&lt;p&gt;We parameterize clipping using a z-score.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;z-score 方法来自于统计学，给定一个阈值 &lt;code&gt;z&lt;&#x2F;code&gt;​，其衡量每个元素偏离平均值的程度。一旦某个元素的偏离值超过阈值 &lt;code&gt;z&lt;&#x2F;code&gt;​，则会被视作离群值。上面的 &lt;code&gt;CLIP&lt;&#x2F;code&gt;​ 操作即将离群值裁剪掉。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906175810-cc0bc8e.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Round-to-nearest with super-activation handling is competitive. Here, &amp;amp;quot;Naive W8A8&amp;amp;quot; indicates the naive round-to-nearest quantification, &amp;amp;quot;SmoothQuant&amp;amp;quot; is an advanced quantification method.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;&#x2F;h2&gt;
&lt;p&gt;We first evaluate the perplexity (PPL) of different quantization method for Wiki-2 and C4.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906181755-c29ddf6.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Handling the super activation improves activation quantization: FP16 indicates the un-quantized model.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;We also evaluate the accuracy on zero-shot benchmarks.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250906182114-jbxzt7u.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Restoring super weight improves block scaling: Here &amp;amp;quot;RTN&amp;amp;quot; refers to &amp;amp;quot;round-to-nearest&amp;amp;quot;.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;Block size 是量化中的一个概念，指的是为了更好地适应 weight tensor 的局部变化，将大的 weight tensor 切割为多个小块独立进行量化&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;大 Block Size：量化粗超，但是高效&lt;&#x2F;li&gt;
&lt;li&gt;小 Block Size：量化精细，但是计算和存储开销大&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
</content>
        </entry>
</feed>
