<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Continual Learning</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - Continual Learning</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/continual-learning/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/continual-learning/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-09-10T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/continual-learning/atom.xml</id><entry xml:lang="zh">
        <title>论文阅读：RL&#x27;s Razor Why Online Reinforcement Learning Forgets Less</title>
        <published>2025-09-10T00:00:00+00:00</published>
        <updated>2025-09-10T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less/</id>
        
            <content type="html">&lt;blockquote&gt;
&lt;p&gt;Original Paper: &lt;a href=&quot;https:&#x2F;&#x2F;www.arxiv.org&#x2F;abs&#x2F;2509.04259&quot;&gt;[2509.04259] RL&#x27;s Razor: Why Online Reinforcement Learning Forgets Less&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Posts: &lt;a href=&quot;https:&#x2F;&#x2F;jyopari.github.io&#x2F;posts&#x2F;rl_razor&quot;&gt;RL&#x27;s Razor: Why On-Policy Reinforcement Learning Forgets Less&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;On-Policy RL&lt;&#x2F;strong&gt;: The agent learns only from experience &lt;u&gt;generated by its currect policy&lt;&#x2F;u&gt;. It follows its current strategy, collects data, updates its strategy, and then discards the old data.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Off-Policy RL&lt;&#x2F;strong&gt;: The agent can learn from experience &lt;u&gt;generated by a different policy&lt;&#x2F;u&gt;. This could be data from an older version of itself, from a human demonstrator, or from another AI.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;On-Policy RL 只学习由【当前最新版本】的策略所产生的经验，Off-Policy 允许学习由【任何策略】（包括过去的自己、人类专家等）所产生的经验。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Online RL&lt;&#x2F;strong&gt;: The agent learns by &lt;u&gt;actively interacting with a live environment&lt;&#x2F;u&gt;. It takes an action, gets a result, and learns from it in real-time.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Offline RL&lt;&#x2F;strong&gt;: The agent learns from a &lt;u&gt;fixed, pre-collected dataset of past interactions&lt;&#x2F;u&gt;. It has no ability to explore or gather new data.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Online 和 On-Policy 的辨析：&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Online 决定了经验是如何产生的：是通过与环境的实时互动，而非提前准备好&lt;&#x2F;li&gt;
&lt;li&gt;On-Policy 决定了采集到的经验是如何被使用的，是“用完即弃”（On-Policy），还是“存起来反复用”（Off-Policy）&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Previous Approaches to Catastrophic Forgetting&lt;&#x2F;strong&gt;: Previous approaches such as &lt;u&gt;constraining weight updates&lt;&#x2F;u&gt;, &lt;u&gt;preserving learned features&lt;&#x2F;u&gt;, or &lt;u&gt;regularizing shift in output distribution&lt;&#x2F;u&gt; focus on its effects rather than its underlying cause. Some prior work claimed that forgetting can be determined by &lt;u&gt;how much the model’s distribution shifts on past tasks&lt;&#x2F;u&gt;, but in practice this is infeasible because the set of prior tasks is vast or even unbounded.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;EWC&lt;&#x2F;strong&gt;: Elastic weight consolidation can be seen as approximations to KL minimization.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;SFT Versus RL&lt;&#x2F;strong&gt;: Prior comparisons between SFT and RL have focused on new task performance rather than the extent of forgetting. It is found that on-policy learning can &lt;u&gt;achieve stronger performance when the expert providing supervision is the same&lt;&#x2F;u&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;ParityMNIST&lt;&#x2F;strong&gt;: ParityMNIST is derived from MNIST, but reframes the task as predicting parity (even vs. odd).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;contribution-of-this-work&quot;&gt;Contribution of This Work&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;RL Forgets Less than SFT&lt;&#x2F;strong&gt;: Even when SFT and RL achieve the same performance on the new task, we observe that SFT often achieves new-task gains by erasing prior knowledge, while RL better preserves old skills.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250909102057-78oqku5.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Bias toward KL-minimal solutions reduces forgetting: (1) Left: RL converges to those closest in KL to the base model. (2) Right: RL preserves better prior-task performance compared to SFT.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;Empirical Forgetting Law&lt;&#x2F;strong&gt;: When fine-tuning a model $\pi$ on a new task $\tau$, the degree of forgetting is accurately predicted by $\mathbb{E}_{x \sim \tau}[\operatorname{KL}(\pi_0 ||\pi)]$, where $\pi_0$ is the base policy. &lt;strong&gt;KL divergence is a reliable predictor of forgetting across settings&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Difference between SFT and RL&lt;&#x2F;strong&gt;: On-policy methods such as RL are inherently &lt;u&gt;biased toward solutions that remain closer to the original policy in KL divergence&lt;&#x2F;u&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;KL Hypothesis Validation&lt;&#x2F;strong&gt;: We construct a “oracle SFT” that minimizes KL divergence while achieving perfect accuracy, which achieves even less forget than RL. This demonstrate that &lt;u&gt;RL’s advantage does not stem from being inherently different, but from its implicit KL minimization&lt;&#x2F;u&gt;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;并不是 RL 好，而是 RL 中内含的 KL Minimizer 减少了遗忘，如果 SFT 能降低 KL Divergence，其也能减少遗忘。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Experimental Setup&lt;&#x2F;strong&gt;: We fine-tuned models using the same set of prompts. One group of models was trained with SFT, and another with RL using GRPO. In RL training, we used only a binary success indicator as the reward, &lt;strong&gt;without explicit KL regularization&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;LLM, Math Reasoning: Qwen 2.5 3B-Instruct on Open-Reasoner-Zero dataset.&lt;&#x2F;li&gt;
&lt;li&gt;LLM, Science Q&amp;amp;A: Qwen 2.5 3B-Instruct on Chemistry L-3 subset of SciKnowEval.&lt;&#x2F;li&gt;
&lt;li&gt;LLM, Tool use: Qwen 2.5 3B-Instruct on ToolAlpaca dataset.&lt;&#x2F;li&gt;
&lt;li&gt;Robotics, Pick and Place: OpenVLA 7B on SimplerEnv environment.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;RL Forgets Less than SFT&lt;&#x2F;strong&gt;: RL is able to learn new tasks while incurring minimal forgetting, whereas SFT reaches similar new-task performance only by sacrificing prior knowledge.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250909112119-amhkwcj.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Pareto frontiers of RL and SFT: Comparing the performance of a fine-tuned model on the new task (x-axis) and prior task (y-axis). Each point corresponds to a model trained with a different set of hyperparameters.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;Smaller KL Divergences Lead to Less Forgetting&lt;&#x2F;strong&gt;: We pretrained a MLP jointly on a subset of ParityMNIST and FashionMNIST, then fine-tuned only on ParityMNIST while measuring forgetting on FashionMNIST. We constructed an oracle SFT distribution (use the KL minimization answer as label instead of the true label).&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250910102102-a77ljmv.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;KL divergence predicts catastrophic forgetting: (1) SFT outperforms RL only when an oracle distribution is used. (2) Forgetting aligns a single curve when plotted against KL divergence. (3) RL improves new-task accuracy with much smaller KL shifts than SFT.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;On-Policy Methods Lead to Smaller KL Divergence&lt;&#x2F;strong&gt;: Here we consider the loss function of SFT and RL:&lt;&#x2F;p&gt;
&lt;p&gt;$$
{\mathcal{L}}_{\mathrm{SFT}}(\pi)=-\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\beta}}[\log\pi(y|x)]
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{L}_{\mathrm{RL}}(\pi)=-\mathbb{E}_{x\sim\mathcal{D},y\sim\pi}\left[A(x,y)\log\pi(y|x)\right]
$$&lt;&#x2F;p&gt;
&lt;p&gt;Here $A(x,y)$ is an advantage function. There are two features that distinguish RL from SFT:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Sampling Distribution: While in RL the training was done on outputs drawn from the model’s own distribution, in SFT they come from fixed external annotations.&lt;&#x2F;li&gt;
&lt;li&gt;Negative Examples: While sampling from $\pi$, some of the responses will be incorrect. These are usually assigned a negative coefficient $A(x,y)$. This pushes probability mass away from poor outputs.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Our hypothesis is that one of these two differences is what causes RL’s resistance to forgetting. So we perform experiments with four different objectives: “GRPO”, “1-0 Reinforce”, “SFT”, and “SimPO”. The results show that &lt;u&gt;the critical factor is the use of on-policy data&lt;&#x2F;u&gt;.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250910112852-pz5kjgm.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Comparison of algorithm classes: &amp;amp;quot;Pos Examples&amp;amp;quot; indicates that the dataset only contains positive examples while &amp;amp;quot;Pos + Neg Examples&amp;amp;quot; indicates that the dataset contains both positive and negative examples.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;p&gt;&lt;strong&gt;Theoretical Perspective&lt;&#x2F;strong&gt;: Sampling from the model’s own distribution keeps it close to the base model, while SFT pushes it toward arbitrary external distributions.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250910113142-i3xep9v.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;KL-minimal path to optimality.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;RL 的 policy 更新可以被视作来回的投影，源于 Information Geometry 的领域。假设空间中有我们的策略 $\pi_k$，可行空间 $\Pi$，最优空间 $P^\ast$。&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;SFT 就像给定一个 $P^\ast$ 的一个坐标，让我们传送到那里，不管距离有多远&lt;&#x2F;li&gt;
&lt;li&gt;RL 则先用 Information Projection 选择 $P^\ast$ 中最近的位置进行投影，再用 Momentum Projection 投影到 $\Pi$ 中。最终 RL 可以视作一次在 $\Pi$ 中的更新。&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;alternative-hypothesis&quot;&gt;Alternative Hypothesis&lt;&#x2F;h2&gt;
&lt;p&gt;We systematically evaluated alternative variables as potential predictors of catastrophic forgetting, grouped into four categories.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Weight-Level Changes&lt;&#x2F;strong&gt;: Many prior work tried to mitigate forgetting by &lt;u&gt;constraining the change in parameter space&lt;&#x2F;u&gt;. We measured parameter changes under L1, Fisher-weighted L2, and spectral norm metrics. These metrics correlated only weakly with forgetting: large parameter shifts could occur without forgetting, and conversely, forgetting sometimes occurred despite small parameter movement.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;一些想法：那 super weight 和 super activation 在这里会有体现吗？&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Representation&#x2F;Activation-Level Changes&lt;&#x2F;strong&gt;: Some other papers focused on &lt;u&gt;maintaining the previous features&lt;&#x2F;u&gt;. We examined hidden activation shifts (L1 and L2 distances) as proxies for changes in internal representations. Although we found that there is representation drift during training, the curves were distinct between training objectives, meaning that it is not a good predictor.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250910105915-bchua9h.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;CKA similarity to the base model during training.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;CKA (Centered Kernel Alignment) 是一种数学工具，用于衡量两个分布的相似度。CKA 越接近 $1$ 说明与原始分布越接近。上图说明了 SFT 中模型的内部知识结构（表征）遭到了破坏。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Sparsity and Rank of Updates&lt;&#x2F;strong&gt;: Some argue that RL updates are sparse while SFT weight updates are dense. We found that the reason for the observed sparse updates was the use of &lt;code&gt;bfloat16&lt;&#x2F;code&gt;​, which may ignoring some small updates. Performing the same training with &lt;code&gt;float32&lt;&#x2F;code&gt;​ leads to identical performance without any sparsity. So we found that &lt;u&gt;all algorithms lead to full rank weight updates&lt;&#x2F;u&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Distributional Distances&lt;&#x2F;strong&gt;: We considered multiple measures of output distribution change: (1) Forward KL $\mathbb{E}_{x \sim \tau}[\operatorname{KL}(\pi_0 || \pi)]$, (2) Reverse KL $\mathbb{E}_{x \sim \tau}[\operatorname{KL}(\pi || \pi_0)]$, (3) Total Variation, (4) $L_2$ distance between distributions.&lt;&#x2F;p&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250910104205-36souvi.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Predictive power of alternative variables compared to forward KL.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;​
&lt;blockquote&gt;
&lt;p&gt;Distribution 在这里指最终模型输出的差别，因为衡量遗忘最直接的办法是衡量其对老问题的回答有没有变。这里 $R^2$ 是一种统计指标，$R^2 = 1$ 表示完美预测。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;additional-results&quot;&gt;Additional Results&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Gradient Similarity versus KL Change&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;lun-wen-yue-du-rl-s-razor-why-online-reinforcement-learning-forgets-less&#x2F;assets&#x2F;image-20250910110729-fbb692h.png&quot; alt=&quot;image&quot; &#x2F;&gt;​&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry><entry xml:lang="zh">
        <title>论文阅读：连续学习的最优控制方式</title>
        <published>2025-08-15T00:00:00+00:00</published>
        <updated>2025-08-15T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi/</id>
        
            <content type="html">&lt;blockquote&gt;
&lt;p&gt;Original Paper: &lt;a href=&quot;https:&#x2F;&#x2F;www.123865.com&#x2F;s&#x2F;plj7Vv-fvR23&quot;&gt;Optimal Protocols for Continual Learning via Statistical Physics and Control Theory&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;概述：本篇文章介绍了如何使用最优控制理论调整连续学习中的 Replay 任务、学习率使得最终的 generalization error 最低。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Multi-Task Learning&lt;&#x2F;strong&gt;: Training a neural network on a series of tasks.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Catastrophic Forgetting&lt;&#x2F;strong&gt;: Multi-task learning can lead to catastrophic forgetting, where learning new tasks degrades performance on older ones.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Replay&lt;&#x2F;strong&gt;: Present the network with examples from the old tasks while training on the new one to minimize forgetting.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;a-teacher-student-framework&quot;&gt;A Teacher-Student Framework&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Teacher-Student Framework&lt;&#x2F;strong&gt;: Here we consider a teacher-student framework&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Student: The student network is trained on synthetic inputs $\boldsymbol{x} \in \mathbb{R}^N$, drawn i.i.d. from a standard Gaussian distribution $x_i \sim \mathcal{N}(0, 1)$. It is a two-layer neural network with $K$ hidden units, first-layer weight $\boldsymbol{W} = (\boldsymbol{w}_1,\cdots,\boldsymbol{w}_K)^{\top} \in \mathbb{R}^{K \times N}$, activation function $g$, and second-layer weights $\boldsymbol{v} \in \mathbb{R}^K$. It outputs the prediction&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{y}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{v}) = \sum\limits_{k = 1}^K g \left( \frac{\boldsymbol{x} \cdot \boldsymbol{w}_k}{\sqrt{N}} \right).
$$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Teacher: The labels for each task $t = 1,2,\cdots, T$ are generated by the teacher networks, $y^{(t)} = g_{\ast}(\boldsymbol{x} \cdot \boldsymbol{w}_{\ast}^{(t)}&#x2F;\sqrt{N})$, where $\boldsymbol{W}_{\ast} = (\boldsymbol{w}_{\ast}^{(1)},\cdots,\boldsymbol{w}_{\ast}^{(T)})^{\top} \in \mathbb{R}^{T \times N}$ denote the corresponding teacher vectors, and $g_{\ast}$ the activation function.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Task-Dependent Weights: We allow for task-dependent readout weights $\boldsymbol{V} = (\boldsymbol{v}^{(1)},\cdots,\boldsymbol{v}^{(T)})^{\top} \in \mathbb{R}^{T \times K}$. Specifically, when the task $t$ is presented, the readout is switched to the corresponding task, the first-layer weights are shared across tasks.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250815095756-eyuazzc.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Representation of the continual learning task in the teacher-student setting: (a) A student network is trained on i.i.d. inputs from two teacher networks, defining two different tasks; (b) Sequential training results in catastrophic forgetting.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;
&lt;p&gt;&lt;strong&gt;Generalization Error&lt;&#x2F;strong&gt;: The generalization error of the student on task $t$ is given by&lt;&#x2F;p&gt;
&lt;p&gt;$$
\varepsilon_t(\boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_*) := \frac{1}{2} \left\langle \left( y^{(t)} - \hat{y}^{(t)} \right)^2 \right\rangle = \frac{1}{2} \mathbb{E}_{\boldsymbol{x}} \left[ \left( g_* \left( \frac{\boldsymbol{w}_*^{(t)} \cdot \boldsymbol{x}}{\sqrt{N}} \right) - \hat{y}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{v}^{(t)}) \right)^2 \right].
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $(\boldsymbol{x}, y^{(t)})$ is a sample, $\hat{y}^{(t)}$ is the prediction, the angular brackets $\langle \cdot \rangle$ denote the expectation over the input distribution.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Overlaps Variables&lt;&#x2F;strong&gt;: The above generalization error depends only through the preactivations&lt;&#x2F;p&gt;
&lt;p&gt;$$
\lambda_k := \frac{\boldsymbol{x} \cdot \boldsymbol{w}_k}{\sqrt{N}}, \quad k = 1, \ldots, K, \qquad \text{and} \qquad \lambda_*^{(t)} := \frac{\boldsymbol{x} \cdot \boldsymbol{w}_*^{(t)}}{\sqrt{N}}, \quad t = 1, \ldots, T.
$$&lt;&#x2F;p&gt;
&lt;p&gt;They define jointly Gaussian variables with zero mean and second moments given by&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;M_{kt} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_k \lambda_*^{(t)} \right] = \frac{\boldsymbol{w}_k \cdot \boldsymbol{w}_*^{(t)}}{N} , \\
&amp;amp;Q_{kh} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_k \lambda_h \right] = \frac{\boldsymbol{w}_k \cdot \boldsymbol{w}_h}{N} , \\
&amp;amp;S_{tt’} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_*^{(t)} \lambda_*^{(t’)} \right] = \frac{\boldsymbol{w}_*^{(t)} \cdot \boldsymbol{w}_*^{(t’)}}{N} ,
\end{aligned}
$$&lt;&#x2F;p&gt;
&lt;p&gt;called &lt;em&gt;overlaps&lt;&#x2F;em&gt; in the statistical physics literature. Therefore, the dynamics of the generalization error is entirely captured by the evolution of the readouts $\boldsymbol{V}$ and the overlaps.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Forward Training Dynamics&lt;&#x2F;strong&gt;: We use the shorthand notation $\mathbb{Q} = (\operatorname{vec}(\boldsymbol{Q}), \operatorname{vec}(\boldsymbol{M}), \operatorname{vec}(\boldsymbol{V})) \in \mathbb{R}^{K^2 + 2KT}$. The training dynamics is described by a set of ODEs&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\mathrm{d}\mathbb{Q}(\alpha)}{\mathrm{d}\alpha} = f_{\mathbb{Q}}(\mathbb{Q}(\alpha), \boldsymbol{u}(\alpha)), \quad \alpha \in (0, \alpha_F].
$$&lt;&#x2F;p&gt;
&lt;p&gt;The parameter $\alpha$ denotes the effective training time, and $\boldsymbol{u}$ is the control variable.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-optimal-control-framework&quot;&gt;The Optimal Control Framework&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Optimal Control Framework&lt;&#x2F;strong&gt;: Our goal is to derive training strategies that are optimal with respect to the generalization performance &lt;em&gt;at the end of the training&lt;&#x2F;em&gt; and on &lt;em&gt;all tasks&lt;&#x2F;em&gt;. In practice, we minimize a linear combination of the generalization errors on different tasks&lt;&#x2F;p&gt;
&lt;p&gt;$$
h(\mathbb{Q}(\alpha_F)) := \sum\limits_{t = 1}^T c_t \epsilon_t(\mathbb{Q}(\alpha_F)), \quad \text{with} \quad c_t \geq 0, \sum\limits_{t = 1}^T c_t = 1.
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $\alpha_F$ is the final training time, the coefficients $c_t$ identify the relative importance of different tasks and $\epsilon_t$ denotes the infinite-dimensional limit of the average generalization error on task $t$. We define the cost functional&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{F}[\mathbb{Q}, \hat{\mathbb{Q}}, \bm{u}] = h\left(\mathbb{Q}(\alpha_F)\right) + \int_0^{\alpha_F} \mathrm{d}\alpha , \hat{\mathbb{Q}}(\alpha)^\top \left[ -\frac{\mathrm{d}\mathbb{Q}(\alpha)}{\mathrm{d}\alpha} + f_{\mathbb{Q}}\left(\mathbb{Q}(\alpha), \bm{u}(\alpha)\right) \right],
$$&lt;&#x2F;p&gt;
&lt;p&gt;Here $\hat{\mathbb{Q}} = (\operatorname{vec}(\hat{\boldsymbol{Q}}), \operatorname{vec}(\hat{\boldsymbol{M}}), \hat{\operatorname{vec}(\boldsymbol{V}}))$ is the conjugate order parameters (We can consider it as a Lagrange multiplier that incorperates the dynamics of $\mathbb{Q}$).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Backward Conjugate Dynamics&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi&#x2F;assets&#x2F;image-20250815150435-10kdupk.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry>
</feed>
