<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="https://reichtumqian.pages.dev/feed_style.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh">
    <tabi:metadata xmlns:tabi="https://github.com/welpo/tabi">
        <tabi:base_url>https:&#x2F;&#x2F;reichtumqian.pages.dev</tabi:base_url>
        <tabi:separator>
            •
        </tabi:separator>
        <tabi:about_feeds>This is a web feed, also known as an Atom feed. Subscribe by copying the URL from the address bar into your newsreader</tabi:about_feeds>
        <tabi:visit_the_site>Visit website</tabi:visit_the_site>
        <tabi:recent_posts>Recent posts</tabi:recent_posts>
        <tabi:last_updated_on>Updated on $DATE</tabi:last_updated_on>
        <tabi:default_theme></tabi:default_theme>
        <tabi:post_listing_date>date</tabi:post_listing_date>
        <tabi:current_section>Continual Learning</tabi:current_section>
    </tabi:metadata><title>Reichtum's Blog - Continual Learning</title>
        <subtitle>Reichtum&#x27;s Blog</subtitle>
    <link href="https://reichtumqian.pages.dev/tags/continual-learning/atom.xml" rel="self" type="application/atom+xml"/>
    <link href="https://reichtumqian.pages.dev/tags/continual-learning/" rel="alternate" type="text/html"/>
    <generator uri="https://www.getzola.org/">Zola</generator><updated>2025-08-15T00:00:00+00:00</updated><id>https://reichtumqian.pages.dev/tags/continual-learning/atom.xml</id><entry xml:lang="zh">
        <title>论文阅读：连续学习的最优控制方式</title>
        <published>2025-08-15T00:00:00+00:00</published>
        <updated>2025-08-15T00:00:00+00:00</updated>
        <author>
            <name>Reichtum</name>
        </author>
        <link rel="alternate" href="https://reichtumqian.pages.dev/blog/reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi/" type="text/html"/>
        <id>https://reichtumqian.pages.dev/blog/reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi/</id>
        
            <content type="html">&lt;blockquote&gt;
&lt;p&gt;Original Paper: &lt;a href=&quot;https:&#x2F;&#x2F;www.123865.com&#x2F;s&#x2F;plj7Vv-fvR23&quot;&gt;Optimal Protocols for Continual Learning via Statistical Physics and Control Theory&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;概述：本篇文章介绍了如何使用最优控制理论调整连续学习中的 Replay 任务、学习率使得最终的 generalization error 最低。&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Multi-Task Learning&lt;&#x2F;strong&gt;: Training a neural network on a series of tasks.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Catastrophic Forgetting&lt;&#x2F;strong&gt;: Multi-task learning can lead to catastrophic forgetting, where learning new tasks degrades performance on older ones.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Replay&lt;&#x2F;strong&gt;: Present the network with examples from the old tasks while training on the new one to minimize forgetting.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;a-teacher-student-framework&quot;&gt;A Teacher-Student Framework&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Teacher-Student Framework&lt;&#x2F;strong&gt;: Here we consider a teacher-student framework&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Student: The student network is trained on synthetic inputs $\boldsymbol{x} \in \mathbb{R}^N$, drawn i.i.d. from a standard Gaussian distribution $x_i \sim \mathcal{N}(0, 1)$. It is a two-layer neural network with $K$ hidden units, first-layer weight $\boldsymbol{W} = (\boldsymbol{w}_1,\cdots,\boldsymbol{w}_K)^{\top} \in \mathbb{R}^{K \times N}$, activation function $g$, and second-layer weights $\boldsymbol{v} \in \mathbb{R}^K$. It outputs the prediction&lt;&#x2F;p&gt;
&lt;p&gt;$$
\hat{y}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{v}) = \sum\limits_{k = 1}^K g \left( \frac{\boldsymbol{x} \cdot \boldsymbol{w}_k}{\sqrt{N}} \right).
$$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Teacher: The labels for each task $t = 1,2,\cdots, T$ are generated by the teacher networks, $y^{(t)} = g_{\ast}(\boldsymbol{x} \cdot \boldsymbol{w}_{\ast}^{(t)}&#x2F;\sqrt{N})$, where $\boldsymbol{W}_{\ast} = (\boldsymbol{w}_{\ast}^{(1)},\cdots,\boldsymbol{w}_{\ast}^{(T)})^{\top} \in \mathbb{R}^{T \times N}$ denote the corresponding teacher vectors, and $g_{\ast}$ the activation function.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Task-Dependent Weights: We allow for task-dependent readout weights $\boldsymbol{V} = (\boldsymbol{v}^{(1)},\cdots,\boldsymbol{v}^{(T)})^{\top} \in \mathbb{R}^{T \times K}$. Specifically, when the task $t$ is presented, the readout is switched to the corresponding task, the first-layer weights are shared across tasks.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;figure&gt;
  &lt;img src=&quot;assets&#x2F;image-20250815095756-eyuazzc.png&quot; alt=&quot;image&quot;&gt;
  &lt;figcaption&gt;Representation of the continual learning task in the teacher-student setting: (a) A student network is trained on i.i.d. inputs from two teacher networks, defining two different tasks; (b) Sequential training results in catastrophic forgetting.&lt;&#x2F;figcaption&gt;
&lt;&#x2F;figure&gt;
&lt;p&gt;&lt;strong&gt;Generalization Error&lt;&#x2F;strong&gt;: The generalization error of the student on task $t$ is given by&lt;&#x2F;p&gt;
&lt;p&gt;$$
\varepsilon_t(\boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_*) := \frac{1}{2} \left\langle \left( y^{(t)} - \hat{y}^{(t)} \right)^2 \right\rangle = \frac{1}{2} \mathbb{E}_{\boldsymbol{x}} \left[ \left( g_* \left( \frac{\boldsymbol{w}_*^{(t)} \cdot \boldsymbol{x}}{\sqrt{N}} \right) - \hat{y}(\boldsymbol{x}; \boldsymbol{W}, \boldsymbol{v}^{(t)}) \right)^2 \right].
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $(\boldsymbol{x}, y^{(t)})$ is a sample, $\hat{y}^{(t)}$ is the prediction, the angular brackets $\langle \cdot \rangle$ denote the expectation over the input distribution.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Overlaps Variables&lt;&#x2F;strong&gt;: The above generalization error depends only through the preactivations&lt;&#x2F;p&gt;
&lt;p&gt;$$
\lambda_k := \frac{\boldsymbol{x} \cdot \boldsymbol{w}_k}{\sqrt{N}}, \quad k = 1, \ldots, K, \qquad \text{and} \qquad \lambda_*^{(t)} := \frac{\boldsymbol{x} \cdot \boldsymbol{w}_*^{(t)}}{\sqrt{N}}, \quad t = 1, \ldots, T.
$$&lt;&#x2F;p&gt;
&lt;p&gt;They define jointly Gaussian variables with zero mean and second moments given by&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{aligned}
&amp;amp;M_{kt} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_k \lambda_*^{(t)} \right] = \frac{\boldsymbol{w}_k \cdot \boldsymbol{w}_*^{(t)}}{N} , \\
&amp;amp;Q_{kh} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_k \lambda_h \right] = \frac{\boldsymbol{w}_k \cdot \boldsymbol{w}_h}{N} , \\
&amp;amp;S_{tt’} := \mathbb{E}_{\boldsymbol{x}} \left[ \lambda_*^{(t)} \lambda_*^{(t’)} \right] = \frac{\boldsymbol{w}_*^{(t)} \cdot \boldsymbol{w}_*^{(t’)}}{N} ,
\end{aligned}
$$&lt;&#x2F;p&gt;
&lt;p&gt;called &lt;em&gt;overlaps&lt;&#x2F;em&gt; in the statistical physics literature. Therefore, the dynamics of the generalization error is entirely captured by the evolution of the readouts $\boldsymbol{V}$ and the overlaps.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Forward Training Dynamics&lt;&#x2F;strong&gt;: We use the shorthand notation $\mathbb{Q} = (\operatorname{vec}(\boldsymbol{Q}), \operatorname{vec}(\boldsymbol{M}), \operatorname{vec}(\boldsymbol{V})) \in \mathbb{R}^{K^2 + 2KT}$. The training dynamics is described by a set of ODEs&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\mathrm{d}\mathbb{Q}(\alpha)}{\mathrm{d}\alpha} = f_{\mathbb{Q}}(\mathbb{Q}(\alpha), \boldsymbol{u}(\alpha)), \quad \alpha \in (0, \alpha_F].
$$&lt;&#x2F;p&gt;
&lt;p&gt;The parameter $\alpha$ denotes the effective training time, and $\boldsymbol{u}$ is the control variable.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;the-optimal-control-framework&quot;&gt;The Optimal Control Framework&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Optimal Control Framework&lt;&#x2F;strong&gt;: Our goal is to derive training strategies that are optimal with respect to the generalization performance &lt;em&gt;at the end of the training&lt;&#x2F;em&gt; and on &lt;em&gt;all tasks&lt;&#x2F;em&gt;. In practice, we minimize a linear combination of the generalization errors on different tasks&lt;&#x2F;p&gt;
&lt;p&gt;$$
h(\mathbb{Q}(\alpha_F)) := \sum\limits_{t = 1}^T c_t \epsilon_t(\mathbb{Q}(\alpha_F)), \quad \text{with} \quad c_t \geq 0, \sum\limits_{t = 1}^T c_t = 1.
$$&lt;&#x2F;p&gt;
&lt;p&gt;where $\alpha_F$ is the final training time, the coefficients $c_t$ identify the relative importance of different tasks and $\epsilon_t$ denotes the infinite-dimensional limit of the average generalization error on task $t$. We define the cost functional&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{F}[\mathbb{Q}, \hat{\mathbb{Q}}, \bm{u}] = h\left(\mathbb{Q}(\alpha_F)\right) + \int_0^{\alpha_F} \mathrm{d}\alpha , \hat{\mathbb{Q}}(\alpha)^\top \left[ -\frac{\mathrm{d}\mathbb{Q}(\alpha)}{\mathrm{d}\alpha} + f_{\mathbb{Q}}\left(\mathbb{Q}(\alpha), \bm{u}(\alpha)\right) \right],
$$&lt;&#x2F;p&gt;
&lt;p&gt;Here $\hat{\mathbb{Q}} = (\operatorname{vec}(\hat{\boldsymbol{Q}}), \operatorname{vec}(\hat{\boldsymbol{M}}), \hat{\operatorname{vec}(\boldsymbol{V}}))$ is the conjugate order parameters (We can consider it as a Lagrange multiplier that incorperates the dynamics of $\mathbb{Q}$).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Backward Conjugate Dynamics&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;reichtumqian.pages.dev&#x2F;blog&#x2F;reading-lian-xu-xue-xi-de-zui-you-kong-zhi-fang-shi&#x2F;assets&#x2F;image-20250815150435-10kdupk.png&quot; alt=&quot;image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
&lt;p&gt;‍&lt;&#x2F;p&gt;
</content>
        </entry>
</feed>
